
R version 3.3.0 (2016-05-03) -- "Supposedly Educational"
Copyright (C) 2016 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "mgcv"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('mgcv')
Loading required package: nlme
This is mgcv 1.8-12. For overview type 'help("mgcv-package")'.
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("Beta")
> ### * Beta
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: betar
> ### Title: GAM beta regression family
> ### Aliases: betar
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## Simulate some beta data...
> set.seed(3);n<-400
> dat <- gamSim(1,n=n)
Gu & Wahba 4 term additive model
> mu <- binomial()$linkinv(dat$f/4-2)
> phi <- .5
> a <- mu*phi;b <- phi - a;
> dat$y <- rbeta(n,a,b) 
> 
> bm <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=betar(link="logit"),data=dat)
> 
> bm

Family: Beta regression(0.491) 
Link function: logit 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
1.73 1.63 5.62 1.00  total = 10.98 

REML score: -991.9735     
> plot(bm,pages=1)
> 
> 
> 
> cleanEx()
> nameEx("Predict.matrix")
> ### * Predict.matrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Predict.matrix
> ### Title: Prediction methods for smooth terms in a GAM
> ### Aliases: Predict.matrix Predict.matrix2
> ### Keywords: models smooth regression
> 
> ### ** Examples
> # See smooth.construct examples
> 
> 
> 
> cleanEx()
> nameEx("Predict.matrix.cr.smooth")
> ### * Predict.matrix.cr.smooth
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Predict.matrix.cr.smooth
> ### Title: Predict matrix method functions
> ### Aliases: Predict.matrix.cr.smooth Predict.matrix.cs.smooth
> ###   Predict.matrix.cyclic.smooth Predict.matrix.pspline.smooth
> ###   Predict.matrix.tensor.smooth Predict.matrix.tprs.smooth
> ###   Predict.matrix.ts.smooth Predict.matrix.t2.smooth
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## see smooth.construct
>  
> 
> 
> 
> 
> cleanEx()
> nameEx("Predict.matrix.soap.film")
> ### * Predict.matrix.soap.film
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Predict.matrix.soap.film
> ### Title: Prediction matrix for soap film smooth
> ### Aliases: Predict.matrix.soap.film Predict.matrix.sw Predict.matrix.sf
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## This is a lower level example. The basis and 
> ## penalties are obtained explicitly 
> ## and `magic' is used as the fitting routine...
> 
> require(mgcv)
> set.seed(66)
> 
> ## create a boundary...
> fsb <- list(fs.boundary())
> 
> ## create some internal knots...
> knots <- data.frame(x=rep(seq(-.5,3,by=.5),4),
+                     y=rep(c(-.6,-.3,.3,.6),rep(8,4)))
> 
> ## Simulate some fitting data, inside boundary...
> n<-1000
> x <- runif(n)*5-1;y<-runif(n)*2-1
> z <- fs.test(x,y,b=1)
> ind <- inSide(fsb,x,y) ## remove outsiders
> z <- z[ind];x <- x[ind]; y <- y[ind] 
> n <- length(z)
> z <- z + rnorm(n)*.3 ## add noise
> 
> ## plot boundary with knot and data locations
> plot(fsb[[1]]$x,fsb[[1]]$y,type="l");points(knots$x,knots$y,pch=20,col=2)
> points(x,y,pch=".",col=3);
> 
> ## set up the basis and penalties...
> sob <- smooth.construct2(s(x,y,bs="so",k=40,xt=list(bnd=fsb,nmax=100)),
+               data=data.frame(x=x,y=y),knots=knots)
> ## ... model matrix is element `X' of sob, penalties matrices 
> ## are in list element `S'.
> 
> ## fit using `magic'
> um <- magic(z,sob$X,sp=c(-1,-1),sob$S,off=c(1,1))
> beta <- um$b
> 
> ## produce plots...
> par(mfrow=c(2,2),mar=c(4,4,1,1))
> m<-100;n<-50 
> xm <- seq(-1,3.5,length=m);yn<-seq(-1,1,length=n)
> xx <- rep(xm,n);yy<-rep(yn,rep(m,n))
> 
> ## plot truth...
> tru <- matrix(fs.test(xx,yy),m,n) ## truth
> image(xm,yn,tru,col=heat.colors(100),xlab="x",ylab="y")
> lines(fsb[[1]]$x,fsb[[1]]$y,lwd=3)
> contour(xm,yn,tru,levels=seq(-5,5,by=.25),add=TRUE)
> 
> ## Plot soap, by first predicting on a fine grid...
> 
> ## First get prediction matrix...
> X <- Predict.matrix2(sob,data=list(x=xx,y=yy))
> 
> ## Now the predictions...
> fv <- X%*%beta
> 
> ## Plot the estimated function...
> image(xm,yn,matrix(fv,m,n),col=heat.colors(100),xlab="x",ylab="y")
> lines(fsb[[1]]$x,fsb[[1]]$y,lwd=3)
> points(x,y,pch=".")
> contour(xm,yn,matrix(fv,m,n),levels=seq(-5,5,by=.25),add=TRUE)
> 
> ## Plot TPRS...
> b <- gam(z~s(x,y,k=100))
> fv.gam <- predict(b,newdata=data.frame(x=xx,y=yy))
> names(sob$sd$bnd[[1]]) <- c("xx","yy","d")
> ind <- inSide(sob$sd$bnd,xx,yy)
> fv.gam[!ind]<-NA
> image(xm,yn,matrix(fv.gam,m,n),col=heat.colors(100),xlab="x",ylab="y")
> lines(fsb[[1]]$x,fsb[[1]]$y,lwd=3)
> points(x,y,pch=".")
> contour(xm,yn,matrix(fv.gam,m,n),levels=seq(-5,5,by=.25),add=TRUE)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("Rrank")
> ### * Rrank
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Rrank
> ### Title: Find rank of upper triangular matrix
> ### Aliases: Rrank
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
>   set.seed(0)
>   n <- 10;p <- 5
>   X <- matrix(runif(n*(p-1)),n,p)
>   qrx <- qr(X,LAPACK=TRUE)
>   Rrank(qr.R(qrx))
[1] 4
> 
> 
> 
> cleanEx()
> nameEx("Tweedie")
> ### * Tweedie
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Tweedie
> ### Title: GAM Tweedie families
> ### Aliases: Tweedie tw
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(3)
> n<-400
> ## Simulate data...
> dat <- gamSim(1,n=n,dist="poisson",scale=.2)
Gu & Wahba 4 term additive model
> dat$y <- rTweedie(exp(dat$f),p=1.3,phi=.5) ## Tweedie response
> 
> ## Fit a fixed p Tweedie, with wrong link ...
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=Tweedie(1.25,power(.1)),
+          data=dat)
> plot(b,pages=1)
> print(b)

Family: Tweedie(1.25) 
Link function: mu^0.1 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.71 2.66 7.36 1.00  total = 14.73 

GCV score: 0.5404011     
> 
> ## Same by approximate REML...
> b1 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=Tweedie(1.25,power(.1)),
+           data=dat,method="REML")
> plot(b1,pages=1)
> print(b1)

Family: Tweedie(1.25) 
Link function: mu^0.1 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
3.37 3.12 7.97 1.00  total = 16.46 

REML score: 860.0066     
> 
> ## estimate p as part of fitting
> 
> b2 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=tw(),
+           data=dat,method="REML")
> plot(b2,pages=1)
> print(b2)

Family: Tweedie(p=1.401) 
Link function: log 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
3.32 3.04 7.89 1.00  total = 16.26 

REML score: 846.1687     
> 
> rm(dat)
> 
> 
> 
> cleanEx()
> nameEx("anova.gam")
> ### * anova.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: anova.gam
> ### Title: Approximate hypothesis tests related to GAM fits
> ### Aliases: anova.gam print.anova.gam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(0)
> dat <- gamSim(5,n=200,scale=2)
Additive model + factor
> 
> b<-gam(y ~ x0 + s(x1) + s(x2) + s(x3),data=dat)
> anova(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ x0 + s(x1) + s(x2) + s(x3)

Parametric Terms:
   df     F p-value
x0  3 77.42  <2e-16

Approximate significance of smooth terms:
        edf Ref.df      F p-value
s(x1) 1.729  2.158 45.309  <2e-16
s(x2) 7.069  8.120 49.230  <2e-16
s(x3) 1.000  1.000  0.056   0.812
> b1<-gam(y ~ x0 + s(x1) + s(x2),data=dat)
> anova(b,b1,test="F")
Analysis of Deviance Table

Model 1: y ~ x0 + s(x1) + s(x2) + s(x3)
Model 2: y ~ x0 + s(x1) + s(x2)
  Resid. Df Resid. Dev       Df  Deviance      F Pr(>F)
1    186.20     793.73                                 
2    187.19     793.78 -0.99189 -0.051961 0.0123 0.9104
> 
> 
> 
> cleanEx()
> nameEx("bam")
> ### * bam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bam
> ### Title: Generalized additive models for very large datasets
> ### Aliases: bam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## See help("mgcv-parallel") for using bam in parallel
> 
> ## Some examples are marked 'Not run' purely to keep 
> ## checking load on CRAN down. Sample sizes are small for 
> ## the same reason.
> 
> set.seed(3)
> dat <- gamSim(1,n=25000,dist="normal",scale=20)
Gu & Wahba 4 term additive model
> bs <- "cr";k <- 12
> b <- bam(y ~ s(x0,bs=bs)+s(x1,bs=bs)+s(x2,bs=bs,k=k)+
+            s(x3,bs=bs),data=dat)
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = bs) + s(x1, bs = bs) + s(x2, bs = bs, k = k) + 
    s(x3, bs = bs)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.8918     0.1275   61.88   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df      F  p-value    
s(x0) 3.113  3.863  6.667 3.47e-05 ***
s(x1) 2.826  3.511 63.015  < 2e-16 ***
s(x2) 8.620  9.905 52.059  < 2e-16 ***
s(x3) 1.002  1.004  3.829   0.0503 .  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.0295   Deviance explained = 3.01%
fREML = 1.1057e+05  Scale est. = 406.15    n = 25000
> plot(b,pages=1,rug=FALSE)  ## plot smooths, but not rug
> plot(b,pages=1,rug=FALSE,seWithMean=TRUE) ## `with intercept' CIs
> 
> ## Not run: 
> ##D  
> ##D ba <- bam(y ~ s(x0,bs=bs,k=k)+s(x1,bs=bs,k=k)+s(x2,bs=bs,k=k)+
> ##D             s(x3,bs=bs,k=k),data=dat,method="GCV.Cp") ## use GCV
> ##D summary(ba)
> ## End(Not run)
> 
> ## A Poisson example...
> 
> k <- 15
> dat <- gamSim(1,n=21000,dist="poisson",scale=.1)
Gu & Wahba 4 term additive model
> 
> system.time(b1 <- bam(y ~ s(x0,bs=bs)+s(x1,bs=bs)+s(x2,bs=bs,k=k),
+             data=dat,family=poisson()))
   user  system elapsed 
   1.66    0.02    1.68 
> b1

Family: poisson 
Link function: log 

Formula:
y ~ s(x0, bs = bs) + s(x1, bs = bs) + s(x2, bs = bs, k = k)

Estimated degrees of freedom:
 4.77  4.36 12.42  total = 22.55 

fREML score: 29782.29     
> 
> 
> ## Sparse smoother example (deprecated)...
> ## Not run: 
> ##D dat <- gamSim(1,n=10000,dist="poisson",scale=.1)
> ##D system.time( b3 <- bam(y ~ te(x0,x1,bs="ps",k=10,np=FALSE)+
> ##D              s(x2,bs="ps",k=30)+s(x3,bs="ps",k=30),data=dat,
> ##D              method="REML",family=poisson(),sparse=TRUE))
> ##D b3
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("bam.update")
> ### * bam.update
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bam.update
> ### Title: Update a strictly additive bam model for new data.
> ### Aliases: bam.update
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## following is not *very* large, for obvious reasons...
> set.seed(8)
> n <- 5000
> dat <- gamSim(1,n=n,dist="normal",scale=5)
Gu & Wahba 4 term additive model
> dat[c(50,13,3000,3005,3100),]<- NA
> dat1 <- dat[(n-999):n,]
> dat0 <- dat[1:(n-1000),]
> bs <- "ps";k <- 20
> method <- "GCV.Cp"
> b <- bam(y ~ s(x0,bs=bs,k=k)+s(x1,bs=bs,k=k)+s(x2,bs=bs,k=k)+
+            s(x3,bs=bs,k=k),data=dat0,method=method)
> 
> b1 <- bam.update(b,dat1)
> 
> b2 <- bam.update(bam.update(b,dat1[1:500,]),dat1[501:1000,])
>  
> b3 <- bam(y ~ s(x0,bs=bs,k=k)+s(x1,bs=bs,k=k)+s(x2,bs=bs,k=k)+
+            s(x3,bs=bs,k=k),data=dat,method=method)
> b1;b2;b3

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = bs, k = k) + s(x1, bs = bs, k = k) + s(x2, bs = bs, 
    k = k) + s(x3, bs = bs, k = k)

Estimated degrees of freedom:
 3.29  3.04 13.29 14.54  total = 35.16 

GCV score: 25.05259     

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = bs, k = k) + s(x1, bs = bs, k = k) + s(x2, bs = bs, 
    k = k) + s(x3, bs = bs, k = k)

Estimated degrees of freedom:
 3.29  3.04 13.29 14.54  total = 35.15 

GCV score: 25.05259     

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = bs, k = k) + s(x1, bs = bs, k = k) + s(x2, bs = bs, 
    k = k) + s(x3, bs = bs, k = k)

Estimated degrees of freedom:
 3.29  3.04 13.26 14.54  total = 35.13 

GCV score: 25.05261     
> 
> ## example with AR1 errors...
> 
> e <- rnorm(n)
> for (i in 2:n) e[i] <- e[i-1]*.7 + e[i]
> dat$y <- dat$f + e*3
> dat[c(50,13,3000,3005,3100),]<- NA
> dat1 <- dat[(n-999):n,]
> dat0 <- dat[1:(n-1000),]
> method <- "ML"
> 
> b <- bam(y ~ s(x0,bs=bs,k=k)+s(x1,bs=bs,k=k)+s(x2,bs=bs,k=k)+
+            s(x3,bs=bs,k=k),data=dat0,method=method,rho=0.7)
> 
> b1 <- bam.update(b,dat1)
> 
> 
> summary(b1);summary(b2);summary(b3)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = bs, k = k) + s(x1, bs = bs, k = k) + s(x2, bs = bs, 
    k = k) + s(x3, bs = bs, k = k)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.9912     0.1419    56.3   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
         edf Ref.df       F p-value    
s(x0)  4.990  6.120  55.035  <2e-16 ***
s(x1)  4.391  5.397 500.357  <2e-16 ***
s(x2) 13.474 15.318 404.598  <2e-16 ***
s(x3)  1.005  1.010   1.646   0.199    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.391   Deviance explained = 39.4%
-ML =  10950  Scale est. = 17.771    n = 4995

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = bs, k = k) + s(x1, bs = bs, k = k) + s(x2, bs = bs, 
    k = k) + s(x3, bs = bs, k = k)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.98248    0.07062     113   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
         edf Ref.df       F  p-value    
s(x0)  3.292  4.074  17.378 2.61e-14 ***
s(x1)  3.037  3.764 157.017  < 2e-16 ***
s(x2) 13.286 15.148  94.751  < 2e-16 ***
s(x3) 14.540 16.204   1.337    0.146    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.298   Deviance explained = 30.3%
GCV = 25.053  Scale est. = 24.876    n = 4995

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = bs, k = k) + s(x1, bs = bs, k = k) + s(x2, bs = bs, 
    k = k) + s(x3, bs = bs, k = k)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.95364    0.07057   112.7   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
         edf Ref.df       F  p-value    
s(x0)  3.292  4.074  17.378 2.62e-14 ***
s(x1)  3.037  3.764 157.010  < 2e-16 ***
s(x2) 13.259 15.124  94.902  < 2e-16 ***
s(x3) 14.539 16.203   1.337    0.146    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.298   Deviance explained = 30.3%
GCV = 25.053  Scale est. = 24.876    n = 4995
> 
> 
> 
> 
> cleanEx()
> nameEx("bandchol")
> ### * bandchol
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bandchol
> ### Title: Choleski decomposition of a band diagonal matrix
> ### Aliases: bandchol
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> ## simulate a banded diagonal matrix
> n <- 7;set.seed(8)
> A <- matrix(0,n,n)
> sdiag(A) <- runif(n);sdiag(A,1) <- runif(n-1)
> sdiag(A,2) <- runif(n-2)
> A <- crossprod(A) 
> 
> ## full matrix form...
> bandchol(A)
          [,1]      [,2]      [,3]      [,4]      [,5]        [,6]      [,7]
[1,] 0.4662952 0.9322698 0.5449621 0.0000000 0.0000000 0.000000000 0.0000000
[2,] 0.0000000 0.2078233 0.7691470 0.1382243 0.0000000 0.000000000 0.0000000
[3,] 0.0000000 0.0000000 0.7996580 0.6444911 0.9278123 0.000000000 0.0000000
[4,] 0.0000000 0.0000000 0.0000000 0.6518713 0.4570449 0.001301721 0.0000000
[5,] 0.0000000 0.0000000 0.0000000 0.0000000 0.3215092 0.089301011 0.2644589
[6,] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.718927504 0.4323914
[7,] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.000000000 0.2908734
> chol(A) ## for comparison
          [,1]      [,2]      [,3]      [,4]      [,5]        [,6]      [,7]
[1,] 0.4662952 0.9322698 0.5449621 0.0000000 0.0000000 0.000000000 0.0000000
[2,] 0.0000000 0.2078233 0.7691470 0.1382243 0.0000000 0.000000000 0.0000000
[3,] 0.0000000 0.0000000 0.7996580 0.6444911 0.9278123 0.000000000 0.0000000
[4,] 0.0000000 0.0000000 0.0000000 0.6518713 0.4570449 0.001301721 0.0000000
[5,] 0.0000000 0.0000000 0.0000000 0.0000000 0.3215092 0.089301011 0.2644589
[6,] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.718927504 0.4323914
[7,] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.000000000 0.2908734
> 
> ## compact storage form...
> B <- matrix(0,3,n)
> B[1,] <- sdiag(A);B[2,1:(n-1)] <- sdiag(A,1)
> B[3,1:(n-2)] <- sdiag(A,2)
> bandchol(B)
          [,1]      [,2]      [,3]        [,4]       [,5]      [,6]      [,7]
[1,] 0.4662952 0.2078233 0.7996580 0.651871318 0.32150918 0.7189275 0.2908734
[2,] 0.9322698 0.7691470 0.6444911 0.457044889 0.08930101 0.4323914 0.0000000
[3,] 0.5449621 0.1382243 0.9278123 0.001301721 0.26445886 0.0000000 0.0000000
> 
> 
> 
> 
> cleanEx()
> nameEx("cSplineDes")
> ### * cSplineDes
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cSplineDes
> ### Title: Evaluate cyclic B spline basis
> ### Aliases: cSplineDes
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
>  require(mgcv)
>  ## create some x's and knots...
>  n <- 200
>  x <- 0:(n-1)/(n-1);k<- 0:5/5
>  X <- cSplineDes(x,k) ## cyclic spline design matrix
>  ## plot evaluated basis functions...
>  plot(x,X[,1],type="l"); for (i in 2:5) lines(x,X[,i],col=i)
>  ## check that the ends match up....
>  ee <- X[1,]-X[n,];ee 
[1] 0 0 0 0 0
>  tol <- .Machine$double.eps^.75
>  if (all.equal(ee,ee*0,tolerance=tol)!=TRUE) 
+    stop("cyclic spline ends don't match!")
>  
>  ## similar with uneven data spacing...
>  x <- sort(runif(n)) + 1 ## sorting just makes end checking easy
>  k <- seq(min(x),max(x),length=8) ## create knots
>  X <- cSplineDes(x,k) ## get cyclic spline model matrix  
>  plot(x,X[,1],type="l"); for (i in 2:ncol(X)) lines(x,X[,i],col=i)
>  ee <- X[1,]-X[n,];ee ## do ends match??
[1] 0 0 0 0 0 0 0
>  tol <- .Machine$double.eps^.75
>  if (all.equal(ee,ee*0,tolerance=tol)!=TRUE) 
+    stop("cyclic spline ends don't match!")
> 
> 
> 
> cleanEx()
> nameEx("choose.k")
> ### * choose.k
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: choose.k
> ### Title: Basis dimension choice for smooths
> ### Aliases: choose.k
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Simulate some data ....
> library(mgcv)
> set.seed(1) 
> dat <- gamSim(1,n=400,scale=2)
Gu & Wahba 4 term additive model
> 
> ## fit a GAM with quite low `k'
> b<-gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=6)+s(x3,k=6),data=dat)
> plot(b,pages=1,residuals=TRUE) ## hint of a problem in s(x2)
> 
> ## the following suggests a problem with s(x2)
> gam.check(b)

Method: GCV   Optimizer: magic
Smoothing parameter selection converged after 17 iterations.
The RMS GCV score gradiant at convergence was 2.675921e-07 .
The Hessian was positive definite.
The estimated model rank was 21 (maximum possible: 21)
Model rank =  21 / 21 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

         k'   edf k-index p-value
s(x0) 5.000 2.628   0.978    0.24
s(x1) 5.000 2.868   0.995    0.48
s(x2) 5.000 4.957   0.918    0.05
s(x3) 5.000 1.000   1.018    0.62
> 
> ## Another approach (see below for more obvious method)....
> ## check for residual pattern, removeable by increasing `k'
> ## typically `k', below, chould be substantially larger than 
> ## the original, `k' but certainly less than n/2.
> ## Note use of cheap "cs" shrinkage smoothers, and gamma=1.4
> ## to reduce chance of overfitting...
> rsd <- residuals(b)
> gam(rsd~s(x0,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x0, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 4.448352     
> gam(rsd~s(x1,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x1, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 4.448352     
> gam(rsd~s(x2,k=40,bs="cs"),gamma=1.4,data=dat) ## `k' too low

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x2, k = 40, bs = "cs")

Estimated degrees of freedom:
9.01  total = 10.01 

GCV score: 4.494652     
> gam(rsd~s(x3,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x3, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 4.448352     
> 
> ## refit...
> b <- gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=20)+s(x3,k=6),data=dat)
> gam.check(b) ## better

Method: GCV   Optimizer: magic
Smoothing parameter selection converged after 9 iterations.
The RMS GCV score gradiant at convergence was 2.918624e-07 .
The Hessian was positive definite.
The estimated model rank was 35 (maximum possible: 35)
Model rank =  35 / 35 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

          k'    edf k-index p-value
s(x0)  5.000  2.701   0.984    0.34
s(x1)  5.000  2.619   0.979    0.31
s(x2) 19.000 11.217   0.980    0.28
s(x3)  5.000  1.000   1.022    0.65
> 
> ## similar example with multi-dimensional smooth
> b1 <- gam(y~s(x0)+s(x1,x2,k=15)+s(x3),data=dat)
> rsd <- residuals(b1)
> gam(rsd~s(x0,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x0, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 5.257725     
> gam(rsd~s(x1,x2,k=100,bs="ts"),gamma=1.4,data=dat) ## `k' too low

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x1, x2, k = 100, bs = "ts")

Estimated degrees of freedom:
30.6  total = 31.62 

GCV score: 5.066268     
> gam(rsd~s(x3,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x3, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 5.257725     
> 
> gam.check(b1) ## shows same problem

Method: GCV   Optimizer: magic
Smoothing parameter selection converged after 15 iterations.
The RMS GCV score gradiant at convergence was 2.956997e-07 .
The Hessian was positive definite.
The estimated model rank was 33 (maximum possible: 33)
Model rank =  33 / 33 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

             k'    edf k-index p-value
s(x0)     9.000  2.456   0.965    0.22
s(x1,x2) 14.000 13.737   0.834    0.00
s(x3)     9.000  1.000   1.001    0.50
> 
> ## and a `te' example
> b2 <- gam(y~s(x0)+te(x1,x2,k=4)+s(x3),data=dat)
> rsd <- residuals(b2)
> gam(rsd~s(x0,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x0, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 6.181669     
> gam(rsd~te(x1,x2,k=10,bs="cs"),gamma=1.4,data=dat) ## `k' too low

Family: gaussian 
Link function: identity 

Formula:
rsd ~ te(x1, x2, k = 10, bs = "cs")

Estimated degrees of freedom:
17  total = 18.05 

GCV score: 4.565294     
> gam(rsd~s(x3,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x3, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 6.181669     
> 
> gam.check(b2) ## shows same problem

Method: GCV   Optimizer: magic
Smoothing parameter selection converged after 15 iterations.
The RMS GCV score gradiant at convergence was 2.875373e-07 .
The Hessian was positive definite.
The estimated model rank was 34 (maximum possible: 34)
Model rank =  34 / 34 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

              k'    edf k-index p-value
s(x0)      9.000  2.603   1.048    0.78
te(x1,x2) 15.000  9.942   0.682    0.00
s(x3)      9.000  1.000   0.959    0.19
> 
> ## same approach works with other families in the original model
> dat <- gamSim(1,n=400,scale=.25,dist="poisson")
Gu & Wahba 4 term additive model
> bp<-gam(y~s(x0,k=5)+s(x1,k=5)+s(x2,k=5)+s(x3,k=5),
+         family=poisson,data=dat,method="ML")
> 
> gam.check(bp)

Method: ML   Optimizer: outer newton
full convergence after 8 iterations.
Gradient range [-9.75731e-08,8.145418e-05]
(score 1014.797 & scale 1).
Hessian positive definite, eigenvalue range [0.3386221,1.485035].
Model rank =  17 / 17 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

         k'   edf k-index p-value
s(x0) 4.000 3.393   1.033    0.78
s(x1) 4.000 3.263   1.059    0.84
s(x2) 4.000 3.992   0.856    0.00
s(x3) 4.000 1.829   0.961    0.21
> 
> rsd <- residuals(bp)
> gam(rsd~s(x0,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x0, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 1.238283     
> gam(rsd~s(x1,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x1, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 1.238283     
> gam(rsd~s(x2,k=40,bs="cs"),gamma=1.4,data=dat) ## `k' too low

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x2, k = 40, bs = "cs")

Estimated degrees of freedom:
8.54  total = 9.54 

GCV score: 1.067269     
> gam(rsd~s(x3,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x3, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 1.238283     
> 
> rm(dat) 
> 
> ## More obvious, but more expensive tactic... Just increase 
> ## suspicious k until fit is stable.
> 
> set.seed(0) 
> dat <- gamSim(1,n=400,scale=2)
Gu & Wahba 4 term additive model
> ## fit a GAM with quite low `k'
> b <- gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=6)+s(x3,k=6),
+          data=dat,method="REML")
> b 

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, k = 6) + s(x1, k = 6) + s(x2, k = 6) + s(x3, k = 6)

Estimated degrees of freedom:
2.90 2.71 4.94 1.00  total = 12.55 

REML score: 892.3017     
> ## edf for 3rd smooth is highest as proportion of k -- increase k
> b <- gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=12)+s(x3,k=6),
+          data=dat,method="REML")
> b 

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, k = 6) + s(x1, k = 6) + s(x2, k = 12) + s(x3, k = 6)

Estimated degrees of freedom:
3.10 2.67 9.07 1.00  total = 16.84 

REML score: 887.9842     
> ## edf substantially up, -ve REML substantially down
> b <- gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=24)+s(x3,k=6),
+          data=dat,method="REML")
> b 

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, k = 6) + s(x1, k = 6) + s(x2, k = 24) + s(x3, k = 6)

Estimated degrees of freedom:
 3.12  2.67 11.32  1.00  total = 19.11 

REML score: 887.0976     
> ## slight edf increase and -ve REML change
> b <- gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=40)+s(x3,k=6),
+          data=dat,method="REML")
> b 

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, k = 6) + s(x1, k = 6) + s(x2, k = 40) + s(x3, k = 6)

Estimated degrees of freedom:
 3.12  2.67 11.54  1.00  total = 19.33 

REML score: 887.1258     
> ## defintely stabilized (but really k around 20 would have been fine)
> 
> 
> 
> 
> cleanEx()
> nameEx("columb")
> ### * columb
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: columb
> ### Title: Reduced version of Columbus OH crime data
> ### Aliases: columb columb.polys
> 
> ### ** Examples
> 
> ## see ?mrf help files
> 
> 
> 
> cleanEx()
> nameEx("concurvity")
> ### * concurvity
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: concurvity
> ### Title: GAM concurvity measures
> ### Aliases: concurvity
> 
> ### ** Examples
> 
> library(mgcv)
> ## simulate data with concurvity...
> set.seed(8);n<- 200
> f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 *
+             (10 * x)^3 * (1 - x)^10
> t <- sort(runif(n)) ## first covariate
> ## make covariate x a smooth function of t + noise...
> x <- f2(t) + rnorm(n)*3
> ## simulate response dependent on t and x...
> y <- sin(4*pi*t) + exp(x/20) + rnorm(n)*.3
> 
> ## fit model...
> b <- gam(y ~ s(t,k=15) + s(x,k=15),method="REML")
> 
> ## assess concurvity between each term and `rest of model'...
> concurvity(b)
                 para       s(t)      s(x)
worst    1.064436e-24 0.60269087 0.6026909
observed 1.064436e-24 0.09576829 0.5728602
estimate 1.064436e-24 0.24513981 0.4659564
> 
> ## ... and now look at pairwise concurvity between terms...
> concurvity(b,full=FALSE)
$worst
             para         s(t)         s(x)
para 1.000000e+00 7.313872e-26 8.950649e-25
s(t) 7.408676e-26 1.000000e+00 6.026909e-01
s(x) 8.983056e-25 6.026909e-01 1.000000e+00

$observed
             para         s(t)         s(x)
para 1.000000e+00 4.557228e-28 1.704959e-32
s(t) 7.408676e-26 1.000000e+00 5.728602e-01
s(x) 8.983056e-25 9.576829e-02 1.000000e+00

$estimate
             para         s(t)         s(x)
para 1.000000e+00 6.993809e-29 3.458685e-27
s(t) 7.408676e-26 1.000000e+00 4.659564e-01
s(x) 8.983056e-25 2.451398e-01 1.000000e+00

> 
> 
> 
> 
> cleanEx()
> nameEx("coxph")
> ### * coxph
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cox.ph
> ### Title: Additive Cox Proportional Hazard Model
> ### Aliases: cox.ph
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> library(survival) ## for data
> col1 <- colon[colon$etype==1,] ## concentrate on single event
> col1$differ <- as.factor(col1$differ)
> col1$sex <- as.factor(col1$sex)
> 
> b <- gam(time~s(age,by=sex)+sex+s(nodes)+perfor+rx+obstruct+adhere,
+          family=cox.ph(),data=col1,weights=status)
> 
> summary(b) 

Family: Cox PH 
Link function: identity 

Formula:
time ~ s(age, by = sex) + sex + s(nodes) + perfor + rx + obstruct + 
    adhere

Parametric coefficients:
          Estimate Std. Error z value Pr(>|z|)    
sex1      -0.11393    0.09464  -1.204   0.2287    
perfor     0.19153    0.25698   0.745   0.4561    
rxLev     -0.03564    0.10857  -0.328   0.7427    
rxLev+5FU -0.51258    0.12088  -4.240 2.23e-05 ***
obstruct   0.22242    0.11716   1.898   0.0576 .  
adhere     0.28372    0.12736   2.228   0.0259 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
              edf Ref.df Chi.sq p-value    
s(age):sex0 1.002  1.003  4.547  0.0331 *  
s(age):sex1 1.383  1.677  0.878  0.4298    
s(nodes)    3.134  3.895 93.725  <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Deviance explained = 7.29%
-REML = 2906.6  Scale est. = 1         n = 911
> 
> plot(b,pages=1,all.terms=TRUE) ## plot effects
> 
> plot(b$linear.predictors,residuals(b))
> 
> ## plot survival function for patient j...
> 
> np <- 300;j <- 6
> newd <- data.frame(time=seq(0,3000,length=np))
> dname <- names(col1)
> for (n in dname) newd[[n]] <- rep(col1[[n]][j],np)
> newd$time <- seq(0,3000,length=np)
> fv <- predict(b,newdata=newd,type="response",se=TRUE)
> plot(newd$time,fv$fit,type="l",ylim=c(0,1),xlab="time",ylab="survival")
> lines(newd$time,fv$fit+2*fv$se.fit,col=2)
> lines(newd$time,fv$fit-2*fv$se.fit,col=2)
> 
> ## crude plot of baseline survival...
> 
> plot(b$family$data$tr,exp(-b$family$data$h),type="l",ylim=c(0,1),
+      xlab="time",ylab="survival")
> lines(b$family$data$tr,exp(-b$family$data$h + 2*b$family$data$q^.5),col=2)
> lines(b$family$data$tr,exp(-b$family$data$h - 2*b$family$data$q^.5),col=2)
> lines(b$family$data$tr,exp(-b$family$data$km),lty=2) ## Kaplan Meier
> 
> ## Simple simulated known truth example...
> ph.weibull.sim <- function(eta,gamma=1,h0=.01,t1=100) { 
+   lambda <- h0*exp(eta)
+   n <- length(eta)
+   U <- runif(n)
+   t <- (-log(U)/lambda)^(1/gamma)
+   d <- as.numeric(t <= t1)
+   t[!d] <- t1
+   list(t=t,d=d)
+ }
> n <- 500;set.seed(2)
> x0 <- runif(n, 0, 1);x1 <- runif(n, 0, 1)
> x2 <- runif(n, 0, 1);x3 <- runif(n, 0, 1)
> f0 <- function(x) 2 * sin(pi * x)
> f1 <- function(x) exp(2 * x)
> f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
> f3 <- function(x) 0*x
> f <- f0(x0) + f1(x1) + f2(x2)
> g <- (f-mean(f))/5
> surv <- ph.weibull.sim(g)
> surv$x0 <- x0;surv$x1 <- x1;surv$x2 <- x2;surv$x3 <- x3
> 
> b <- gam(t~s(x0)+s(x1)+s(x2,k=15)+s(x3),family=cox.ph,weights=d,data=surv)
> 
> plot(b,pages=1)
> 
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("exclude.too.far")
> ### * exclude.too.far
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: exclude.too.far
> ### Title: Exclude prediction grid points too far from data
> ### Aliases: exclude.too.far
> ### Keywords: hplot
> 
> ### ** Examples
> 
> library(mgcv)
> x<-rnorm(100);y<-rnorm(100) # some "data"
> n<-40 # generate a grid....
> mx<-seq(min(x),max(x),length=n)
> my<-seq(min(y),max(y),length=n)
> gx<-rep(mx,n);gy<-rep(my,rep(n,n))
> tf<-exclude.too.far(gx,gy,x,y,0.1)
> plot(gx[!tf],gy[!tf],pch=".");points(x,y,col=2)
> 
> 
> 
> cleanEx()
> nameEx("extract.lme.cov")
> ### * extract.lme.cov
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: extract.lme.cov
> ### Title: Extract the data covariance matrix from an lme object
> ### Aliases: extract.lme.cov extract.lme.cov2
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## see also ?formXtViX for use of extract.lme.cov2
> require(mgcv)
> library(nlme)
> data(Rail)
> b <- lme(travel~1,Rail,~1|Rail)
> extract.lme.cov(b,Rail)
          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]
 [1,] 631.4778 615.3111 615.3111   0.0000   0.0000   0.0000   0.0000   0.0000
 [2,] 615.3111 631.4778 615.3111   0.0000   0.0000   0.0000   0.0000   0.0000
 [3,] 615.3111 615.3111 631.4778   0.0000   0.0000   0.0000   0.0000   0.0000
 [4,]   0.0000   0.0000   0.0000 631.4778 615.3111 615.3111   0.0000   0.0000
 [5,]   0.0000   0.0000   0.0000 615.3111 631.4778 615.3111   0.0000   0.0000
 [6,]   0.0000   0.0000   0.0000 615.3111 615.3111 631.4778   0.0000   0.0000
 [7,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000 631.4778 615.3111
 [8,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000 615.3111 631.4778
 [9,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000 615.3111 615.3111
[10,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[11,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[12,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[13,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[14,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[15,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[16,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[17,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[18,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]
 [1,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [2,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [3,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [4,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [5,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [6,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [7,] 615.3111   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [8,] 615.3111   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [9,] 631.4778   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[10,]   0.0000 631.4778 615.3111 615.3111   0.0000   0.0000   0.0000   0.0000
[11,]   0.0000 615.3111 631.4778 615.3111   0.0000   0.0000   0.0000   0.0000
[12,]   0.0000 615.3111 615.3111 631.4778   0.0000   0.0000   0.0000   0.0000
[13,]   0.0000   0.0000   0.0000   0.0000 631.4778 615.3111 615.3111   0.0000
[14,]   0.0000   0.0000   0.0000   0.0000 615.3111 631.4778 615.3111   0.0000
[15,]   0.0000   0.0000   0.0000   0.0000 615.3111 615.3111 631.4778   0.0000
[16,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000 631.4778
[17,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000 615.3111
[18,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000 615.3111
         [,17]    [,18]
 [1,]   0.0000   0.0000
 [2,]   0.0000   0.0000
 [3,]   0.0000   0.0000
 [4,]   0.0000   0.0000
 [5,]   0.0000   0.0000
 [6,]   0.0000   0.0000
 [7,]   0.0000   0.0000
 [8,]   0.0000   0.0000
 [9,]   0.0000   0.0000
[10,]   0.0000   0.0000
[11,]   0.0000   0.0000
[12,]   0.0000   0.0000
[13,]   0.0000   0.0000
[14,]   0.0000   0.0000
[15,]   0.0000   0.0000
[16,] 615.3111 615.3111
[17,] 631.4778 615.3111
[18,] 615.3111 631.4778
> 
> 
> 
> cleanEx()
> nameEx("fixDependence")
> ### * fixDependence
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fixDependence
> ### Title: Detect linear dependencies of one matrix on another
> ### Aliases: fixDependence
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> n<-20;c1<-4;c2<-7
> X1<-matrix(runif(n*c1),n,c1)
> X2<-matrix(runif(n*c2),n,c2)
> X2[,3]<-X1[,2]+X2[,4]*.1
> X2[,5]<-X1[,1]*.2+X1[,2]*.04
> fixDependence(X1,X2)
[1] 3 5
> fixDependence(X1,X2,strict=TRUE)
[1] 5
> 
> 
> 
> cleanEx()
> nameEx("formXtViX")
> ### * formXtViX
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: formXtViX
> ### Title: Form component of GAMM covariance matrix
> ### Aliases: formXtViX
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> library(nlme)
> data(ergoStool)
> b <- lme(effort ~ Type, data=ergoStool, random=~1|Subject)
> V1 <- extract.lme.cov(b, ergoStool)
> V2 <- extract.lme.cov2(b, ergoStool)
> X <- model.matrix(b, data=ergoStool)
> crossprod(formXtViX(V2, X))
            (Intercept)    TypeT2    TypeT3    TypeT4
(Intercept)    4.330827  1.082707  1.082707  1.082707
TypeT2         1.082707  5.846203 -1.587832 -1.587832
TypeT3         1.082707 -1.587832  5.846203 -1.587832
TypeT4         1.082707 -1.587832 -1.587832  5.846203
> t(X)
            1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
(Intercept) 1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
TypeT2      0 1 0 0 0 1 0 0 0  1  0  0  0  1  0  0  0  1  0  0  0  1  0  0  0
TypeT3      0 0 1 0 0 0 1 0 0  0  1  0  0  0  1  0  0  0  1  0  0  0  1  0  0
TypeT4      0 0 0 1 0 0 0 1 0  0  0  1  0  0  0  1  0  0  0  1  0  0  0  1  0
            26 27 28 29 30 31 32 33 34 35 36
(Intercept)  1  1  1  1  1  1  1  1  1  1  1
TypeT2       1  0  0  0  1  0  0  0  1  0  0
TypeT3       0  1  0  0  0  1  0  0  0  1  0
TypeT4       0  0  1  0  0  0  1  0  0  0  1
attr(,"assign")
[1] 0 1 1 1
attr(,"contrasts")
attr(,"contrasts")$Type
[1] "contr.treatment"

> 
> 
> 
> cleanEx()
> nameEx("fs.test")
> ### * fs.test
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fs.test
> ### Title: FELSPLINE test function
> ### Aliases: fs.test fs.boundary
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> ## plot the function, and its boundary...
> fsb <- fs.boundary()
> m<-300;n<-150 
> xm <- seq(-1,4,length=m);yn<-seq(-1,1,length=n)
> xx <- rep(xm,n);yy<-rep(yn,rep(m,n))
> tru <- matrix(fs.test(xx,yy),m,n) ## truth
> image(xm,yn,tru,col=heat.colors(100),xlab="x",ylab="y")
> lines(fsb$x,fsb$y,lwd=3)
> contour(xm,yn,tru,levels=seq(-5,5,by=.25),add=TRUE)
> 
> 
> 
> cleanEx()
> nameEx("gam")
> ### * gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gam
> ### Title: Generalized additive models with integrated smoothness
> ###   estimation
> ### Aliases: gam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## see also examples in ?gam.models (e.g. 'by' variables, 
> ## random effects and tricks for large binary datasets)
> 
> library(mgcv)
> set.seed(2) ## simulate some data... 
> dat <- gamSim(1,n=400,dist="normal",scale=2)
Gu & Wahba 4 term additive model
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.83328    0.09878    79.3   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df      F  p-value    
s(x0) 2.500  3.115  6.921 0.000129 ***
s(x1) 2.401  2.984 81.858  < 2e-16 ***
s(x2) 7.698  8.564 88.158  < 2e-16 ***
s(x3) 1.000  1.000  4.343 0.037806 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.715   Deviance explained = 72.5%
GCV = 4.0505  Scale est. = 3.9027    n = 400
> plot(b,pages=1,residuals=TRUE)  ## show partial residuals
> plot(b,pages=1,seWithMean=TRUE) ## `with intercept' CIs
> ## run some basic model checks, including checking
> ## smoothing basis dimensions...
> gam.check(b)

Method: GCV   Optimizer: magic
Smoothing parameter selection converged after 12 iterations.
The RMS GCV score gradiant at convergence was 1.739918e-07 .
The Hessian was positive definite.
The estimated model rank was 37 (maximum possible: 37)
Model rank =  37 / 37 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

         k'   edf k-index p-value
s(x0) 9.000 2.500   1.045    0.77
s(x1) 9.000 2.401   1.027    0.64
s(x2) 9.000 7.698   0.969    0.30
s(x3) 9.000 1.000   1.030    0.74
> 
> ## same fit in two parts .....
> G <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),fit=FALSE,data=dat)
> b <- gam(G=G)
> print(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.5 2.4 7.7 1.0  total = 14.6 

GCV score: 4.050519     
> 
> ## change the smoothness selection method to REML
> b0 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="REML")
> ## use alternative plotting scheme, and way intervals include
> ## smoothing parameter uncertainty...
> plot(b0,pages=1,scheme=1,unconditional=TRUE) 
> 
> ## Would a smooth interaction of x0 and x1 be better?
> ## Use tensor product smooth of x0 and x1, basis 
> ## dimension 49 (see ?te for details, also ?t2).
> bt <- gam(y~te(x0,x1,k=7)+s(x2)+s(x3),data=dat,
+           method="REML")
> plot(bt,pages=1) 
> plot(bt,pages=1,scheme=2) ## alternative visualization
> AIC(b0,bt) ## interaction worse than additive
         df      AIC
b0 17.68999 1698.504
bt 24.48943 1698.189
> 
> ## Alternative: test for interaction with a smooth ANOVA 
> ## decomposition (this time between x2 and x1)
> bt <- gam(y~s(x0)+s(x1)+s(x2)+s(x3)+ti(x1,x2,k=6),
+             data=dat,method="REML")
> summary(bt)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3) + ti(x1, x2, k = 6)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.83338    0.09883   79.26   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
            edf Ref.df      F  p-value    
s(x0)     3.013  3.744  6.226 0.000133 ***
s(x1)     2.840  3.524 69.568  < 2e-16 ***
s(x2)     8.012  8.736 85.971  < 2e-16 ***
s(x3)     1.000  1.000  4.176 0.041664 *  
ti(x1,x2) 1.006  1.011  0.087 0.775150    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.715   Deviance explained = 72.6%
-REML = 860.37  Scale est. = 3.9068    n = 400
> 
> ## If it is believed that x0 and x1 are naturally on 
> ## the same scale, and should be treated isotropically 
> ## then could try...
> bs <- gam(y~s(x0,x1,k=40)+s(x2)+s(x3),data=dat,
+           method="REML")
> plot(bs,pages=1)
> AIC(b0,bt,bs) ## additive still better. 
         df      AIC
b0 17.68999 1698.504
bt 18.79727 1700.592
bs 23.78889 1699.827
> 
> ## Now do automatic terms selection as well
> b1 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,
+        method="REML",select=TRUE)
> plot(b1,pages=1)
> 
> 
> ## set the smoothing parameter for the first term, estimate rest ...
> bp <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),sp=c(0.01,-1,-1,-1),data=dat)
> plot(bp,pages=1,scheme=1)
> ## alternatively...
> bp <- gam(y~s(x0,sp=.01)+s(x1)+s(x2)+s(x3),data=dat)
> 
> 
> # set lower bounds on smoothing parameters ....
> bp<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),
+         min.sp=c(0.001,0.01,0,10),data=dat) 
> print(b);print(bp)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.5 2.4 7.7 1.0  total = 14.6 

GCV score: 4.050519     

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.5 2.4 7.7 1.0  total = 14.6 

GCV score: 4.050519     
> 
> # same with REML
> bp<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),
+         min.sp=c(0.1,0.1,0,10),data=dat,method="REML") 
> print(b0);print(bp)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
3.02 2.84 8.02 1.00  total = 15.89 

REML score: 861.1296     

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
3.02 2.84 8.02 1.00  total = 15.88 

REML score: 861.1292     
> 
> 
> ## now a GAM with 3df regression spline term & 2 penalized terms
> 
> b0 <- gam(y~s(x0,k=4,fx=TRUE,bs="tp")+s(x1,k=12)+s(x2,k=15),data=dat)
> plot(b0,pages=1)
> 
> ## now simulate poisson data...
> dat <- gamSim(1,n=2000,dist="poisson",scale=.1)
Gu & Wahba 4 term additive model
> 
> ## use "cr" basis to save time, with 2000 data...
> b2<-gam(y~s(x0,bs="cr")+s(x1,bs="cr")+s(x2,bs="cr")+
+         s(x3,bs="cr"),family=poisson,data=dat,method="REML")
> plot(b2,pages=1)
> 
> ## drop x3, but initialize sp's from previous fit, to 
> ## save more time...
> 
> b2a<-gam(y~s(x0,bs="cr")+s(x1,bs="cr")+s(x2,bs="cr"),
+          family=poisson,data=dat,method="REML",
+          in.out=list(sp=b2$sp[1:3],scale=1))
> par(mfrow=c(2,2))
> plot(b2a)
> 
> par(mfrow=c(1,1))
> ## similar example using performance iteration
> dat <- gamSim(1,n=400,dist="poisson",scale=.25)
Gu & Wahba 4 term additive model
> 
> b3<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=poisson,
+         data=dat,optimizer="perf")
> plot(b3,pages=1)
> 
> ## repeat using GACV as in Wood 2008...
> 
> b4<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=poisson,
+         data=dat,method="GACV.Cp",scale=-1)
> plot(b4,pages=1)
> 
> ## repeat using REML as in Wood 2011...
> 
> b5<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=poisson,
+         data=dat,method="REML")
> plot(b5,pages=1)
> 
>  
> ## a binary example (see ?gam.models for large dataset version)...
> 
> dat <- gamSim(1,n=400,dist="binary",scale=.33)
Gu & Wahba 4 term additive model
> 
> lr.fit <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=binomial,
+               data=dat,method="REML")
> 
> ## plot model components with truth overlaid in red
> op <- par(mfrow=c(2,2))
> fn <- c("f0","f1","f2","f3");xn <- c("x0","x1","x2","x3")
> for (k in 1:4) {
+   plot(lr.fit,residuals=TRUE,select=k)
+   ff <- dat[[fn[k]]];xx <- dat[[xn[k]]]
+   ind <- sort.int(xx,index.return=TRUE)$ix
+   lines(xx[ind],(ff-mean(ff))[ind]*.33,col=2)
+ }
> par(op)
> anova(lr.fit)

Family: binomial 
Link function: logit 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Approximate significance of smooth terms:
        edf Ref.df Chi.sq  p-value
s(x0) 2.114  2.635  3.948 0.302533
s(x1) 2.267  2.826 19.929 0.000161
s(x2) 5.889  7.032 49.115 2.31e-08
s(x3) 1.950  2.435  3.118 0.238012
> lr.fit1 <- gam(y~s(x0)+s(x1)+s(x2),family=binomial,
+                data=dat,method="REML")
> lr.fit2 <- gam(y~s(x1)+s(x2),family=binomial,
+                data=dat,method="REML")
> AIC(lr.fit,lr.fit1,lr.fit2)
              df      AIC
lr.fit  15.92791 429.0081
lr.fit1 13.46775 428.7898
lr.fit2 10.55142 427.6818
> 
> ## For a Gamma example, see ?summary.gam...
> 
> ## For inverse Gaussian, see ?rig
> 
> ## now 2D smoothing...
> 
> eg <- gamSim(2,n=500,scale=.1)
Bivariate smoothing example
> attach(eg)
> 
> op <- par(mfrow=c(2,2),mar=c(4,4,1,1))
> 
> contour(truth$x,truth$z,truth$f) ## contour truth
> b4 <- gam(y~s(x,z),data=data) ## fit model
> fit1 <- matrix(predict.gam(b4,pr,se=FALSE),40,40)
> contour(truth$x,truth$z,fit1)   ## contour fit
> persp(truth$x,truth$z,truth$f)    ## persp truth
> vis.gam(b4)                     ## persp fit
> detach(eg)
> par(op)
> 
> ## Not run: 
> ##D ##################################################
> ##D ## largish dataset example with user defined knots
> ##D ##################################################
> ##D 
> ##D par(mfrow=c(2,2))
> ##D n <- 5000
> ##D eg <- gamSim(2,n=n,scale=.5)
> ##D attach(eg)
> ##D 
> ##D ind<-sample(1:n,200,replace=FALSE)
> ##D b5<-gam(y~s(x,z,k=40),data=data,
> ##D         knots=list(x=data$x[ind],z=data$z[ind]))
> ##D ## various visualizations
> ##D vis.gam(b5,theta=30,phi=30)
> ##D plot(b5)
> ##D plot(b5,scheme=1,theta=50,phi=20)
> ##D plot(b5,scheme=2)
> ##D 
> ##D par(mfrow=c(1,1))
> ##D ## and a pure "knot based" spline of the same data
> ##D b6<-gam(y~s(x,z,k=64),data=data,knots=list(x= rep((1:8-0.5)/8,8),
> ##D         z=rep((1:8-0.5)/8,rep(8,8))))
> ##D vis.gam(b6,color="heat",theta=30,phi=30)
> ##D 
> ##D ## varying the default large dataset behaviour via `xt'
> ##D b7 <- gam(y~s(x,z,k=40,xt=list(max.knots=500,seed=2)),data=data)
> ##D vis.gam(b7,theta=30,phi=30)
> ##D detach(eg)
> ## End(Not run)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("gam.check")
> ### * gam.check
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gam.check
> ### Title: Some diagnostics for a fitted gam model
> ### Aliases: gam.check
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(0)
> dat <- gamSim(1,n=200)
Gu & Wahba 4 term additive model
> b<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
> plot(b,pages=1)
> gam.check(b,pch=19,cex=.3)

Method: GCV   Optimizer: magic
Smoothing parameter selection converged after 8 iterations.
The RMS GCV score gradiant at convergence was 1.072609e-05 .
The Hessian was positive definite.
The estimated model rank was 37 (maximum possible: 37)
Model rank =  37 / 37 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

         k'   edf k-index p-value
s(x0) 9.000 2.318   0.996    0.45
s(x1) 9.000 2.306   0.969    0.35
s(x2) 9.000 7.655   0.961    0.25
s(x3) 9.000 1.233   1.037    0.68
> 
> 
> 
> cleanEx()
> nameEx("gam.models")
> ### * gam.models
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gam.models
> ### Title: Specifying generalized additive models
> ### Aliases: gam.models
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv)
> set.seed(10)
> ## simulate date from y = f(x2)*x1 + error
> dat <- gamSim(3,n=400)
Continuous `by' variable example
> 
> b<-gam(y ~ s(x2,by=x1),data=dat)
> plot(b,pages=1)
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x2, by = x1)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  -0.1317     0.2128  -0.619    0.536

Approximate significance of smooth terms:
           edf Ref.df     F p-value    
s(x2):x1 9.199  9.822 32.29  <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.439   Deviance explained = 45.2%
GCV = 4.5182  Scale est. = 4.403     n = 400
> 
> ## Factor `by' variable example (with a spurious covariate x0)
> ## simulate data...
> 
> dat <- gamSim(4)
Factor `by' variable example
> 
> ## fit model...
> b <- gam(y ~ fac+s(x2,by=fac)+s(x0),data=dat)
> plot(b,pages=1)
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ fac + s(x2, by = fac) + s(x0)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   1.2423     0.1859   6.684 8.24e-11 ***
fac2         -1.3863     0.2598  -5.336 1.63e-07 ***
fac3          2.0069     0.2698   7.438 6.80e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
             edf Ref.df      F p-value    
s(x2):fac1 2.152  2.687  2.193  0.0787 .  
s(x2):fac2 2.069  2.572 40.878  <2e-16 ***
s(x2):fac3 7.763  8.608 20.803  <2e-16 ***
s(x0)      2.982  3.712  1.017  0.3304    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.529   Deviance explained = 54.9%
GCV = 4.7024  Scale est. = 4.4912    n = 400
> 
> ## note that the preceding fit is the same as....
> b1<-gam(y ~ s(x2,by=as.numeric(fac==1))+s(x2,by=as.numeric(fac==2))+
+             s(x2,by=as.numeric(fac==3))+s(x0)-1,data=dat)
> ## ... the `-1' is because the intercept is confounded with the 
> ## *uncentred* smooths here.
> plot(b1,pages=1)
> summary(b1)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x2, by = as.numeric(fac == 1)) + s(x2, by = as.numeric(fac == 
    2)) + s(x2, by = as.numeric(fac == 3)) + s(x0) - 1

Approximate significance of smooth terms:
                             edf Ref.df      F  p-value    
s(x2):as.numeric(fac == 1) 3.152  3.687 14.838 2.83e-10 ***
s(x2):as.numeric(fac == 2) 3.069  3.572 30.707  < 2e-16 ***
s(x2):as.numeric(fac == 3) 8.763  9.608 44.954  < 2e-16 ***
s(x0)                      2.982  3.712  1.017     0.33    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.529   Deviance explained = 61.4%
GCV = 4.7024  Scale est. = 4.4912    n = 400
> 
> ## repeat forcing all s(x2) terms to have the same smoothing param
> ## (not a very good idea for these data!)
> b2 <- gam(y ~ fac+s(x2,by=fac,id=1)+s(x0),data=dat)
> plot(b2,pages=1)
> summary(b2)

Family: gaussian 
Link function: identity 

Formula:
y ~ fac + s(x2, by = fac, id = 1) + s(x0)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   1.2233     0.1884   6.493 2.67e-10 ***
fac2         -1.3889     0.2626  -5.288 2.11e-07 ***
fac3          2.0482     0.2710   7.559 3.16e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
             edf Ref.df      F p-value    
s(x2):fac1 6.697  7.798  1.463   0.171    
s(x2):fac2 6.771  7.881 13.897  <2e-16 ***
s(x2):fac3 6.662  7.778 22.009  <2e-16 ***
s(x0)      2.751  3.424  0.975   0.357    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.527   Deviance explained = 55.6%
GCV =  4.827  Scale est. = 4.5147    n = 400
> 
> ## now repeat with a single reference level smooth, and 
> ## two `difference' smooths...
> dat$fac <- ordered(dat$fac)
> b3 <- gam(y ~ fac+s(x2)+s(x2,by=fac)+s(x0),data=dat,method="REML")
> plot(b3,pages=1)
> summary(b3)

Family: gaussian 
Link function: identity 

Formula:
y ~ fac + s(x2) + s(x2, by = fac) + s(x0)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   1.4502     0.1081  13.412  < 2e-16 ***
fac.L         1.4276     0.1908   7.481 5.13e-13 ***
fac.Q         1.9544     0.1846  10.589  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
             edf Ref.df      F  p-value    
s(x2)      2.381  2.965  1.962    0.139    
s(x2):fac2 2.800  3.479 18.191 4.93e-12 ***
s(x2):fac3 7.340  8.324 13.790  < 2e-16 ***
s(x0)      2.338  2.919  0.872    0.371    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.527   Deviance explained = 54.7%
-REML =  886.7  Scale est. = 4.512     n = 400
> 
> 
> rm(dat)
> 
> ## An example of a simple random effects term implemented via 
> ## penalization of the parametric part of the model...
> 
> dat <- gamSim(1,n=400,scale=2) ## simulate 4 term additive truth
Gu & Wahba 4 term additive model
> ## Now add some random effects to the simulation. Response is 
> ## grouped into one of 20 groups by `fac' and each groups has a
> ## random effect added....
> fac <- as.factor(sample(1:20,400,replace=TRUE))
> dat$X <- model.matrix(~fac-1)
> b <- rnorm(20)*.5
> dat$y <- dat$y + dat$X%*%b
> 
> ## now fit appropriate random effect model...
> PP <- list(X=list(rank=20,diag(20)))
> rm <- gam(y~ X+s(x0)+s(x1)+s(x2)+s(x3),data=dat,paraPen=PP)
> plot(rm,pages=1)
> ## Get estimated random effects standard deviation...
> sig.b <- sqrt(rm$sig2/rm$sp[1]);sig.b 
        X 
0.4030462 
> 
> ## a much simpler approach uses "re" terms...
> 
> rm1 <- gam(y ~ s(fac,bs="re")+s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="ML")
> gam.vcomp(rm1)

Standard deviations and 0.95 confidence intervals:

           std.dev        lower        upper
s(fac)  0.38798122 1.736750e-01 8.667303e-01
s(x0)  10.59231394 3.074173e+00 3.649669e+01
s(x1)   6.16405260 2.245159e+00 1.692332e+01
s(x2)  96.69250817 5.419487e+01 1.725152e+02
s(x3)   0.02572732 5.703663e-31 1.160473e+27
scale   2.12620981 1.976631e+00 2.287108e+00

Rank: 6/6
> 
> ## Simple comparison with lme, using Rail data. 
> ## See ?random.effects for a simpler method
> require(nlme)
> b0 <- lme(travel~1,data=Rail,~1|Rail,method="ML") 
> Z <- model.matrix(~Rail-1,data=Rail,
+      contrasts.arg=list(Rail="contr.treatment"))
> b <- gam(travel~Z,data=Rail,paraPen=list(Z=list(diag(6))),method="ML")
> 
> b0 
Linear mixed-effects model fit by maximum likelihood
  Data: Rail 
  Log-likelihood: -64.28002
  Fixed: travel ~ 1 
(Intercept) 
       66.5 

Random effects:
 Formula: ~1 | Rail
        (Intercept) Residual
StdDev:    22.62435 4.020779

Number of Observations: 18
Number of Groups: 6 
> (b$reml.scale/b$sp)^.5 ## `gam' ML estimate of Rail sd
       Z 
22.62435 
> b$reml.scale^.5         ## `gam' ML estimate of residual sd
[1] 4.020779
> 
> b0 <- lme(travel~1,data=Rail,~1|Rail,method="REML") 
> Z <- model.matrix(~Rail-1,data=Rail,
+      contrasts.arg=list(Rail="contr.treatment"))
> b <- gam(travel~Z,data=Rail,paraPen=list(Z=list(diag(6))),method="REML")
> 
> b0 
Linear mixed-effects model fit by REML
  Data: Rail 
  Log-restricted-likelihood: -61.0885
  Fixed: travel ~ 1 
(Intercept) 
       66.5 

Random effects:
 Formula: ~1 | Rail
        (Intercept) Residual
StdDev:    24.80547 4.020779

Number of Observations: 18
Number of Groups: 6 
> (b$reml.scale/b$sp)^.5 ## `gam' REML estimate of Rail sd
       Z 
24.80547 
> b$reml.scale^.5         ## `gam' REML estimate of residual sd
[1] 4.020779
> 
> ################################################################
> ## Approximate large dataset logistic regression for rare events
> ## based on subsampling the zeroes, and adding an offset to
> ## approximately allow for this.
> ## Doing the same thing, but upweighting the sampled zeroes
> ## leads to problems with smoothness selection, and CIs.
> ################################################################
> n <- 50000  ## simulate n data 
> dat <- gamSim(1,n=n,dist="binary",scale=.33)
Gu & Wahba 4 term additive model
> p <- binomial()$linkinv(dat$f-6) ## make 1's rare
> dat$y <- rbinom(p,1,p)      ## re-simulate rare response
> 
> ## Now sample all the 1's but only proportion S of the 0's
> S <- 0.02                   ## sampling fraction of zeroes
> dat <- dat[dat$y==1 | runif(n) < S,] ## sampling
> 
> ## Create offset based on total sampling fraction
> dat$s <- rep(log(nrow(dat)/n),nrow(dat))
> 
> lr.fit <- gam(y~s(x0,bs="cr")+s(x1,bs="cr")+s(x2,bs="cr")+s(x3,bs="cr")+
+               offset(s),family=binomial,data=dat,method="REML")
> 
> ## plot model components with truth overlaid in red
> op <- par(mfrow=c(2,2))
> fn <- c("f0","f1","f2","f3");xn <- c("x0","x1","x2","x3")
> for (k in 1:4) {
+        plot(lr.fit,select=k,scale=0)
+        ff <- dat[[fn[k]]];xx <- dat[[xn[k]]]
+        ind <- sort.int(xx,index.return=TRUE)$ix
+        lines(xx[ind],(ff-mean(ff))[ind]*.33,col=2)
+ }
> par(op)
> rm(dat)
> 
> ## A Gamma example, by modify `gamSim' output...
>  
> dat <- gamSim(1,n=400,dist="normal",scale=1)
Gu & Wahba 4 term additive model
> dat$f <- dat$f/4 ## true linear predictor 
> Ey <- exp(dat$f);scale <- .5 ## mean and GLM scale parameter
> ## Note that `shape' and `scale' in `rgamma' are almost
> ## opposite terminology to that used with GLM/GAM...
> dat$y <- rgamma(Ey*0,shape=1/scale,scale=Ey*scale)
> bg <- gam(y~ s(x0)+ s(x1)+s(x2)+s(x3),family=Gamma(link=log),
+           data=dat,method="REML")
> plot(bg,pages=1,scheme=1)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("gam.selection")
> ### * gam.selection
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gam.selection
> ### Title: Generalized Additive Model Selection
> ### Aliases: gam.selection
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## an example of automatic model selection via null space penalization
> library(mgcv)
> set.seed(3);n<-200
> dat <- gamSim(1,n=n,scale=.15,dist="poisson") ## simulate data
Gu & Wahba 4 term additive model
> dat$x4 <- runif(n, 0, 1);dat$x5 <- runif(n, 0, 1) ## spurious
> 
> b<-gam(y~s(x0)+s(x1)+s(x2)+s(x3)+s(x4)+s(x5),data=dat,
+          family=poisson,select=TRUE,method="REML")
> summary(b)

Family: poisson 
Link function: log 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3) + s(x4) + s(x5)

Parametric coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  1.21758    0.04082   29.83   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
            edf Ref.df  Chi.sq p-value    
s(x0) 1.7655119      9   5.264  0.0397 *  
s(x1) 1.9271039      9  65.356  <2e-16 ***
s(x2) 6.1351372      9 156.204  <2e-16 ***
s(x3) 0.0002618      9   0.000  0.4088    
s(x4) 0.0002766      9   0.000  1.0000    
s(x5) 0.1757146      9   0.195  0.2963    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.545   Deviance explained = 51.6%
-REML = 430.78  Scale est. = 1         n = 200
> plot(b,pages=1)
> 
> 
> 
> cleanEx()
> nameEx("gam.side")
> ### * gam.side
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gam.side
> ### Title: Identifiability side conditions for a GAM
> ### Aliases: gam.side
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## The first two examples here iluustrate models that cause
> ## gam.side to impose constraints, but both are a bad way 
> ## of estimating such models. The 3rd example is the right
> ## way.... 
> set.seed(7)
> require(mgcv)
> dat <- gamSim(n=400,scale=2) ## simulate data
Gu & Wahba 4 term additive model
> ## estimate model with redundant smooth interaction (bad idea).
> b<-gam(y~s(x0)+s(x1)+s(x0,x1)+s(x2),data=dat)
> plot(b,pages=1)
> 
> ## Simulate data with real interation...
> dat <- gamSim(2,n=500,scale=.1)
Bivariate smoothing example
> old.par<-par(mfrow=c(2,2))
> 
> ## a fully nested tensor product example (bad idea)
> b <- gam(y~s(x,bs="cr",k=6)+s(z,bs="cr",k=6)+te(x,z,k=6),
+        data=dat$data)
> plot(b)
> 
> old.par<-par(mfrow=c(2,2))
> ## A fully nested tensor product example, done properly,
> ## so that gam.side is not needed to ensure identifiability.
> ## ti terms are designed to produce interaction smooths
> ## suitable for adding to main effects (we could also have
> ## used s(x) and s(z) without a problem, but not s(z,x) 
> ## or te(z,x)).
> b <- gam(y ~ ti(x,k=6) + ti(z,k=6) + ti(x,z,k=6),
+        data=dat$data)
> plot(b)
> 
> par(old.par)
> rm(dat)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("gam.vcomp")
> ### * gam.vcomp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gam.vcomp
> ### Title: Report gam smoothness estimates as variance components
> ### Aliases: gam.vcomp
> ### Keywords: models smooth regression
> 
> ### ** Examples
>  
>   set.seed(3) 
>   require(mgcv)
>   ## simulate some data, consisting of a smooth truth + random effects
> 
>   dat <- gamSim(1,n=400,dist="normal",scale=2)
Gu & Wahba 4 term additive model
>   a <- factor(sample(1:10,400,replace=TRUE))
>   b <- factor(sample(1:7,400,replace=TRUE))
>   Xa <- model.matrix(~a-1)    ## random main effects
>   Xb <-  model.matrix(~b-1)
>   Xab <- model.matrix(~a:b-1) ## random interaction
>   dat$y <- dat$y + Xa%*%rnorm(10)*.5 + 
+            Xb%*%rnorm(7)*.3 + Xab%*%rnorm(70)*.7
>   dat$a <- a;dat$b <- b
> 
>   ## Fit the model using "re" terms, and smoother linkage  
>   
>   mod <- gam(y~s(a,bs="re")+s(b,bs="re")+s(a,b,bs="re")+s(x0,id=1)+s(x1,id=1)+
+                s(x2,k=15)+s(x3),data=dat,method="ML")
> 
>   gam.vcomp(mod) 

Standard deviations and 0.95 confidence intervals:

           std.dev        lower        upper
s(a)    0.66503790 3.572538e-01 1.237987e+00
s(b)    0.10546837 1.505697e-04 7.387660e+01
s(a,b)  0.60340909 3.047108e-01 1.194912e+00
s(x0)   7.92402527 4.311082e+00 1.456483e+01
s(x2)  86.44110460 5.072838e+01 1.472956e+02
s(x3)   0.02863398 5.871342e-31 1.396452e+27
scale   2.06350202 1.905839e+00 2.234208e+00

Rank: 7/7

All smooth components:
       s(a)        s(b)      s(a,b)       s(x0)       s(x1)       s(x2) 
 0.66503790  0.10546837  0.60340909  7.92402527  7.92402527 86.44110460 
      s(x3) 
 0.02863398 
> 
> 
> 
> 
> cleanEx()
> nameEx("gamSim")
> ### * gamSim
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gamSim
> ### Title: Simulate example data for GAMs
> ### Aliases: gamSim
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## see ?gam
> 
> 
> 
> cleanEx()
> nameEx("gamm")
> ### * gamm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gamm
> ### Title: Generalized Additive Mixed Models
> ### Aliases: gamm
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## simple examples using gamm as alternative to gam
> set.seed(0) 
> dat <- gamSim(1,n=200,scale=2)
Gu & Wahba 4 term additive model
> b <- gamm(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
> plot(b$gam,pages=1)
> summary(b$lme) # details of underlying lme fit
Linear mixed-effects model fit by maximum likelihood
 Data: strip.offset(mf) 
       AIC      BIC    logLik
  929.4264 962.4096 -454.7132

Random effects:
 Formula: ~Xr - 1 | g
 Structure: pdIdnot
             Xr1      Xr2      Xr3      Xr4      Xr5      Xr6      Xr7      Xr8
StdDev: 1.832454 1.832454 1.832454 1.832454 1.832454 1.832454 1.832454 1.832454

 Formula: ~Xr.0 - 1 | g.0 %in% g
 Structure: pdIdnot
           Xr.01    Xr.02    Xr.03    Xr.04    Xr.05    Xr.06    Xr.07    Xr.08
StdDev: 1.517091 1.517091 1.517091 1.517091 1.517091 1.517091 1.517091 1.517091

 Formula: ~Xr.1 - 1 | g.1 %in% g.0 %in% g
 Structure: pdIdnot
           Xr.11    Xr.12    Xr.13    Xr.14    Xr.15    Xr.16    Xr.17    Xr.18
StdDev: 24.29583 24.29583 24.29583 24.29583 24.29583 24.29583 24.29583 24.29583

 Formula: ~Xr.2 - 1 | g.2 %in% g.1 %in% g.0 %in% g
 Structure: pdIdnot
               Xr.21        Xr.22        Xr.23        Xr.24        Xr.25
StdDev: 0.0001638801 0.0001638801 0.0001638801 0.0001638801 0.0001638801
               Xr.26        Xr.27        Xr.28 Residual
StdDev: 0.0001638801 0.0001638801 0.0001638801 2.142508

Fixed effects: y.0 ~ X - 1 
                 Value Std.Error  DF  t-value p-value
X(Intercept)  7.318201 0.1534282 195 47.69790  0.0000
Xs(x0)Fx1    -0.152108 0.6446780 195 -0.23594  0.8137
Xs(x1)Fx1     1.858918 0.5380987 195  3.45460  0.0007
Xs(x2)Fx1     4.576313 2.8309536 195  1.61653  0.1076
Xs(x3)Fx1     0.071710 0.1572356 195  0.45607  0.6488
 Correlation: 
          X(Int) X(0)F1 X(1)F1 X(2)F1
Xs(x0)Fx1  0.000                     
Xs(x1)Fx1  0.000 -0.009              
Xs(x2)Fx1  0.000  0.025 -0.031       
Xs(x3)Fx1  0.000 -0.052  0.023 -0.064

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.63556106 -0.66952304 -0.05179306  0.65709885  2.25213794 

Number of Observations: 200
Number of Groups: 
                           g                   g.0 %in% g 
                           1                            1 
         g.1 %in% g.0 %in% g g.2 %in% g.1 %in% g.0 %in% g 
                           1                            1 
> summary(b$gam) # gam style summary of fitted model

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.3182     0.1519   48.19   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df      F p-value    
s(x0) 2.489  2.489  5.266 0.00549 ** 
s(x1) 2.209  2.209 56.511 < 2e-16 ***
s(x2) 7.305  7.305 36.985 < 2e-16 ***
s(x3) 1.000  1.000  0.212 0.64554    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.686   
  Scale est. = 4.5903    n = 200
> anova(b$gam) 

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Approximate significance of smooth terms:
        edf Ref.df      F p-value
s(x0) 2.489  2.489  5.266 0.00549
s(x1) 2.209  2.209 56.511 < 2e-16
s(x2) 7.305  7.305 36.985 < 2e-16
s(x3) 1.000  1.000  0.212 0.64554
> gam.check(b$gam) # simple checking plots
> 
> b <- gamm(y~te(x0,x1)+s(x2)+s(x3),data=dat) 
> op <- par(mfrow=c(2,2))
> plot(b$gam)
> par(op)
> rm(dat)
> 
> ## Add a factor to the linear predictor, to be modelled as random
> dat <- gamSim(6,n=200,scale=.2,dist="poisson")
4 term additive + random effectGu & Wahba 4 term additive model
> b2<-gamm(y~s(x0)+s(x1)+s(x2),family=poisson,
+          data=dat,random=list(fac=~1))

 Maximum number of PQL iterations:  20 
iteration 1
iteration 2
iteration 3
iteration 4
iteration 5
> plot(b2$gam,pages=1)
> fac <- dat$fac
> rm(dat)
> vis.gam(b2$gam)
> 
> 
> ## now an example with autocorrelated errors....
> n <- 200;sig <- 2
> x <- 0:(n-1)/(n-1)
> f <- 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
> e <- rnorm(n,0,sig)
> for (i in 2:n) e[i] <- 0.6*e[i-1] + e[i]
> y <- f + e
> op <- par(mfrow=c(2,2))
> ## Fit model with AR1 residuals
> b <- gamm(y~s(x,k=20),correlation=corAR1())
> plot(b$gam);lines(x,f-mean(f),col=2)
> ## Raw residuals still show correlation, of course...
> acf(residuals(b$gam),main="raw residual ACF")
> ## But standardized are now fine...
> acf(residuals(b$lme,type="normalized"),main="standardized residual ACF")
> ## compare with model without AR component...
> b <- gam(y~s(x,k=20))
> plot(b);lines(x,f-mean(f),col=2)
> 
> ## more complicated autocorrelation example - AR errors
> ## only within groups defined by `fac'
> e <- rnorm(n,0,sig)
> for (i in 2:n) e[i] <- 0.6*e[i-1]*(fac[i-1]==fac[i]) + e[i]
> y <- f + e
> b <- gamm(y~s(x,k=20),correlation=corAR1(form=~1|fac))
> plot(b$gam);lines(x,f-mean(f),col=2)
> par(op) 
> 
> ## more complex situation with nested random effects and within
> ## group correlation 
> 
> set.seed(0)
> n.g <- 10
> n<-n.g*10*4
> ## simulate smooth part...
> dat <- gamSim(1,n=n,scale=2)
Gu & Wahba 4 term additive model
> f <- dat$f
> ## simulate nested random effects....
> fa <- as.factor(rep(1:10,rep(4*n.g,10)))
> ra <- rep(rnorm(10),rep(4*n.g,10))
> fb <- as.factor(rep(rep(1:4,rep(n.g,4)),10))
> rb <- rep(rnorm(4),rep(n.g,4))
> for (i in 1:9) rb <- c(rb,rep(rnorm(4),rep(n.g,4)))
> ## simulate auto-correlated errors within groups
> e<-array(0,0)
> for (i in 1:40) {
+   eg <- rnorm(n.g, 0, sig)
+   for (j in 2:n.g) eg[j] <- eg[j-1]*0.6+ eg[j]
+   e<-c(e,eg)
+ }
> dat$y <- f + ra + rb + e
> dat$fa <- fa;dat$fb <- fb
> ## fit model .... 
> b <- gamm(y~s(x0,bs="cr")+s(x1,bs="cr")+s(x2,bs="cr")+
+   s(x3,bs="cr"),data=dat,random=list(fa=~1,fb=~1),
+   correlation=corAR1())
> plot(b$gam,pages=1)
> summary(b$gam)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = "cr") + s(x1, bs = "cr") + s(x2, bs = "cr") + 
    s(x3, bs = "cr")

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.7756     0.4413   17.62   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df       F  p-value    
s(x0) 3.393  3.393  11.033 2.55e-07 ***
s(x1) 2.757  2.757 127.438  < 2e-16 ***
s(x2) 8.006  8.006 111.492  < 2e-16 ***
s(x3) 1.000  1.000   0.203    0.652    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.567   
  Scale est. = 4.9578    n = 400
> vis.gam(b$gam)
> 
> ## Prediction from gam object, optionally adding 
> ## in random effects. 
> 
> ## Extract random effects and make names more convenient...
> refa <- ranef(b$lme,level=5)
> rownames(refa) <- substr(rownames(refa),start=9,stop=20)
> refb <- ranef(b$lme,level=6)
> rownames(refb) <- substr(rownames(refb),start=9,stop=20)
> 
> ## make a prediction, with random effects zero...
> p0 <- predict(b$gam,data.frame(x0=.3,x1=.6,x2=.98,x3=.77))
> 
> ## add in effect for fa = "2" and fb="2/4"...
> p <- p0 + refa["2",1] + refb["2/4",1] 
> 
> ## and a "spatial" example...
> library(nlme);set.seed(1);n <- 100
> dat <- gamSim(2,n=n,scale=0) ## standard example
Bivariate smoothing example
> attach(dat)
> old.par<-par(mfrow=c(2,2))
> contour(truth$x,truth$z,truth$f)  ## true function
> f <- data$f                       ## true expected response
> ## Now simulate correlated errors...
> cstr <- corGaus(.1,form = ~x+z)  
> cstr <- Initialize(cstr,data.frame(x=data$x,z=data$z))
> V <- corMatrix(cstr) ## correlation matrix for data
> Cv <- chol(V)
> e <- t(Cv) %*% rnorm(n)*0.05 # correlated errors
> ## next add correlated simulated errors to expected values
> data$y <- f + e ## ... to produce response
> b<- gamm(y~s(x,z,k=50),correlation=corGaus(.1,form=~x+z),
+          data=data)
> plot(b$gam) # gamm fit accounting for correlation
> # overfits when correlation ignored.....  
> b1 <- gamm(y~s(x,z,k=50),data=data);plot(b1$gam) 
> b2 <- gam(y~s(x,z,k=50),data=data);plot(b2)
> par(old.par)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()

detaching ‘dat’

> nameEx("gaulss")
> ### * gaulss
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gaulss
> ### Title: Gaussian location-scale model family
> ### Aliases: gaulss
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv);library(MASS)
> b <- gam(list(accel~s(times,k=20,bs="ad"),~s(times)),
+             data=mcycle,family=gaulss())
> summary(b) 

Family: gaulss 
Link function: identity logb 

Formula:
accel ~ s(times, k = 20, bs = "ad")
~s(times)

Parametric coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)   -25.3249     1.8530  -13.67   <2e-16 ***
(Intercept).1   2.6131     0.0632   41.35   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
              edf Ref.df Chi.sq p-value    
s(times)   10.584 11.951  548.1  <2e-16 ***
s.1(times)  7.265  8.233  272.7  <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Deviance explained = 98.7%
-REML = 580.04  Scale est. = 1         n = 133
> plot(b,pages=1,scale=0)
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’

> nameEx("get.var")
> ### * get.var
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: get.var
> ### Title: Get named variable or evaluate expression from list or
> ###   data.frame
> ### Aliases: get.var
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> y <- 1:4;dat<-data.frame(x=5:10)
> get.var("x",dat)
[1]  5  6  7  8  9 10
> get.var("y",dat)
NULL
> get.var("x==6",dat)
NULL
> dat <- list(X=matrix(1:6,3,2))
> get.var("X",dat)
[1] 1 2 3 4 5 6
attr(,"matrix")
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("in.out")
> ### * in.out
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: in.out
> ### Title: Which of a set of points lie within a polygon defined region
> ### Aliases: in.out
> 
> ### ** Examples
> 
> library(mgcv)
> data(columb.polys)
> bnd <- columb.polys[[2]]
> plot(bnd,type="n")
> polygon(bnd)
> x <- seq(7.9,8.7,length=20)
> y <- seq(13.7,14.3,length=20)
> gr <- as.matrix(expand.grid(x,y))
> inside <- in.out(bnd,gr)
> points(gr,col=as.numeric(inside)+1)
> 
> 
> 
> cleanEx()
> nameEx("inSide")
> ### * inSide
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: inSide
> ### Title: Are points inside boundary?
> ### Aliases: inSide
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> m <- 300;n <- 150
> xm <- seq(-1,4,length=m);yn<-seq(-1,1,length=n)
> x <- rep(xm,n);y<-rep(yn,rep(m,n))
> er <- matrix(fs.test(x,y),m,n)
> bnd <- fs.boundary()
> in.bnd <- inSide(bnd,x,y)
> plot(x,y,col=as.numeric(in.bnd)+1,pch=".")
> lines(bnd$x,bnd$y,col=3)
> points(x,y,col=as.numeric(in.bnd)+1,pch=".")
> ## check boundary details ...
> plot(x,y,col=as.numeric(in.bnd)+1,pch=".",ylim=c(-1,0),xlim=c(3,3.5))
> lines(bnd$x,bnd$y,col=3)
> points(x,y,col=as.numeric(in.bnd)+1,pch=".")
> 
> 
> 
> 
> cleanEx()
> nameEx("jagam")
> ### * jagam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: jagam
> ### Title: Just Another Gibbs Additive Modeller: JAGS support for mgcv.
> ### Aliases: jagam sim2jam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## the following illustrates a typical workflow. To run the 
> ## 'Not run' code you need rjags (and JAGS) to be installed.
> require(mgcv)
>   
> set.seed(2) ## simulate some data... 
> n <- 400
> dat <- gamSim(1,n=n,dist="normal",scale=2)
Gu & Wahba 4 term additive model
> ## regular gam fit for comparison...
> b0 <- gam(y~s(x0)+s(x1) + s(x2)+s(x3),data=dat,method="REML")
> 
> ## Set up JAGS code and data. In this one might want to diagonalize
> ## to use conjugate samplers. Usually call 'setwd' first, to set
> ## directory in which model file ("test.jags") will be written.
> jd <- jagam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,file="test.jags",
+             sp.prior="gamma",diagonalize=TRUE)
> 
> ## In normal use the model in "test.jags" would now be edited to add 
> ## the non-standard stochastic elements that require use of JAGS....
> 
> ## Not run: 
> ##D require(rjags)
> ##D load.module("glm") ## improved samplers for GLMs often worth loading
> ##D jm <-jags.model("test.jags",data=jd$jags.data,inits=jd$jags.ini,n.chains=1)
> ##D list.samplers(jm)
> ##D sam <- jags.samples(jm,c("b","rho","scale"),n.iter=10000,thin=10)
> ##D jam <- sim2jam(sam,jd$pregam)
> ##D plot(jam,pages=1)
> ##D jam
> ##D pd <- data.frame(x0=c(.5,.6),x1=c(.4,.2),x2=c(.8,.4),x3=c(.1,.1))
> ##D fv <- predict(jam,newdata=pd)
> ##D ## and some minimal checking...
> ##D require(coda)
> ##D effectiveSize(as.mcmc.list(sam$b))
> ## End(Not run)
> 
> ## a gamma example...
> set.seed(1); n <- 400
> dat <- gamSim(1,n=n,dist="normal",scale=2)
Gu & Wahba 4 term additive model
> scale <- .5; Ey <- exp(dat$f/2)
> dat$y <- rgamma(n,shape=1/scale,scale=Ey*scale)
> jd <- jagam(y~s(x0)+te(x1,x2)+s(x3),data=dat,family=Gamma(link=log),
+             file="test.jags",sp.prior="log.uniform")
> 
> ## In normal use the model in "test.jags" would now be edited to add 
> ## the non-standard stochastic elements that require use of JAGS....
> 
> ## Not run: 
> ##D require(rjags)
> ##D ## following sets random seed, but note that under JAGS 3.4 many
> ##D ## models are still not fully repeatable (JAGS 4 should fix this)
> ##D jd$jags.ini$.RNG.name <- "base::Mersenne-Twister" ## setting RNG
> ##D jd$jags.ini$.RNG.seed <- 6 ## how to set RNG seed
> ##D jm <-jags.model("test.jags",data=jd$jags.data,inits=jd$jags.ini,n.chains=1)
> ##D list.samplers(jm)
> ##D sam <- jags.samples(jm,c("b","rho","scale","mu"),n.iter=10000,thin=10)
> ##D jam <- sim2jam(sam,jd$pregam)
> ##D plot(jam,pages=1)
> ##D jam
> ##D pd <- data.frame(x0=c(.5,.6),x1=c(.4,.2),x2=c(.8,.4),x3=c(.1,.1))
> ##D fv <- predict(jam,newdata=pd)
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("ldTweedie")
> ### * ldTweedie
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ldTweedie
> ### Title: Log Tweedie density evaluation
> ### Aliases: ldTweedie
> ### Keywords: models regression
> 
> ### ** Examples
> 
>   library(mgcv)
>   ## convergence to Poisson illustrated
>   ## notice how p>1.1 is OK
>   y <- seq(1e-10,10,length=1000)
>   p <- c(1.0001,1.001,1.01,1.1,1.2,1.5,1.8,2)
>   phi <- .5
>   fy <- exp(ldTweedie(y,mu=2,p=p[1],phi=phi)[,1])
>   plot(y,fy,type="l",ylim=c(0,3),main="Tweedie density as p changes")
>   for (i in 2:length(p)) {
+     fy <- exp(ldTweedie(y,mu=2,p=p[i],phi=phi)[,1])
+     lines(y,fy,col=i)
+   }
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("linear.functional.terms")
> ### * linear.functional.terms
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: linear.functional.terms
> ### Title: Linear functionals of a smooth in GAMs
> ### Aliases: linear.functional.terms function.predictors signal.regression
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ### matrix argument `linear operator' smoothing
> library(mgcv)
> set.seed(0)
> 
> ###############################
> ## simple summation example...#
> ###############################
> 
> n<-400
> sig<-2
> x <- runif(n, 0, .9)
> f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
> x1 <- x + .1
> 
> f <- f2(x) + f2(x1)  ## response is sum of f at two adjacent x values 
> y <- f + rnorm(n)*sig
> 
> X <- matrix(c(x,x1),n,2) ## matrix covariate contains both x values
> b <- gam(y~s(X))         
> 
> plot(b)  ## reconstruction of f
> plot(f,fitted(b))
> 
> ######################################################################
> ## multivariate integral example. Function `test1' will be integrated# 
> ## (by midpoint quadrature) over 100 equal area sub-squares covering # 
> ## the unit square. Noise is added to the resulting simulated data.  #
> ## `test1' is estimated from the resulting data using two alternative#
> ## smooths.                                                          #
> ######################################################################
> 
> test1 <- function(x,z,sx=0.3,sz=0.4)
+   { (pi**sx*sz)*(1.2*exp(-(x-0.2)^2/sx^2-(z-0.3)^2/sz^2)+
+     0.8*exp(-(x-0.7)^2/sx^2-(z-0.8)^2/sz^2))
+   }
> 
> ## create quadrature (integration) grid, in useful order
> ig <- 5 ## integration grid within square
> mx <- mz <- (1:ig-.5)/ig
> ix <- rep(mx,ig);iz <- rep(mz,rep(ig,ig))
> 
> og <- 10 ## observarion grid
> mx <- mz <- (1:og-1)/og
> ox <- rep(mx,og);ox <- rep(ox,rep(ig^2,og^2))
> oz <- rep(mz,rep(og,og));oz <- rep(oz,rep(ig^2,og^2))
> 
> x <- ox + ix/og;z <- oz + iz/og ## full grid, subsquare by subsquare
> 
> ## create matrix covariates...
> X <- matrix(x,og^2,ig^2,byrow=TRUE)
> Z <- matrix(z,og^2,ig^2,byrow=TRUE)
> 
> ## create simulated test data...
> dA <- 1/(og*ig)^2  ## quadrature square area
> F <- test1(X,Z)    ## evaluate on grid
> f <- rowSums(F)*dA ## integrate by midpoint quadrature
> y <- f + rnorm(og^2)*5e-4 ## add noise
> ## ... so each y is a noisy observation of the integral of `test1'
> ## over a 0.1 by 0.1 sub-square from the unit square
> 
> ## Now fit model to simulated data...
> 
> L <- X*0 + dA
> 
> ## ... let F be the matrix of the smooth evaluated at the x,z values
> ## in matrices X and Z. rowSums(L*F) gives the model predicted
> ## integrals of `test1' corresponding to the observed `y'
> 
> L1 <- rowSums(L) ## smooths are centred --- need to add in L%*%1
> 
> ## fit models to reconstruct `test1'....
> 
> b <- gam(y~s(X,Z,by=L)+L1-1)   ## (L1 and const are confounded here)
> b1 <- gam(y~te(X,Z,by=L)+L1-1) ## tensor product alternative
> 
> ## plot results...
> 
> old.par<-par(mfrow=c(2,2))
> x<-runif(n);z<-runif(n);
> xs<-seq(0,1,length=30);zs<-seq(0,1,length=30)
> pr<-data.frame(x=rep(xs,30),z=rep(zs,rep(30,30)))
> truth<-matrix(test1(pr$x,pr$z),30,30)
> contour(xs,zs,truth)
> plot(b)
> vis.gam(b,view=c("X","Z"),cond=list(L1=1,L=1),plot.type="contour")
> vis.gam(b1,view=c("X","Z"),cond=list(L1=1,L=1),plot.type="contour")
> 
> ####################################
> ## A "signal" regression example...#
> ####################################
> 
> rf <- function(x=seq(0,1,length=100)) {
+ ## generates random functions...
+   m <- ceiling(runif(1)*5) ## number of components
+   f <- x*0;
+   mu <- runif(m,min(x),max(x));sig <- (runif(m)+.5)*(max(x)-min(x))/10
+   for (i in 1:m) f <- f+ dnorm(x,mu[i],sig[i])
+   f
+ }
> 
> x <- seq(0,1,length=100) ## evaluation points
> 
> ## example functional predictors...
> par(mfrow=c(3,3));for (i in 1:9) plot(x,rf(x),type="l",xlab="x")
> 
> ## simulate 200 functions and store in rows of L...
> L <- matrix(NA,200,100) 
> for (i in 1:200) L[i,] <- rf()  ## simulate the functional predictors
> 
> f2 <- function(x) { ## the coefficient function
+   (0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10)/10 
+ }
> 
> f <- f2(x) ## the true coefficient function
> 
> y <- L%*%f + rnorm(200)*20 ## simulated response data
> 
> ## Now fit the model E(y) = L%*%f(x) where f is a smooth function.
> ## The summation convention is used to evaluate smooth at each value
> ## in matrix X to get matrix F, say. Then rowSum(L*F) gives E(y).
> 
> ## create matrix of eval points for each function. Note that
> ## `smoothCon' is smart and will recognize the duplication...
> X <- matrix(x,200,100,byrow=TRUE) 
> 
> b <- gam(y~s(X,by=L,k=20)) 
> par(mfrow=c(1,1))
> plot(b,shade=TRUE);lines(x,f,col=2)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("ls.size")
> ### * ls.size
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ls.size
> ### Title: Size of list elements
> ### Aliases: ls.size
> 
> ### ** Examples
> 
> library(mgcv)
> b <- list(M=matrix(runif(100),10,10),quote=
+ "The world is ruled by idiots because only an idiot would want to rule the world.",
+ fam=binomial())
> ls.size(b)
     M  quote    fam 
  1000    216 133800 
> 
> 
> 
> cleanEx()
> nameEx("magic")
> ### * magic
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: magic
> ### Title: Stable Multiple Smoothing Parameter Estimation by GCV or UBRE
> ### Aliases: magic
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## Use `magic' for a standard additive model fit ... 
>    library(mgcv)
>    set.seed(1);n <- 200;sig <- 1
>    dat <- gamSim(1,n=n,scale=sig)
Gu & Wahba 4 term additive model
>    k <- 30
> ## set up additive model
>    G <- gam(y~s(x0,k=k)+s(x1,k=k)+s(x2,k=k)+s(x3,k=k),fit=FALSE,data=dat)
> ## fit using magic (and gam default tolerance)
>    mgfit <- magic(G$y,G$X,G$sp,G$S,G$off,rank=G$rank,
+                   control=list(tol=1e-7,step.half=15))
> ## and fit using gam as consistency check
>    b <- gam(G=G)
>    mgfit$sp;b$sp  # compare smoothing parameter estimates
[1] 1.232642e+00 3.131825e+00 6.530096e-03 1.051427e+10
       s(x0)        s(x1)        s(x2)        s(x3) 
1.232642e+00 3.131825e+00 6.530096e-03 1.051427e+10 
>    edf <- magic.post.proc(G$X,mgfit,G$w)$edf # get e.d.f. per param
>    range(edf-b$edf)  # compare
[1] 0 0
> 
> ## p>n example... fit model to first 100 data only, so more
> ## params than data...
> 
>    mgfit <- magic(G$y[1:100],G$X[1:100,],G$sp,G$S,G$off,rank=G$rank)
>    edf <- magic.post.proc(G$X[1:100,],mgfit,G$w[1:100])$edf
> 
> ## constrain first two smooths to have identical smoothing parameters
>    L <- diag(3);L <- rbind(L[1,],L)
>    mgfit <- magic(G$y,G$X,rep(-1,3),G$S,G$off,L=L,rank=G$rank,C=G$C)
> 
> ## Now a correlated data example ... 
>     library(nlme)
> ## simulate truth
>     set.seed(1);n<-400;sig<-2
>     x <- 0:(n-1)/(n-1)
>     f <- 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
> ## produce scaled covariance matrix for AR1 errors...
>     V <- corMatrix(Initialize(corAR1(.6),data.frame(x=x)))
>     Cv <- chol(V)  # t(Cv)%*%Cv=V
> ## Simulate AR1 errors ...
>     e <- t(Cv)%*%rnorm(n,0,sig) # so cov(e) = V * sig^2
> ## Observe truth + AR1 errors
>     y <- f + e 
> ## GAM ignoring correlation
>     par(mfrow=c(1,2))
>     b <- gam(y~s(x,k=20))
>     plot(b);lines(x,f-mean(f),col=2);title("Ignoring correlation")
> ## Fit smooth, taking account of *known* correlation...
>     w <- solve(t(Cv)) # V^{-1} = w'w
>     ## Use `gam' to set up model for fitting...
>     G <- gam(y~s(x,k=20),fit=FALSE)
>     ## fit using magic, with weight *matrix*
>     mgfit <- magic(G$y,G$X,G$sp,G$S,G$off,rank=G$rank,C=G$C,w=w)
> ## Modify previous gam object using new fit, for plotting...    
>     mg.stuff <- magic.post.proc(G$X,mgfit,w)
>     b$edf <- mg.stuff$edf;b$Vp <- mg.stuff$Vb
>     b$coefficients <- mgfit$b 
>     plot(b);lines(x,f-mean(f),col=2);title("Known correlation")
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("mgcv-package")
> ### * mgcv-package
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mgcv.package
> ### Title: Mixed GAM Computation Vehicle with GCV/AIC/REML smoothness
> ###   estimation and GAMMs by REML/PQL
> ### Aliases: mgcv.package mgcv-package mgcv
> ### Keywords: package models smooth regression
> 
> ### ** Examples
> 
> ## see examples for gam and gamm
> 
> 
> 
> cleanEx()
> nameEx("mgcv-parallel")
> ### * mgcv-parallel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mgcv.parallel
> ### Title: Parallel computation in mgcv.
> ### Aliases: mgcv.parallel
> ### Keywords: package models smooth regression
> 
> ### ** Examples
> 
> ## illustration of multi-threading with gam...
> 
> require(mgcv);set.seed(9)
> dat <- gamSim(1,n=2000,dist="poisson",scale=.1)
Gu & Wahba 4 term additive model
> k <- 12;bs <- "cr";ctrl <- list(nthreads=2)
> 
> system.time(b1<-gam(y~s(x0,bs=bs)+s(x1,bs=bs)+s(x2,bs=bs,k=k)
+             ,family=poisson,data=dat,method="REML"))[3]
elapsed 
  0.112 
> 
> system.time(b2<-gam(y~s(x0,bs=bs)+s(x1,bs=bs)+s(x2,bs=bs,k=k),
+             family=poisson,data=dat,method="REML",control=ctrl))[3]
elapsed 
  0.154 
> 
> ## Poisson example on a cluster with 'bam'. 
> ## Note that there is some overhead in initializing the 
> ## computation on the cluster, associated with loading 
> ## the Matrix package on each node. For this reason the 
> ## sample sizes here are very small to keep CRAN happy, but at
> ## this low sample size you see little advantage of parallel computation.
> 
> k <- 13
> dat <- gamSim(1,n=6000,dist="poisson",scale=.1)
Gu & Wahba 4 term additive model
> require(parallel)  
Loading required package: parallel
> nc <- 2   ## cluster size, set for example portability
> if (detectCores()>1) { ## no point otherwise
+   cl <- makeCluster(nc) 
+   ## could also use makeForkCluster, but read warnings first!
+ } else cl <- NULL
>   
> system.time(b3 <- bam(y ~ s(x0,bs=bs,k=7)+s(x1,bs=bs,k=7)+s(x2,bs=bs,k=k)
+             ,data=dat,family=poisson(),chunk.size=5000,cluster=cl))
   user  system elapsed 
  1.184   0.008   2.255 
> 
> fv <- predict(b3,cluster=cl) ## parallel prediction
> 
> if (!is.null(cl)) stopCluster(cl)
> b3

Family: poisson 
Link function: log 

Formula:
y ~ s(x0, bs = bs, k = 7) + s(x1, bs = bs, k = 7) + s(x2, bs = bs, 
    k = k)

Estimated degrees of freedom:
4.17 3.32 9.64  total = 18.13 

fREML score: 8610.186     
> 
> ## Alternative using the discrete option with bam...
> 
> system.time(b4 <- bam(y ~ s(x0,bs=bs,k=7)+s(x1,bs=bs,k=7)+s(x2,bs=bs,k=k)
+             ,data=dat,family=poisson(),discrete=TRUE,nthreads=2))
   user  system elapsed 
  0.752   0.000   0.709 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:parallel’

> nameEx("missing.data")
> ### * missing.data
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: missing.data
> ### Title: Missing data in GAMs
> ### Aliases: missing.data
> ### Keywords: regression
> 
> ### ** Examples
> 
> ## The example takes a couple of minutes to run, so is marked 
> ## don't run to save CRAN time.
> ## Not run: 
> ##D require(mgcv)
> ##D par(mfrow=c(4,4),mar=c(4,4,1,1))
> ##D for (sim in c(1,7)) { ## cycle over uncorrelated and correlated covariates
> ##D   n <- 350;set.seed(2)
> ##D   ## simulate data but randomly drop 300 covariate measurements
> ##D   ## leaving only 50 complete cases...
> ##D   dat <- gamSim(sim,n=n,scale=3) ## 1 or 7
> ##D   drop <- sample(1:n,300) ## to
> ##D   for (i in 2:5) dat[drop[1:75+(i-2)*75],i] <- NA
> ##D 
> ##D   ## process data.frame producing binary indicators of missingness,
> ##D   ## mx0, mx1 etc. For each missing value create a level of a factor
> ##D   ## idx0, idx1, etc. So idx0 has as many levels as x0 has missing 
> ##D   ## values. Replace the NA's in each variable by the mean of the 
> ##D   ## non missing for that variable...
> ##D 
> ##D   dname <- names(dat)[2:5]
> ##D   dat1 <- dat
> ##D   for (i in 1:4) {
> ##D     by.name <- paste("m",dname[i],sep="") 
> ##D     dat1[[by.name]] <- is.na(dat1[[dname[i]]])
> ##D     dat1[[dname[i]]][dat1[[by.name]]] <- mean(dat1[[dname[i]]],na.rm=TRUE)
> ##D     lev <- rep(1,n);lev[dat1[[by.name]]] <- 1:sum(dat1[[by.name]])
> ##D     id.name <- paste("id",dname[i],sep="")
> ##D     dat1[[id.name]] <- factor(lev) 
> ##D     dat1[[by.name]] <- as.numeric(dat1[[by.name]])
> ##D   }
> ##D 
> ##D   ## Fit a gam, in which any missing value contributes zero 
> ##D   ## to the linear predictor from its smooth, but each 
> ##D   ## missing has its own random effect, with the random effect 
> ##D   ## variances being specific to the variable. e.g.
> ##D   ## for s(x0,by=ordered(!mx0)), declaring the `by' as an ordered
> ##D   ## factor ensures that the smooth is centred, but multiplied
> ##D   ## by zero when mx0 is one (indicating a missing x0). This means
> ##D   ## that any value (within range) can be put in place of the 
> ##D   ## NA for x0.  s(idx0,bs="re",by=mx0) produces a separate Gaussian 
> ##D   ## random effect for each missing value of x0 (in place of s(x0),
> ##D   ## effectively). The `by' variable simply sets the random effect to 
> ##D   ## zero when x0 is non-missing, so that we can set idx0 to any 
> ##D   ## existing level for these cases.   
> ##D 
> ##D   b <- gam(y~s(x0,by=ordered(!mx0))+s(x1,by=ordered(!mx1))+
> ##D              s(x2,by=ordered(!mx2))+s(x3,by=ordered(!mx3))+
> ##D              s(idx0,bs="re",by=mx0)+s(idx1,bs="re",by=mx1)+
> ##D              s(idx2,bs="re",by=mx2)+s(idx3,bs="re",by=mx3)
> ##D              ,data=dat1,method="REML")
> ##D 
> ##D   for (i in 1:4) plot(b,select=i) ## plot the smooth effects from b
> ##D 
> ##D   ## fit the model to the `complete case' data...
> ##D   b2 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="REML")
> ##D   plot(b2) ## plot the complete case results
> ##D }
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("model.matrix.gam")
> ### * model.matrix.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: model.matrix.gam
> ### Title: Extract model matrix from GAM fit
> ### Aliases: model.matrix.gam
> ### Keywords: models smooth regression
> 
> ### ** Examples
>  
> require(mgcv)
> n <- 15
> x <- runif(n)
> y <- sin(x*2*pi) + rnorm(n)*.2
> mod <- gam(y~s(x,bs="cc",k=6),knots=list(x=seq(0,1,length=6)))
> model.matrix(mod)
   (Intercept)      s(x).1      s(x).2      s(x).3       s(x).4
1            1  0.64005926  0.28713835 -0.22520339 -0.074786976
2            1  0.01026326  0.89582182 -0.18450287 -0.069653495
3            1 -0.23534306  0.03365477  0.81648663 -0.244122129
4            1 -0.59048631 -0.16217974 -0.46763064  0.186386299
5            1  0.74479124 -0.12347968 -0.19396253 -0.203204806
6            1 -0.56089257 -0.14955417 -0.45727097  0.269751709
7            1 -0.66201291 -0.21573683 -0.48187308 -0.120102457
8            1 -0.11632150 -0.22703879  0.70081516  0.201332878
9            1 -0.15249390 -0.18723726  0.81733754 -0.005827532
10           1 -0.20178755 -0.37543794 -0.35493419 -0.581672810
11           1  0.75078035 -0.10086471 -0.19420184 -0.191330859
12           1  0.66625477 -0.23439198 -0.20146231 -0.281093403
13           1 -0.09734475 -0.22684275  0.56076919  0.384389211
14           1 -0.05556123  0.92102724 -0.15332991 -0.083659778
15           1 -0.13990510 -0.13487834  0.01896321  0.813594149
> 
> 
> 
> cleanEx()
> nameEx("mono.con")
> ### * mono.con
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mono.con
> ### Title: Monotonicity constraints for a cubic regression spline
> ### Aliases: mono.con
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## see ?pcls
> 
> 
> 
> cleanEx()
> nameEx("mroot")
> ### * mroot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mroot
> ### Title: Smallest square root of matrix
> ### Aliases: mroot
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
>   require(mgcv)
>   set.seed(0)
>   a <- matrix(runif(24),6,4)
>   A <- a%*%t(a) ## A is +ve semi-definite, rank 4
>   B <- mroot(A) ## default pivoted choleski method
>   tol <- 100*.Machine$double.eps
>   chol.err <- max(abs(A-B%*%t(B)));chol.err
[1] 2.220446e-16
>   if (chol.err>tol) warning("mroot (chol) suspect")
>   B <- mroot(A,method="svd") ## svd method
>   svd.err <- max(abs(A-B%*%t(B)));svd.err
[1] 1.332268e-15
>   if (svd.err>tol) warning("mroot (svd) suspect")  
> 
> 
> 
> cleanEx()
> nameEx("multinom")
> ### * multinom
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: multinom
> ### Title: GAM multinomial logistic regression
> ### Aliases: multinom
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(6)
> ## simulate some data from a three class model
> n <- 1000
> f1 <- function(x) sin(3*pi*x)*exp(-x)
> f2 <- function(x) x^3
> f3 <- function(x) .5*exp(-x^2)-.2
> f4 <- function(x) 1
> x1 <- runif(n);x2 <- runif(n)
> eta1 <- 2*(f1(x1) + f2(x2))-.5
> eta2 <- 2*(f3(x1) + f4(x2))-1
> p <- exp(cbind(0,eta1,eta2))
> p <- p/rowSums(p) ## prob. of each category 
> cp <- t(apply(p,1,cumsum)) ## cumulative prob.
> ## simulate multinomial response with these probabilities
> ## see also ?rmultinom
> y <- apply(cp,1,function(x) min(which(x>runif(1))))-1
> ## plot simulated data...
> plot(x1,x2,col=y+3)
> 
> ## now fit the model...
> b <- gam(list(y~s(x1)+s(x2),~s(x1)+s(x2)),family=multinom(K=2))
> plot(b,pages=1)
> gam.check(b)

Method: REML   Optimizer: outer newton
full convergence after 4 iterations.
Gradient range [-1.322458e-05,0.0001114286]
(score 729.5551 & scale 1).
Hessian positive definite, eigenvalue range [0.193781,2.149341].
Model rank =  38 / 38 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

           k'   edf k-index p-value
s(x1)   9.000 6.750   1.032    0.87
s(x2)   9.000 2.701   0.965    0.12
s.1(x1) 9.000 2.341   1.032    0.87
s.1(x2) 9.000 1.871   0.965    0.12
> 
> ## now a simple classification plot...
> expand.grid(x1=seq(0,1,length=40),x2=seq(0,1,length=40)) -> gr
> pp <- predict(b,newdata=gr,type="response")
> pc <- apply(pp,1,function(x) which(max(x)==x)[1])-1
> plot(gr,col=pc+3,pch=19)
> 
> 
> 
> 
> cleanEx()
> nameEx("mvn")
> ### * mvn
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mvn
> ### Title: Multivariate normal additive models
> ### Aliases: mvn
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## simulate some data...
> V <- matrix(c(2,1,1,2),2,2)
> f0 <- function(x) 2 * sin(pi * x)
> f1 <- function(x) exp(2 * x)
> f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
+             (10 * x)^3 * (1 - x)^10
> n <- 300
> x0 <- runif(n);x1 <- runif(n);
> x2 <- runif(n);x3 <- runif(n)
> y <- matrix(0,n,2)
> for (i in 1:n) {
+   mu <- c(f0(x0[i])+f1(x1[i]),f2(x2[i]))
+   y[i,] <- rmvn(1,mu,V)
+ }
> dat <- data.frame(y0=y[,1],y1=y[,2],x0=x0,x1=x1,x2=x2,x3=x3)
> 
> ## fit model...
> 
> b <- gam(list(y0~s(x0)+s(x1),y1~s(x2)+s(x3)),family=mvn(d=2),data=dat)
> b

Family: Multivariate normal 
Link function: 

Formula:
y0 ~ s(x0) + s(x1)
y1 ~ s(x2) + s(x3)

Estimated degrees of freedom:
3.73 3.79 8.47 1.00  total = 21.99 

REML score: 537.5184     
> summary(b)

Family: Multivariate normal 
Link function: 

Formula:
y0 ~ s(x0) + s(x1)
y1 ~ s(x2) + s(x3)

Parametric coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)    4.59554    0.08571   53.62   <2e-16 ***
(Intercept).1  3.07060    0.08636   35.56   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
          edf Ref.df Chi.sq p-value    
s(x0)   3.729  4.620   69.0 2.3e-13 ***
s(x1)   3.792  4.681  706.7 < 2e-16 ***
s.1(x2) 8.471  8.920 1401.5 < 2e-16 ***
s.1(x3) 1.000  1.001    0.0   0.994    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Deviance explained = 79.6%
-REML = 537.52  Scale est. = 1         n = 300
> plot(b,pages=1)
> solve(crossprod(b$family$data$R)) ## estimated cov matrix
         [,1]     [,2]
[1,] 2.204051 1.185763
[2,] 1.185763 2.237392
> 
> 
> 
> 
> cleanEx()
> nameEx("negbin")
> ### * negbin
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: negbin
> ### Title: GAM negative binomial families
> ### Aliases: negbin nb
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(3)
> n<-400
> dat <- gamSim(1,n=n)
Gu & Wahba 4 term additive model
> g <- exp(dat$f/5)
> 
> ## negative binomial data... 
> dat$y <- rnbinom(g,size=3,mu=g)
> ## known theta fit ...
> b0 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=negbin(3),data=dat)
> plot(b0,pages=1)
> print(b0)

Family: Negative Binomial(3) 
Link function: log 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.05 2.51 6.38 1.00  total = 12.94 

UBRE score: 0.1872599     
> 
> ## same with theta estimation...
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=nb(),data=dat)
> plot(b,pages=1)
> print(b)

Family: Negative Binomial(2.693) 
Link function: log 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.27 2.76 6.92 1.00  total = 13.95 

REML score: 1062.259     
> b$family$getTheta(TRUE) ## extract final theta estimate
[1] 2.692607
> 
> ## unknown theta via performance iteration...
> b1 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=negbin(c(1,10)),
+           optimizer="perf",data=dat)
> plot(b1,pages=1)
> print(b1)

Family: Negative Binomial(2.862) 
Link function: log 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.03 2.49 6.35 1.00  total = 12.87 

UBRE score: 1.033241     
> 
> ## another example...
> set.seed(1)
> f <- dat$f
> f <- f - min(f)+5;g <- f^2/10
> dat$y <- rnbinom(g,size=3,mu=g)
> b2 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=nb(link="sqrt"),
+          data=dat,method="REML") 
> plot(b2,pages=1)
> print(b2)

Family: Negative Binomial(2.786) 
Link function: sqrt 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.49 2.08 7.29 1.85  total = 14.71 

REML score: 1356.263     
> rm(dat)
> 
> 
> 
> cleanEx()
> nameEx("new.name")
> ### * new.name
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: new.name
> ### Title: Obtain a name for a new variable that is not already in use
> ### Aliases: new.name
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> old <- c("a","tuba","is","tubby")
> new.name("tubby",old)
[1] "tubby.0"
> 
> 
> 
> cleanEx()
> nameEx("notExp")
> ### * notExp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: notExp
> ### Title: Functions for better-than-log positive parameterization
> ### Aliases: notExp notLog
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## Illustrate the notExp function: 
> ## less steep than exp, but still monotonic.
> require(mgcv)
> x <- -100:100/10
> op <- par(mfrow=c(2,2))
> plot(x,notExp(x),type="l")
> lines(x,exp(x),col=2)
> plot(x,log(notExp(x)),type="l")
> lines(x,log(exp(x)),col=2) # redundancy intended
> x <- x/4
> plot(x,notExp(x),type="l")
> lines(x,exp(x),col=2)
> plot(x,log(notExp(x)),type="l")
> lines(x,log(exp(x)),col=2) # redundancy intended
> par(op)
> range(notLog(notExp(x))-x) # show that inverse works!
[1] -4.440892e-16  4.440892e-16
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("notExp2")
> ### * notExp2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: notExp2
> ### Title: Alternative to log parameterization for variance components
> ### Aliases: notExp2 notLog2
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## Illustrate the notExp2 function:
> require(mgcv)
> x <- seq(-50,50,length=1000)
> op <- par(mfrow=c(2,2))
> plot(x,notExp2(x),type="l")
> lines(x,exp(x),col=2)
> plot(x,log(notExp2(x)),type="l")
> lines(x,log(exp(x)),col=2) # redundancy intended
> x <- x/4
> plot(x,notExp2(x),type="l")
> lines(x,exp(x),col=2)
> plot(x,log(notExp2(x)),type="l")
> lines(x,log(exp(x)),col=2) # redundancy intended
> par(op)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("null.space.dimension")
> ### * null.space.dimension
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: null.space.dimension
> ### Title: The basis of the space of un-penalized functions for a TPRS
> ### Aliases: null.space.dimension
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv)
> null.space.dimension(2,0)
[1] 3
> 
> 
> 
> cleanEx()
> nameEx("ocat")
> ### * ocat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ocat
> ### Title: GAM ordered categorical family
> ### Aliases: ocat ordered.categorical
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## Simulate some ordered categorical data...
> set.seed(3);n<-400
> dat <- gamSim(1,n=n)
Gu & Wahba 4 term additive model
> dat$f <- dat$f - mean(dat$f)
> 
> alpha <- c(-Inf,-1,0,5,Inf)
> R <- length(alpha)-1
> y <- dat$f
> u <- runif(n)
> u <- dat$f + log(u/(1-u)) 
> for (i in 1:R) {
+   y[u > alpha[i]&u <= alpha[i+1]] <- i
+ }
> dat$y <- y
> 
> ## plot the data...
> par(mfrow=c(2,2))
> with(dat,plot(x0,y));with(dat,plot(x1,y))
> with(dat,plot(x2,y));with(dat,plot(x3,y))
> 
> ## fit ocat model to data...
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=ocat(R=R),data=dat)
> b

Family: Ordered Categorical(-1,0.07,5.15) 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
3.32 3.11 7.81 1.59  total = 16.84 

REML score: 283.8245     
> plot(b,pages=1)
> gam.check(b)

Method: REML   Optimizer: outer newton
full convergence after 5 iterations.
Gradient range [-4.800542e-06,4.973802e-08]
(score 283.8245 & scale 1).
Hessian positive definite, eigenvalue range [0.09977744,150.0804].
Model rank =  37 / 37 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

         k'   edf k-index p-value
s(x0) 9.000 3.317   0.939    0.10
s(x1) 9.000 3.115   1.064    0.92
s(x2) 9.000 7.814   0.945    0.13
s(x3) 9.000 1.593   0.966    0.28
> summary(b)

Family: Ordered Categorical(-1,0.07,5.15) 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Parametric coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   0.1221     0.1319   0.926    0.354

Approximate significance of smooth terms:
        edf Ref.df  Chi.sq  p-value    
s(x0) 3.317  4.116  21.623 0.000263 ***
s(x1) 3.115  3.871 188.368  < 2e-16 ***
s(x2) 7.814  8.616 402.300  < 2e-16 ***
s(x3) 1.593  1.970   0.936 0.640434    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Deviance explained = 57.7%
-REML = 283.82  Scale est. = 1         n = 400
> b$family$getTheta(TRUE) ## the estimated cut points
[1] -1.00000000  0.07295739  5.14663505
> 
> ## predict probabilities of being in each category
> predict(b,dat[1:2,],type="response",se=TRUE)
$fit
        [,1]        [,2]        [,3]         [,4]
1 0.99085777 0.005996704 0.003125774 0.0000197507
2 0.06793525 0.107745442 0.795787468 0.0285318416

$se.fit
         [,1]       [,2]        [,3]         [,4]
1 0.006829264 0.00446533 0.002349044 1.488965e-05
2 0.028948637 0.03725874 0.053535372 1.267200e-02

> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("pcls")
> ### * pcls
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pcls
> ### Title: Penalized Constrained Least Squares Fitting
> ### Aliases: pcls
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> # first an un-penalized example - fit E(y)=a+bx subject to a>0
> set.seed(0)
> n <- 100
> x <- runif(n); y <- x - 0.2 + rnorm(n)*0.1
> M <- list(X=matrix(0,n,2),p=c(0.1,0.5),off=array(0,0),S=list(),
+ Ain=matrix(0,1,2),bin=0,C=matrix(0,0,0),sp=array(0,0),y=y,w=y*0+1)
> M$X[,1] <- 1; M$X[,2] <- x; M$Ain[1,] <- c(1,0)
> pcls(M) -> M$p
> plot(x,y); abline(M$p,col=2); abline(coef(lm(y~x)),col=3)
> 
> # Penalized example: monotonic penalized regression spline .....
> 
> # Generate data from a monotonic truth.
> x <- runif(100)*4-1;x <- sort(x);
> f <- exp(4*x)/(1+exp(4*x)); y <- f+rnorm(100)*0.1; plot(x,y)
> dat <- data.frame(x=x,y=y)
> # Show regular spline fit (and save fitted object)
> f.ug <- gam(y~s(x,k=10,bs="cr")); lines(x,fitted(f.ug))
> # Create Design matrix, constraints etc. for monotonic spline....
> sm <- smoothCon(s(x,k=10,bs="cr"),dat,knots=NULL)[[1]]
> F <- mono.con(sm$xp);   # get constraints
> G <- list(X=sm$X,C=matrix(0,0,0),sp=f.ug$sp,p=sm$xp,y=y,w=y*0+1)
> G$Ain <- F$A;G$bin <- F$b;G$S <- sm$S;G$off <- 0
> 
> p <- pcls(G);  # fit spline (using s.p. from unconstrained fit)
> 
> fv<-Predict.matrix(sm,data.frame(x=x))%*%p
> lines(x,fv,col=2)
> 
> # now a tprs example of the same thing....
> 
> f.ug <- gam(y~s(x,k=10)); lines(x,fitted(f.ug))
> # Create Design matrix, constriants etc. for monotonic spline....
> sm <- smoothCon(s(x,k=10,bs="tp"),dat,knots=NULL)[[1]]
> xc <- 0:39/39 # points on [0,1]  
> nc <- length(xc)  # number of constraints
> xc <- xc*4-1  # points at which to impose constraints
> A0 <- Predict.matrix(sm,data.frame(x=xc)) 
> # ... A0%*%p evaluates spline at xc points
> A1 <- Predict.matrix(sm,data.frame(x=xc+1e-6)) 
> A <- (A1-A0)/1e-6    
> ##  ... approx. constraint matrix (A%*%p is -ve 
> ## spline gradient at points xc)
> G <- list(X=sm$X,C=matrix(0,0,0),sp=f.ug$sp,y=y,w=y*0+1,S=sm$S,off=0)
> G$Ain <- A;    # constraint matrix
> G$bin <- rep(0,nc);  # constraint vector
> G$p <- rep(0,10); G$p[10] <- 0.1  
> # ... monotonic start params, got by setting coefs of polynomial part
> p <- pcls(G);  # fit spline (using s.p. from unconstrained fit)
> 
> fv2 <- Predict.matrix(sm,data.frame(x=x))%*%p
> lines(x,fv2,col=3)
> 
> ######################################
> ## monotonic additive model example...
> ######################################
> 
> ## First simulate data...
> 
> set.seed(10)
> f1 <- function(x) 5*exp(4*x)/(1+exp(4*x));
> f2 <- function(x) {
+   ind <- x > .5
+   f <- x*0
+   f[ind] <- (x[ind] - .5)^2*10
+   f 
+ }
> f3 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 
+       10 * (10 * x)^3 * (1 - x)^10
> n <- 200
> x <- runif(n); z <- runif(n); v <- runif(n)
> mu <- f1(x) + f2(z) + f3(v)
> y <- mu + rnorm(n)
> 
> ## Preliminary unconstrained gam fit...
> G <- gam(y~s(x)+s(z)+s(v,k=20),fit=FALSE)
> b <- gam(G=G)
> 
> ## generate constraints, by finite differencing
> ## using predict.gam ....
> eps <- 1e-7
> pd0 <- data.frame(x=seq(0,1,length=100),z=rep(.5,100),
+                   v=rep(.5,100))
> pd1 <- data.frame(x=seq(0,1,length=100)+eps,z=rep(.5,100),
+                   v=rep(.5,100))
> X0 <- predict(b,newdata=pd0,type="lpmatrix")
> X1 <- predict(b,newdata=pd1,type="lpmatrix")
> Xx <- (X1 - X0)/eps ## Xx %*% coef(b) must be positive 
> pd0 <- data.frame(z=seq(0,1,length=100),x=rep(.5,100),
+                   v=rep(.5,100))
> pd1 <- data.frame(z=seq(0,1,length=100)+eps,x=rep(.5,100),
+                   v=rep(.5,100))
> X0 <- predict(b,newdata=pd0,type="lpmatrix")
> X1 <- predict(b,newdata=pd1,type="lpmatrix")
> Xz <- (X1-X0)/eps
> G$Ain <- rbind(Xx,Xz) ## inequality constraint matrix
> G$bin <- rep(0,nrow(G$Ain))
> G$C = matrix(0,0,ncol(G$X))
> G$sp <- b$sp
> G$p <- coef(b)
> G$off <- G$off-1 ## to match what pcls is expecting
> ## force inital parameters to meet constraint
> G$p[11:18] <- G$p[2:9]<- 0
> p <- pcls(G) ## constrained fit
> par(mfrow=c(2,3))
> plot(b) ## original fit
> b$coefficients <- p
> plot(b) ## constrained fit
> ## note that standard errors in preceding plot are obtained from
> ## unconstrained fit
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("pdIdnot")
> ### * pdIdnot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pdIdnot
> ### Title: Overflow proof pdMat class for multiples of the identity matrix
> ### Aliases: pdIdnot pdConstruct.pdIdnot pdFactor.pdIdnot pdMatrix.pdIdnot
> ###   coef.pdIdnot corMatrix.pdIdnot Dim.pdIdnot logDet.pdIdnot
> ###   solve.pdIdnot summary.pdIdnot
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> # see gamm
> 
> 
> 
> cleanEx()
> nameEx("pdTens")
> ### * pdTens
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pdTens
> ### Title: Functions implementing a pdMat class for tensor product smooths
> ### Aliases: pdTens pdConstruct.pdTens pdFactor.pdTens pdMatrix.pdTens
> ###   coef.pdTens summary.pdTens
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> # see gamm
> 
> 
> 
> cleanEx()
> nameEx("pen.edf")
> ### * pen.edf
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pen.edf
> ### Title: Extract the effective degrees of freedom associated with each
> ###   penalty in a gam fit
> ### Aliases: pen.edf
> ### Keywords: models smooth regression
> 
> ### ** Examples
>  
>   require(mgcv)
>   set.seed(20) 
>   dat <- gamSim(1,n=400,scale=2) ## simulate data
Gu & Wahba 4 term additive model
>   ## following `t2' smooth basically separates smooth 
>   ## of x0,x1 into main effects + interaction.... 
>   
>   b <- gam(y~t2(x0,x1,bs="tp",m=1,k=7)+s(x2)+s(x3),
+            data=dat,method="ML")
>   pen.edf(b)
t2(x0,x1)rr t2(x0,x1)nr t2(x0,x1)rn       s(x2)       s(x3) 
0.001236044 5.269350274 4.454572599 8.118427635 1.000034170 
>   
>   ## label "rr" indicates interaction edf (range space times range space)
>   ## label "nr" (null space for x0 times range space for x1) is main
>   ##            effect for x1.
>   ## label "rn" is main effect for x0
>   ## clearly interaction is negligible
>   
>   ## second example with higher order marginals. 
>   
>   b <- gam(y~t2(x0,x1,bs="tp",m=2,k=7,full=TRUE)
+              +s(x2)+s(x3),data=dat,method="ML")
>   pen.edf(b)
 t2(x0,x1)rr  t2(x0,x1)1r  t2(x0,x1)2r  t2(x0,x1)r1  t2(x0,x1)r2        s(x2) 
8.341040e-06 1.281162e+00 7.873262e-05 2.137126e+00 8.127323e-05 8.134455e+00 
       s(x3) 
1.000096e+00 
>   
>   ## In this case the EDF is negligible for all terms in the t2 smooth
>   ## apart from the `main effects' (r2 and 2r). To understand the labels
>   ## consider the following 2 examples....
>   ## "r1" relates to the interaction of the range space of the first 
>   ##      marginal smooth and the first basis function of the null 
>   ##      space of the second marginal smooth
>   ## "2r" relates to the interaction of the second basis function of 
>   ##      the null space of the first marginal smooth with the range 
>   ##      space of the second marginal smooth. 
> 
> 
> 
> cleanEx()
> nameEx("place.knots")
> ### * place.knots
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: place.knots
> ### Title: Automatically place a set of knots evenly through covariate
> ###   values
> ### Aliases: place.knots
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> x<-runif(30)
> place.knots(x,7)
[1] 0.01339033 0.20525913 0.36153227 0.44190667 0.66953948 0.87447399 0.99190609
> rm(x)
> 
> 
> 
> cleanEx()
> nameEx("plot.gam")
> ### * plot.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.gam
> ### Title: Default GAM plotting
> ### Aliases: plot.gam
> ### Keywords: models smooth regression hplot
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(0)
> ## fake some data...
> f1 <- function(x) {exp(2 * x)}
> f2 <- function(x) { 
+   0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10 
+ }
> f3 <- function(x) {x*0}
> 
> n<-200
> sig2<-4
> x0 <- rep(1:4,50)
> x1 <- runif(n, 0, 1)
> x2 <- runif(n, 0, 1)
> x3 <- runif(n, 0, 1)
> e <- rnorm(n, 0, sqrt(sig2))
> y <- 2*x0 + f1(x1) + f2(x2) + f3(x3) + e
> x0 <- factor(x0)
> 
> ## fit and plot...
> b<-gam(y~x0+s(x1)+s(x2)+s(x3))
> plot(b,pages=1,residuals=TRUE,all.terms=TRUE,shade=TRUE,shade.col=2)
> plot(b,pages=1,seWithMean=TRUE) ## better coverage intervals
> 
> ## just parametric term alone...
> termplot(b,terms="x0",se=TRUE)
> 
> ## more use of color...
> op <- par(mfrow=c(2,2),bg="blue")
> x <- 0:1000/1000
> for (i in 1:3) {
+   plot(b,select=i,rug=FALSE,col="green",
+     col.axis="white",col.lab="white",all.terms=TRUE)
+   for (j in 1:2) axis(j,col="white",labels=FALSE)
+   box(col="white")
+   eval(parse(text=paste("fx <- f",i,"(x)",sep="")))
+   fx <- fx-mean(fx)
+   lines(x,fx,col=2) ## overlay `truth' in red
+ }
> par(op)
> 
> ## example with 2-d plots, and use of schemes...
> b1 <- gam(y~x0+s(x1,x2)+s(x3))
> op <- par(mfrow=c(2,2))
> plot(b1,all.terms=TRUE)
> par(op) 
> op <- par(mfrow=c(2,2))
> plot(b1,all.terms=TRUE,scheme=1)
> par(op)
> op <- par(mfrow=c(2,2))
> plot(b1,all.terms=TRUE,scheme=c(2,1))
> par(op)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("polys.plot")
> ### * polys.plot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: polys.plot
> ### Title: Plot geographic regions defined as polygons
> ### Aliases: polys.plot
> ### Keywords: hplot models smooth regression
> 
> ### ** Examples
> 
> ## see also ?mrf for use of z
> require(mgcv)
> data(columb.polys)
> polys.plot(columb.polys)
> 
> 
> 
> cleanEx()
> nameEx("predict.bam")
> ### * predict.bam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.bam
> ### Title: Prediction from fitted Big Additive Model model
> ### Aliases: predict.bam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## for parallel computing see examples for ?bam
> 
> ## for general useage follow examples in ?predict.gam
> 
> 
> 
> 
> cleanEx()
> nameEx("predict.gam")
> ### * predict.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.gam
> ### Title: Prediction from fitted GAM model
> ### Aliases: predict.gam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> n<-200
> sig <- 2
> dat <- gamSim(1,n=n,scale=sig)
Gu & Wahba 4 term additive model
> 
> b<-gam(y~s(x0)+s(I(x1^2))+s(x2)+offset(x3),data=dat)
> 
> newd <- data.frame(x0=(0:30)/30,x1=(0:30)/30,x2=(0:30)/30,x3=(0:30)/30)
> pred <- predict.gam(b,newd)
> 
> #############################################
> ## difference between "terms" and "iterms"
> #############################################
> nd2 <- data.frame(x0=c(.25,.5),x1=c(.25,.5),x2=c(.25,.5),x3=c(.25,.5))
> predict(b,nd2,type="terms",se=TRUE)
$fit
        s(x0) s(I(x1^2))     s(x2)
1 -0.05203567 -1.4080075 5.8224469
2  0.46405199 -0.3019055 0.2471463

$se.fit
      s(x0) s(I(x1^2))     s(x2)
1 0.1991066 0.12604172 0.5158322
2 0.1799396 0.02702592 0.3930834

attr(,"constant")
(Intercept) 
   6.836839 
> predict(b,nd2,type="iterms",se=TRUE)
$fit
        s(x0) s(I(x1^2))     s(x2)
1 -0.05203567 -1.4080075 5.8224469
2  0.46405199 -0.3019055 0.2471463

$se.fit
      s(x0) s(I(x1^2))     s(x2)
1 0.2523494  0.1998081 0.5386275
2 0.2375185  0.1573759 0.4225534

attr(,"constant")
(Intercept) 
   6.836839 
> 
> #########################################################
> ## now get variance of sum of predictions using lpmatrix
> #########################################################
> 
> Xp <- predict(b,newd,type="lpmatrix") 
> 
> ## Xp %*% coef(b) yields vector of predictions
> 
> a <- rep(1,31)
> Xs <- t(a) %*% Xp ## Xs %*% coef(b) gives sum of predictions
> var.sum <- Xs %*% b$Vp %*% t(Xs)
> 
> 
> #############################################################
> ## Now get the variance of non-linear function of predictions
> ## by simulation from posterior distribution of the params
> #############################################################
> 
> rmvn <- function(n,mu,sig) { ## MVN random deviates
+   L <- mroot(sig);m <- ncol(L);
+   t(mu + L%*%matrix(rnorm(m*n),m,n)) 
+ }
> 
> br <- rmvn(1000,coef(b),b$Vp) ## 1000 replicate param. vectors
> res <- rep(0,1000)
> for (i in 1:1000)
+ { pr <- Xp %*% br[i,] ## replicate predictions
+   res[i] <- sum(log(abs(pr))) ## example non-linear function
+ }
> mean(res);var(res)
[1] 57.36801
[1] 2.140942
> 
> ## loop is replace-able by following .... 
> 
> res <- colSums(log(abs(Xp %*% t(br))))
> 
> ##################################################################
> ## The following shows how to use use an "lpmatrix" as a lookup 
> ## table for approximate prediction. The idea is to create 
> ## approximate prediction matrix rows by appropriate linear 
> ## interpolation of an existing prediction matrix. The additivity 
> ## of a GAM makes this possible. 
> ## There is no reason to ever do this in R, but the following 
> ## code provides a useful template for predicting from a fitted 
> ## gam *outside* R: all that is needed is the coefficient vector 
> ## and the prediction matrix. Use larger `Xp'/ smaller `dx' and/or 
> ## higher order interpolation for higher accuracy.  
> ###################################################################
> 
> xn <- c(.341,.122,.476,.981) ## want prediction at these values
> x0 <- 1         ## intercept column
> dx <- 1/30      ## covariate spacing in `newd'
> for (j in 0:2) { ## loop through smooth terms
+   cols <- 1+j*9 +1:9      ## relevant cols of Xp
+   i <- floor(xn[j+1]*30)  ## find relevant rows of Xp
+   w1 <- (xn[j+1]-i*dx)/dx ## interpolation weights
+   ## find approx. predict matrix row portion, by interpolation
+   x0 <- c(x0,Xp[i+2,cols]*w1 + Xp[i+1,cols]*(1-w1))
+ }
> dim(x0)<-c(1,28) 
> fv <- x0%*%coef(b) + xn[4];fv    ## evaluate and add offset
         [,1]
[1,] 6.801448
> se <- sqrt(x0%*%b$Vp%*%t(x0));se ## get standard error
          [,1]
[1,] 0.4654633
> ## compare to normal prediction
> predict(b,newdata=data.frame(x0=xn[1],x1=xn[2],
+         x2=xn[3],x3=xn[4]),se=TRUE)
$fit
       1 
6.790702 

$se.fit
        1 
0.4754891 

> 
> ####################################################################
> ## Differentiating the smooths in a model (with CIs for derivatives)
> ####################################################################
> 
> ## simulate data and fit model...
> dat <- gamSim(1,n=300,scale=sig)
Gu & Wahba 4 term additive model
> b<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
> plot(b,pages=1)
> 
> ## now evaluate derivatives of smooths with associated standard 
> ## errors, by finite differencing...
> x.mesh <- seq(0,1,length=200) ## where to evaluate derivatives
> newd <- data.frame(x0 = x.mesh,x1 = x.mesh, x2=x.mesh,x3=x.mesh)
> X0 <- predict(b,newd,type="lpmatrix") 
> 
> eps <- 1e-7 ## finite difference interval
> x.mesh <- x.mesh + eps ## shift the evaluation mesh
> newd <- data.frame(x0 = x.mesh,x1 = x.mesh, x2=x.mesh,x3=x.mesh)
> X1 <- predict(b,newd,type="lpmatrix")
> 
> Xp <- (X1-X0)/eps ## maps coefficients to (fd approx.) derivatives
> colnames(Xp)      ## can check which cols relate to which smooth
 [1] "(Intercept)" "s(x0).1"     "s(x0).2"     "s(x0).3"     "s(x0).4"    
 [6] "s(x0).5"     "s(x0).6"     "s(x0).7"     "s(x0).8"     "s(x0).9"    
[11] "s(x1).1"     "s(x1).2"     "s(x1).3"     "s(x1).4"     "s(x1).5"    
[16] "s(x1).6"     "s(x1).7"     "s(x1).8"     "s(x1).9"     "s(x2).1"    
[21] "s(x2).2"     "s(x2).3"     "s(x2).4"     "s(x2).5"     "s(x2).6"    
[26] "s(x2).7"     "s(x2).8"     "s(x2).9"     "s(x3).1"     "s(x3).2"    
[31] "s(x3).3"     "s(x3).4"     "s(x3).5"     "s(x3).6"     "s(x3).7"    
[36] "s(x3).8"     "s(x3).9"    
> 
> par(mfrow=c(2,2))
> for (i in 1:4) {  ## plot derivatives and corresponding CIs
+   Xi <- Xp*0 
+   Xi[,(i-1)*9+1:9+1] <- Xp[,(i-1)*9+1:9+1] ## Xi%*%coef(b) = smooth deriv i
+   df <- Xi%*%coef(b)              ## ith smooth derivative 
+   df.sd <- rowSums(Xi%*%b$Vp*Xi)^.5 ## cheap diag(Xi%*%b$Vp%*%t(Xi))^.5
+   plot(x.mesh,df,type="l",ylim=range(c(df+2*df.sd,df-2*df.sd)))
+   lines(x.mesh,df+2*df.sd,lty=2);lines(x.mesh,df-2*df.sd,lty=2)
+ }
> 
> 
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("qq.gam")
> ### * qq.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: qq.gam
> ### Title: QQ plots for gam model residuals
> ### Aliases: qq.gam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> 
> library(mgcv)
> ## simulate binomial data...
> set.seed(0)
> n.samp <- 400
> dat <- gamSim(1,n=n.samp,dist="binary",scale=.33)
Gu & Wahba 4 term additive model
> p <- binomial()$linkinv(dat$f) ## binomial p
> n <- sample(c(1,3),n.samp,replace=TRUE) ## binomial n
> dat$y <- rbinom(n,n,p)
> dat$n <- n
> 
> lr.fit <- gam(y/n~s(x0)+s(x1)+s(x2)+s(x3)
+              ,family=binomial,data=dat,weights=n,method="REML")
> 
> par(mfrow=c(2,2))
> ## normal QQ-plot of deviance residuals
> qqnorm(residuals(lr.fit),pch=19,cex=.3)
> ## Quick QQ-plot of deviance residuals
> qq.gam(lr.fit,pch=19,cex=.3)
> ## Simulation based QQ-plot with reference bands 
> qq.gam(lr.fit,rep=100,level=.9)
> ## Simulation based QQ-plot, Pearson resids, all
> ## simulated reference plots shown...  
> qq.gam(lr.fit,rep=100,level=1,type="pearson",pch=19,cex=.2)
> 
> ## Now fit the wrong model and check....
> 
> pif <- gam(y~s(x0)+s(x1)+s(x2)+s(x3)
+              ,family=poisson,data=dat,method="REML")
> par(mfrow=c(2,2))
> qqnorm(residuals(pif),pch=19,cex=.3)
> qq.gam(pif,pch=19,cex=.3)
> qq.gam(pif,rep=100,level=.9)
> qq.gam(pif,rep=100,level=1,type="pearson",pch=19,cex=.2)
> 
> ## Example of binary data model violation so gross that you see a problem 
> ## on the QQ plot...
> 
> y <- c(rep(1,10),rep(0,20),rep(1,40),rep(0,10),rep(1,40),rep(0,40))
> x <- 1:160
> b <- glm(y~x,family=binomial)
> par(mfrow=c(2,2))
> ## Note that the next two are not necessarily similar under gross 
> ## model violation...
> qq.gam(b)
> qq.gam(b,rep=50,level=1)
> ## and a much better plot for detecting the problem
> plot(x,residuals(b),pch=19,cex=.3)
> plot(x,y);lines(x,fitted(b))
> 
> ## alternative model
> b <- gam(y~s(x,k=5),family=binomial,method="ML")
> qq.gam(b)
> qq.gam(b,rep=50,level=1)
> plot(x,residuals(b),pch=19,cex=.3)
> plot(b,residuals=TRUE,pch=19,cex=.3)
> 
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("rTweedie")
> ### * rTweedie
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rTweedie
> ### Title: Generate Tweedie random deviates
> ### Aliases: rTweedie
> ### Keywords: models regression
> 
> ### ** Examples
> 
>  library(mgcv)
>  f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 *
+             (10 * x)^3 * (1 - x)^10
>  n <- 300
>  x <- runif(n)
>  mu <- exp(f2(x)/3+.1);x <- x*10 - 4
>  y <- rTweedie(mu,p=1.5,phi=1.3)
>  b <- gam(y~s(x,k=20),family=Tweedie(p=1.5))
>  b

Family: Tweedie(1.5) 
Link function: log 

Formula:
y ~ s(x, k = 20)

Estimated degrees of freedom:
7.12  total = 8.12 

GCV score: 1.598862     
>  plot(b) 
> 
> 
> 
> 
> cleanEx()
> nameEx("random.effects")
> ### * random.effects
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: random.effects
> ### Title: Random effects in GAMs
> ### Aliases: random.effects
> ### Keywords: regression
> 
> ### ** Examples
> 
> ## see also examples for gam.models, gam.vcomp, gamm and smooth.construct.re.smooth.spec
> 
> ## simple comparison of lme and gam
> require(mgcv)
> require(nlme)
> b0 <- lme(travel~1,data=Rail,~1|Rail,method="REML") 
> 
> b <- gam(travel~s(Rail,bs="re"),data=Rail,method="REML")
> 
> intervals(b0)
Approximate 95% confidence intervals

 Fixed effects:
               lower est.    upper
(Intercept) 44.33921 66.5 88.66079
attr(,"label")
[1] "Fixed effects:"

 Random Effects:
  Level: Rail 
                   lower     est.    upper
sd((Intercept)) 13.27434 24.80547 46.35341

 Within-group standard error:
   lower     est.    upper 
2.695007 4.020779 5.998747 
> gam.vcomp(b)

Standard deviations and 0.95 confidence intervals:

          std.dev     lower     upper
s(Rail) 24.805465 13.274315 46.353510
scale    4.020779  2.695004  5.998753

Rank: 2/2
> anova(b)

Family: gaussian 
Link function: identity 

Formula:
travel ~ s(Rail, bs = "re")

Approximate significance of smooth terms:
          edf Ref.df     F p-value
s(Rail) 4.957  5.000 114.2  <2e-16
> 
> 
> 
> 
> cleanEx()
> nameEx("rig")
> ### * rig
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rig
> ### Title: Generate inverse Gaussian random deviates
> ### Aliases: rig
> 
> ### ** Examples
> 
> require(mgcv)
> set.seed(7)
> ## An inverse.gaussian GAM example, by modify `gamSim' output... 
> dat <- gamSim(1,n=400,dist="normal",scale=1)
Gu & Wahba 4 term additive model
> dat$f <- dat$f/4 ## true linear predictor 
> Ey <- exp(dat$f);scale <- .5 ## mean and GLM scale parameter
> ## simulate inverse Gaussian response...
> dat$y <- rig(Ey,mean=Ey,scale=.2)
> big <- gam(y~ s(x0)+ s(x1)+s(x2)+s(x3),family=inverse.gaussian(link=log),
+           data=dat,method="REML")
> plot(big,pages=1)
> gam.check(big)

Method: REML   Optimizer: outer newton
full convergence after 10 iterations.
Gradient range [-6.688524e-06,9.46893e-08]
(score 1154.207 & scale 0.1808153).
Hessian positive definite, eigenvalue range [0.2484775,197.55].
Model rank =  37 / 37 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

         k'   edf k-index p-value
s(x0) 9.000 3.020   0.757    0.08
s(x1) 9.000 2.985   0.892    0.94
s(x2) 9.000 6.412   0.846    0.70
s(x3) 9.000 1.887   0.893    0.92
> summary(big)

Family: inverse.gaussian 
Link function: log 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.93015    0.06261   30.83   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df      F  p-value    
s(x0) 3.020  3.744  3.894  0.00551 ** 
s(x1) 2.985  3.712 20.370 4.40e-14 ***
s(x2) 6.412  7.524 12.139 7.37e-15 ***
s(x3) 1.887  2.343  1.637  0.16889    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.0874   Deviance explained = 38.6%
-REML = 1154.2  Scale est. = 0.18082   n = 400
> 
> 
> 
> cleanEx()
> nameEx("rmvn")
> ### * rmvn
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rmvn
> ### Title: Generate multivariate normal deviates
> ### Aliases: rmvn
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> V <- matrix(c(2,1,1,2),2,2) 
> mu <- c(1,3)
> n <- 1000
> z <- rmvn(n,mu,V)
> crossprod(sweep(z,2,colMeans(z)))/n ## observed covariance matrix
         [,1]     [,2]
[1,] 2.273002 1.135038
[2,] 1.135038 2.084711
> colMeans(z) ## observed mu 
[1] 0.9353139 2.9894941
> 
> 
> 
> cleanEx()
> nameEx("s")
> ### * s
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: s
> ### Title: Defining smooths in GAM formulae
> ### Aliases: s
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> # example utilising `by' variables
> library(mgcv)
> set.seed(0)
> n<-200;sig2<-4
> x1 <- runif(n, 0, 1);x2 <- runif(n, 0, 1);x3 <- runif(n, 0, 1)
> fac<-c(rep(1,n/2),rep(2,n/2)) # create factor
> fac.1<-rep(0,n)+(fac==1);fac.2<-1-fac.1 # and dummy variables
> fac<-as.factor(fac)
> f1 <-  exp(2 * x1) - 3.75887
> f2 <-  0.2 * x1^11 * (10 * (1 - x1))^6 + 10 * (10 * x1)^3 * (1 - x1)^10
> f<-f1*fac.1+f2*fac.2+x2
> e <- rnorm(n, 0, sqrt(abs(sig2)))
> y <- f + e
> # NOTE: smooths will be centered, so need to include fac in model....
> b<-gam(y~fac+s(x1,by=fac)+x2) 
> plot(b,pages=1)
> 
> 
> 
> cleanEx()
> nameEx("scat")
> ### * scat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: scat
> ### Title: GAM scaled t family for heavy tailed data
> ### Aliases: scat t.scaled
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## Simulate some t data...
> set.seed(3);n<-400
> dat <- gamSim(1,n=n)
Gu & Wahba 4 term additive model
> dat$y <- dat$f + rt(n,df=3)*2
> 
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=scat(link="identity"),data=dat)
> 
> b

Family: Scaled t(2.757,1.798) 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
3.85 2.85 7.82 1.87  total = 17.4 

REML score: 979.2437     
> plot(b,pages=1)
> 
> 
> 
> 
> cleanEx()
> nameEx("sdiag")
> ### * sdiag
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sdiag
> ### Title: Extract of modify diagonals of a matrix
> ### Aliases: sdiag sdiag<-
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> A <- matrix(1:35,7,5)
> A
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    8   15   22   29
[2,]    2    9   16   23   30
[3,]    3   10   17   24   31
[4,]    4   11   18   25   32
[5,]    5   12   19   26   33
[6,]    6   13   20   27   34
[7,]    7   14   21   28   35
> sdiag(A,1) ## first super diagonal
[1]  8 16 24 32
> sdiag(A,-1) ## first sub diagonal
[1]  2 10 18 26 34
> 
> sdiag(A) <- 1 ## leading diagonal set to 1
> sdiag(A,3) <- c(-1,-2) ## set 3rd super diagonal 
> 
> 
> 
> 
> cleanEx()
> nameEx("single.index")
> ### * single.index
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: single.index
> ### Title: Single index models with mgcv
> ### Aliases: single.index
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv)
> 
> si <- function(theta,y,x,z,opt=TRUE,k=10,fx=FALSE) {
+ ## Fit single index model using gam call, given theta (defines alpha). 
+ ## Return ML is opt==TRUE and fitted gam with theta added otherwise.
+ ## Suitable for calling from 'optim' to find optimal theta/alpha.
+   alpha <- c(1,theta) ## constrained alpha defined using free theta
+   kk <- sqrt(sum(alpha^2))
+   alpha <- alpha/kk  ## so now ||alpha||=1
+   a <- x%*%alpha     ## argument of smooth
+   b <- gam(y~s(a,fx=fx,k=k)+s(z),family=poisson,method="ML") ## fit model
+   if (opt) return(b$gcv.ubre) else {
+     b$alpha <- alpha  ## add alpha
+     J <- outer(alpha,-theta/kk^2) ## compute Jacobian
+     for (j in 1:length(theta)) J[j+1,j] <- J[j+1,j] + 1/kk
+     b$J <- J ## dalpha_i/dtheta_j 
+     return(b)
+   }
+ } ## si
> 
> ## simulate some data from a single index model...
> 
> set.seed(1)
> f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
+             (10 * x)^3 * (1 - x)^10
> n <- 200;m <- 3
> x <- matrix(runif(n*m),n,m) ## the covariates for the single index part
> z <- runif(n) ## another covariate
> alpha <- c(1,-1,.5); alpha <- alpha/sqrt(sum(alpha^2))
> eta <- as.numeric(f2((x%*%alpha+.41)/1.4)+1+z^2*2)/4
> mu <- exp(eta)
> y <- rpois(n,mu) ## Poi response 
> 
> ## now fit to the simulated data...
> 
> 
> th0 <- c(-.8,.4) ## close to truth for speed
> ## get initial theta, using no penalization...
> f0 <- nlm(si,th0,y=y,x=x,z=z,fx=TRUE,k=5)
> ## now get theta/alpha with smoothing parameter selection...
> f1 <- nlm(si,f0$estimate,y=y,x=x,z=z,hessian=TRUE,k=10)
> theta.est <-f1$estimate 
> 
> ## Alternative using 'optim' ('Not run' simply to keep
> ## CRAN check time down)... 
> ## Not run: 
> ##D th0 <- rep(0,m-1) 
> ##D ## get initial theta, using no penalization...
> ##D f0 <- optim(th0,si,y=y,x=x,z=z,fx=TRUE,k=5)
> ##D ## now get theta/alpha with smoothing parameter selection...
> ##D f1 <- optim(f0$par,si,y=y,x=x,z=z,hessian=TRUE,k=10)
> ##D theta.est <-f1$par 
> ## End(Not run)
> ## extract and examine fitted model...
> 
> b <- si(theta.est,y,x,z,opt=FALSE) ## extract best fit model
> plot(b,pages=1)
> b

Family: poisson 
Link function: log 

Formula:
y ~ s(a, fx = fx, k = k) + s(z)

Estimated degrees of freedom:
7.3 1.0  total = 9.3 

ML score: 468.4854     
> b$alpha 
[1]  0.6160274 -0.6987148  0.3637415
> ## get sd for alpha...
> Vt <- b$J%*%solve(f1$hessian,t(b$J))
> diag(Vt)^.5
[1] 0.01655776 0.01298555 0.01968088
> 
> 
> 
> 
> cleanEx()
> nameEx("slanczos")
> ### * slanczos
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: slanczos
> ### Title: Compute truncated eigen decomposition of a symmetric matrix
> ### Aliases: slanczos
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
>  require(mgcv)
>  ## create some x's and knots...
>  set.seed(1);
>  n <- 700;A <- matrix(runif(n*n),n,n);A <- A+t(A)
>  
>  ## compare timings of slanczos and eigen
>  system.time(er <- slanczos(A,10))
   user  system elapsed 
  0.068   0.000   0.070 
>  system.time(um <- eigen(A,symmetric=TRUE))
   user  system elapsed 
  0.460   0.000   0.462 
>  
>  ## confirm values are the same...
>  ind <- c(1:6,(n-3):n)
>  range(er$values-um$values[ind]);range(abs(er$vectors)-abs(um$vectors[,ind]))
[1] -2.145839e-12  1.477929e-12
[1] -1.246456e-07  1.332938e-07
> 
> 
> 
> cleanEx()
> nameEx("smooth.construct")
> ### * smooth.construct
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct
> ### Title: Constructor functions for smooth terms in a GAM
> ### Aliases: smooth.construct smooth.construct2 user.defined.smooth
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## Adding a penalized truncated power basis class and methods
> ## as favoured by Ruppert, Wand and Carroll (2003) 
> ## Semiparametric regression CUP. (No advantage to actually
> ## using this, since mgcv can happily handle non-identity 
> ## penalties.)
> 
> smooth.construct.tr.smooth.spec<-function(object,data,knots)
+ ## a truncated power spline constructor method function
+ ## object$p.order = null space dimension
+ { m <- object$p.order[1]
+   if (is.na(m)) m <- 2 ## default 
+   if (m<1) stop("silly m supplied")
+   if (object$bs.dim<0) object$bs.dim <- 10 ## default
+   nk<-object$bs.dim-m-1 ## number of knots
+   if (nk<=0) stop("k too small for m")
+   x <- data[[object$term]]  ## the data
+   x.shift <- mean(x) # shift used to enhance stability
+   k <- knots[[object$term]] ## will be NULL if none supplied
+   if (is.null(k)) # space knots through data
+   { n<-length(x)
+     k<-quantile(x[2:(n-1)],seq(0,1,length=nk+2))[2:(nk+1)]
+   }
+   if (length(k)!=nk) # right number of knots?
+   stop(paste("there should be ",nk," supplied knots"))
+   x <- x - x.shift # basis stabilizing shift
+   k <- k - x.shift # knots treated the same!
+   X<-matrix(0,length(x),object$bs.dim)
+   for (i in 1:(m+1)) X[,i] <- x^(i-1)
+   for (i in 1:nk) X[,i+m+1]<-(x-k[i])^m*as.numeric(x>k[i])
+   object$X<-X # the finished model matrix
+   if (!object$fixed) # create the penalty matrix
+   { object$S[[1]]<-diag(c(rep(0,m+1),rep(1,nk)))
+   }
+   object$rank<-nk  # penalty rank
+   object$null.space.dim <- m+1  # dim. of unpenalized space
+   ## store "tr" specific stuff ...
+   object$knots<-k;object$m<-m;object$x.shift <- x.shift
+  
+   object$df<-ncol(object$X)     # maximum DoF (if unconstrained)
+  
+   class(object)<-"tr.smooth"  # Give object a class
+   object
+ }
> 
> Predict.matrix.tr.smooth<-function(object,data)
+ ## prediction method function for the `tr' smooth class
+ { x <- data[[object$term]]
+   x <- x - object$x.shift # stabilizing shift
+   m <- object$m;     # spline order (3=cubic)
+   k<-object$knots    # knot locations
+   nk<-length(k)      # number of knots
+   X<-matrix(0,length(x),object$bs.dim)
+   for (i in 1:(m+1)) X[,i] <- x^(i-1)
+   for (i in 1:nk) X[,i+m+1] <- (x-k[i])^m*as.numeric(x>k[i])
+   X # return the prediction matrix
+ }
> 
> # an example, using the new class....
> require(mgcv)
> set.seed(100)
> dat <- gamSim(1,n=400,scale=2)
Gu & Wahba 4 term additive model
> b<-gam(y~s(x0,bs="tr",m=2)+s(x1,bs="ps",m=c(1,3))+
+          s(x2,bs="tr",m=3)+s(x3,bs="tr",m=2),data=dat)
> plot(b,pages=1)
> b<-gamm(y~s(x0,bs="tr",m=2)+s(x1,bs="ps",m=c(1,3))+
+          s(x2,bs="tr",m=3)+s(x3,bs="tr",m=2),data=dat)
> plot(b$gam,pages=1)
> # another example using tensor products of the new class
> dat <- gamSim(2,n=400,scale=.1)$data
Bivariate smoothing example
> b <- gam(y~te(x,z,bs=c("tr","tr"),m=c(2,2)),data=dat)
> vis.gam(b)
> 
> 
> 
> cleanEx()
> nameEx("smooth.construct.ad.smooth.spec")
> ### * smooth.construct.ad.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.ad.smooth.spec
> ### Title: Adaptive smooths in GAMs
> ### Aliases: smooth.construct.ad.smooth.spec adaptive.smooth
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Comparison using an example taken from AdaptFit
> ## library(AdaptFit)
> require(mgcv)
> set.seed(0)
> x <- 1:1000/1000
> mu <- exp(-400*(x-.6)^2)+5*exp(-500*(x-.75)^2)/3+2*exp(-500*(x-.9)^2)
> y <- mu+0.5*rnorm(1000)
> 
> ##fit with default knots
> ## y.fit <- asp(y~f(x))
> 
> par(mfrow=c(2,2))
> ## plot(y.fit,main=round(cor(fitted(y.fit),mu),digits=4))
> ## lines(x,mu,col=2)
> 
> b <- gam(y~s(x,bs="ad",k=40,m=5)) ## adaptive
> plot(b,shade=TRUE,main=round(cor(fitted(b),mu),digits=4))
> lines(x,mu-mean(mu),col=2)
>  
> b <- gam(y~s(x,k=40))             ## non-adaptive
> plot(b,shade=TRUE,main=round(cor(fitted(b),mu),digits=4))
> lines(x,mu-mean(mu),col=2)
> 
> b <- gam(y~s(x,bs="ad",k=40,m=5,xt=list(bs="cr")))
> plot(b,shade=TRUE,main=round(cor(fitted(b),mu),digits=4))
> lines(x,mu-mean(mu),col=2)
> 
> ## A 2D example (marked, 'Not run' purely to reduce
> ## checking load on CRAN).
> ## Not run: 
> ##D par(mfrow=c(2,2),mar=c(1,1,1,1))
> ##D x <- seq(-.5, 1.5, length= 60)
> ##D z <- x
> ##D f3 <- function(x,z,k=15) { r<-sqrt(x^2+z^2);f<-exp(-r^2*k);f}  
> ##D f <- outer(x, z, f3)
> ##D op <- par(bg = "white")
> ##D 
> ##D ## Plot truth....
> ##D persp(x,z,f,theta=30,phi=30,col="lightblue",ticktype="detailed")
> ##D 
> ##D n <- 2000
> ##D x <- runif(n)*2-.5
> ##D z <- runif(n)*2-.5
> ##D f <- f3(x,z)
> ##D y <- f + rnorm(n)*.1
> ##D 
> ##D ## Try tprs for comparison...
> ##D b0 <- gam(y~s(x,z,k=150))
> ##D vis.gam(b0,theta=30,phi=30,ticktype="detailed")
> ##D 
> ##D ## Tensor product with non-adaptive version of adaptive penalty
> ##D b1 <- gam(y~s(x,z,bs="ad",k=15,m=1),gamma=1.4)
> ##D vis.gam(b1,theta=30,phi=30,ticktype="detailed")
> ##D 
> ##D ## Now adaptive...
> ##D b <- gam(y~s(x,z,bs="ad",k=15,m=3),gamma=1.4)
> ##D vis.gam(b,theta=30,phi=30,ticktype="detailed")
> ##D cor(fitted(b0),f);cor(fitted(b),f)
> ## End(Not run)
> 
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.bs.smooth.spec")
> ### * smooth.construct.bs.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.bs.smooth.spec
> ### Title: Penalized B-splines in GAMs
> ### Aliases: smooth.construct.bs.smooth.spec Predict.matrix.Bspline.smooth
> ###   b.spline
> ### Keywords: models regression
> 
> ### ** Examples
> 
>   require(mgcv)
>   set.seed(5)
>   dat <- gamSim(1,n=400,dist="normal",scale=2)
Gu & Wahba 4 term additive model
>   bs <- "bs"
>   b <- gam(y~s(x0,bs=bs,m=c(4,2))+s(x1,bs=bs)+s(x2,k=15,bs=bs,m=c(4,3))+
+            s(x3,bs=bs,m=c(1,0)),data=dat,method="REML")
>   plot(b,pages=1)
> 
>   ## construct smooth of x. Model matrix sm$X and penalty 
>   ## matrix sm$S[[1]] will have many zero entries...
>   x <- seq(0,1,length=100)
>   sm <- smoothCon(s(x,bs="bs"),data.frame(x))[[1]]
> 
>   ## another example checking penalty numerically...
>   m <- c(4,2); k <- 15; b <- runif(k)
>   sm <- smoothCon(s(x,bs="bs",m=m,k=k),data.frame(x),scale.penalty=FALSE)[[1]]
>   sm$deriv <- m[2]
>   h0 <- 1e-3; xk <- sm$knots[(m[1]+1):(k+1)]
>   Xp <- PredictMat(sm,data.frame(x=seq(xk[1]+h0/2,max(xk)-h0/2,h0)))
>   sum((Xp%*%b)^2*h0) ## numerical approximation to penalty
[1] 2326
>   b%*%sm$S[[1]]%*%b  ## `exact' version
         [,1]
[1,] 2325.999
>   
> 
> 
> 
> cleanEx()
> nameEx("smooth.construct.cr.smooth.spec")
> ### * smooth.construct.cr.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.cr.smooth.spec
> ### Title: Penalized Cubic regression splines in GAMs
> ### Aliases: smooth.construct.cr.smooth.spec
> ###   smooth.construct.cs.smooth.spec smooth.construct.cc.smooth.spec
> ###   cubic.regression.spline cyclic.cubic.spline
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## cyclic spline example...
>   require(mgcv)
>   set.seed(6)
>   x <- sort(runif(200)*10)
>   z <- runif(200)
>   f <- sin(x*2*pi/10)+.5
>   y <- rpois(exp(f),exp(f)) 
> 
> ## finished simulating data, now fit model...
>   b <- gam(y ~ s(x,bs="cc",k=12) + s(z),family=poisson,
+                       knots=list(x=seq(0,10,length=12)))
> ## or more simply
>    b <- gam(y ~ s(x,bs="cc",k=12) + s(z),family=poisson,
+                       knots=list(x=c(0,10)))
> 
> ## plot results...
>   par(mfrow=c(2,2))
>   plot(x,y);plot(b,select=1,shade=TRUE);lines(x,f-mean(f),col=2)
>   plot(b,select=2,shade=TRUE);plot(fitted(b),residuals(b))
>   
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.ds.smooth.spec")
> ### * smooth.construct.ds.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.ds.smooth.spec
> ### Title: Low rank Duchon 1977 splines
> ### Aliases: smooth.construct.ds.smooth.spec Predict.matrix.duchon.spline
> ###   Duchon.spline
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv)
> eg <- gamSim(2,n=200,scale=.05)
Bivariate smoothing example
> attach(eg)
> op <- par(mfrow=c(2,2),mar=c(4,4,1,1))
> b0 <- gam(y~s(x,z,bs="ds",m=c(2,0),k=50),data=data)  ## tps
> b <- gam(y~s(x,z,bs="ds",m=c(1,.5),k=50),data=data)  ## first deriv penalty
> b1 <- gam(y~s(x,z,bs="ds",m=c(2,.5),k=50),data=data) ## modified 2nd deriv
> 
> persp(truth$x,truth$z,truth$f,theta=30) ## truth
> vis.gam(b0,theta=30)
> vis.gam(b,theta=30)
> vis.gam(b1,theta=30)
> 
> detach(eg)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.fs.smooth.spec")
> ### * smooth.construct.fs.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.fs.smooth.spec
> ### Title: Factor smooth interactions in GAMs
> ### Aliases: smooth.construct.fs.smooth.spec Predict.matrix.fs.interaction
> ###   factor.smooth.interaction
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(0)
> ## simulate data...
> f0 <- function(x) 2 * sin(pi * x)
> f1 <- function(x,a=2,b=-1) exp(a * x)+b
> f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
+             (10 * x)^3 * (1 - x)^10
> n <- 500;nf <- 25
> fac <- sample(1:nf,n,replace=TRUE)
> x0 <- runif(n);x1 <- runif(n);x2 <- runif(n)
> a <- rnorm(nf)*.2 + 2;b <- rnorm(nf)*.5
> f <- f0(x0) + f1(x1,a[fac],b[fac]) + f2(x2)
> fac <- factor(fac)
> y <- f + rnorm(n)*2
> ## so response depends on global smooths of x0 and 
> ## x2, and a smooth of x1 for each level of fac.
> 
> ## fit model (note p-values not available when fit 
> ## using gamm)...
> bm <- gamm(y~s(x0)+ s(x1,fac,bs="fs",k=5)+s(x2,k=20))
> plot(bm$gam,pages=1)
> summary(bm$gam)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1, fac, bs = "fs", k = 5) + s(x2, k = 20)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.2938     0.1382   52.76   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
             edf Ref.df     F  p-value    
s(x0)      3.222  3.222 13.47 9.94e-09 ***
s(x1,fac) 37.070     NA    NA       NA    
s(x2)     10.251 10.251 93.50  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.739   
  Scale est. = 4.0572    n = 500
> 
> ## Could also use...
> ## b <- gam(y~s(x0)+ s(x1,fac,bs="fs",k=5)+s(x2,k=20),method="ML")
> ## ... but its slower (increasingly so with increasing nf)
> ## b <- gam(y~s(x0)+ t2(x1,fac,bs=c("tp","re"),k=5,full=TRUE)+
> ##        s(x2,k=20),method="ML"))
> ## ... is exactly equivalent. 
> 
> 
> 
> cleanEx()
> nameEx("smooth.construct.gp.smooth.spec")
> ### * smooth.construct.gp.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.gp.smooth.spec
> ### Title: Low rank Gaussian process smooths
> ### Aliases: smooth.construct.gp.smooth.spec Predict.matrix.gp.smooth
> ###   gp.smooth
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv)
> eg <- gamSim(2,n=200,scale=.05)
Bivariate smoothing example
> attach(eg)
> op <- par(mfrow=c(2,2),mar=c(4,4,1,1))
> b0 <- gam(y~s(x,z,k=50),data=data)  ## tps
> b <- gam(y~s(x,z,bs="gp",k=50),data=data)  ## Matern spline default range
> b1 <- gam(y~s(x,z,bs="gp",k=50,m=c(1,.5)),data=data)  ## spherical 
> 
> persp(truth$x,truth$z,truth$f,theta=30) ## truth
> vis.gam(b0,theta=30)
> vis.gam(b,theta=30)
> vis.gam(b1,theta=30)
> 
> detach(eg)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.mrf.smooth.spec")
> ### * smooth.construct.mrf.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.mrf.smooth.spec
> ### Title: Markov Random Field Smooths
> ### Aliases: smooth.construct.mrf.smooth.spec Predict.matrix.mrf.smooth mrf
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## Load Columbus Ohio crime data (see ?columbus for details and credits)
> data(columb)       ## data frame
> data(columb.polys) ## district shapes list
> xt <- list(polys=columb.polys) ## neighbourhood structure info for MRF
> par(mfrow=c(2,2))
> ## First a full rank MRF...
> b <- gam(crime ~ s(district,bs="mrf",xt=xt),data=columb,method="REML")
> plot(b,scheme=1)
> ## Compare to reduced rank version...
> b <- gam(crime ~ s(district,bs="mrf",k=20,xt=xt),data=columb,method="REML")
> plot(b,scheme=1)
> ## An important covariate added...
> b <- gam(crime ~ s(district,bs="mrf",k=20,xt=xt)+s(income),
+          data=columb,method="REML")
> plot(b,scheme=c(0,1))
> 
> ## plot fitted values by district
> par(mfrow=c(1,1))
> fv <- fitted(b)
> names(fv) <- as.character(columb$district)
> polys.plot(columb.polys,fv)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.ps.smooth.spec")
> ### * smooth.construct.ps.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.ps.smooth.spec
> ### Title: P-splines in GAMs
> ### Aliases: smooth.construct.ps.smooth.spec
> ###   smooth.construct.cp.smooth.spec p.spline cyclic.p.spline
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## see ?gam
> ## cyclic example ...
>   require(mgcv)
>   set.seed(6)
>   x <- sort(runif(200)*10)
>   z <- runif(200)
>   f <- sin(x*2*pi/10)+.5
>   y <- rpois(exp(f),exp(f)) 
> 
> ## finished simulating data, now fit model...
>   b <- gam(y ~ s(x,bs="cp") + s(z,bs="ps"),family=poisson)
> 
> ## example with supplied knot ranges for x and z (can do just one)
>   b <- gam(y ~ s(x,bs="cp") + s(z,bs="ps"),family=poisson,
+            knots=list(x=c(0,10),z=c(0,1))) 
> 
> ## example with supplied knots...
>   bk <- gam(y ~ s(x,bs="cp",k=12) + s(z,bs="ps",k=13),family=poisson,
+                       knots=list(x=seq(0,10,length=13),z=(-3):13/10))
> 
> ## plot results...
>   par(mfrow=c(2,2))
>   plot(b,select=1,shade=TRUE);lines(x,f-mean(f),col=2)
>   plot(b,select=2,shade=TRUE);lines(z,0*z,col=2)
>   plot(bk,select=1,shade=TRUE);lines(x,f-mean(f),col=2)
>   plot(bk,select=2,shade=TRUE);lines(z,0*z,col=2)
>   
> ## Example using montonic constraints via the SCOP-spline
> ## construction, and of computng derivatives...
>   x <- seq(0,1,length=100); dat <- data.frame(x)
>   sspec <- s(x,bs="ps")
>   sspec$mono <- 1
>   sm <- smoothCon(sspec,dat)[[1]]
>   sm$deriv <- 1
>   Xd <- PredictMat(sm,dat)
> ## generate random coeffients in the unconstrainted 
> ## parameterization...
>   b <- runif(10)*3-2.5
> ## exponentiate those parameters indicated by sm$g.index 
> ## to obtain coefficients meeting the constraints...
>   b[sm$g.index] <- exp(b[sm$g.index]) 
> ## plot monotonic spline and its derivative
>   par(mfrow=c(2,2))
>   plot(x,sm$X%*%b,type="l",ylab="f(x)")
>   plot(x,Xd%*%b,type="l",ylab="f'(x)")
> ## repeat for decrease...
>   sspec$mono <- -1
>   sm1 <- smoothCon(sspec,dat)[[1]]
>   sm1$deriv <- 1
>   Xd1 <- PredictMat(sm1,dat)
>   plot(x,sm1$X%*%b,type="l",ylab="f(x)")
>   plot(x,Xd1%*%b,type="l",ylab="f'(x)")
> 
> ## Now with sum to zero constraints as well...
>   sspec$mono <- 1
>   sm <- smoothCon(sspec,dat,absorb.cons=TRUE)[[1]]
>   sm$deriv <- 1
>   Xd <- PredictMat(sm,dat)
>   b <- b[-1] ## dropping first param
>   plot(x,sm$X%*%b,type="l",ylab="f(x)")
>   plot(x,Xd%*%b,type="l",ylab="f'(x)")
>   
>   sspec$mono <- -1
>   sm1 <- smoothCon(sspec,dat,absorb.cons=TRUE)[[1]]
>   sm1$deriv <- 1
>   Xd1 <- PredictMat(sm1,dat)
>   plot(x,sm1$X%*%b,type="l",ylab="f(x)")
>   plot(x,Xd1%*%b,type="l",ylab="f'(x)")
>   
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.re.smooth.spec")
> ### * smooth.construct.re.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.re.smooth.spec
> ### Title: Simple random effects in GAMs
> ### Aliases: smooth.construct.re.smooth.spec Predict.matrix.random.effect
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## see ?gam.vcomp
> 
> require(mgcv)
> ## simulate simple random effect example
> set.seed(4)
> nb <- 50; n <- 400
> b <- rnorm(nb)*2 ## random effect
> r <- sample(1:nb,n,replace=TRUE) ## r.e. levels
> y <- 2 + b[r] + rnorm(n)
> r <- factor(r)
> ## fit model....
> b <- gam(y ~ s(r,bs="re"),method="REML")
> gam.vcomp(b)

Standard deviations and 0.95 confidence intervals:

        std.dev     lower    upper
s(r)  1.8604844 1.5149636 2.284809
scale 0.9836892 0.9134534 1.059325

Rank: 2/2
> 
> ## example with supplied precision matrices...
> b <- c(rnorm(nb/2)*2,rnorm(nb/2)*.5) ## random effect now with 2 variances
> r <- sample(1:nb,n,replace=TRUE) ## r.e. levels
> y <- 2 + b[r] + rnorm(n)
> r <- factor(r)
> ## known precision matrix components...
> S <- list(diag(rep(c(1,0),each=nb/2)),diag(rep(c(0,1),each=nb/2)))
> b <- gam(y ~ s(r,bs="re",xt=list(S=S,rank=c(nb/2,nb/2))),method="REML")
> gam.vcomp(b)

Standard deviations and 0.95 confidence intervals:

        std.dev     lower    upper
s(r)1 1.6675587 1.2420232 2.238889
s(r)2 0.4328550 0.2695872 0.695001
scale 0.9939163 0.9230990 1.070166

Rank: 3/3
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(r, bs = "re", xt = list(S = S, rank = c(nb/2, nb/2)))

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)    1.941      0.107   18.14   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
      edf Ref.df     F p-value    
s(r) 37.8     49 10.96  <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.574   Deviance explained = 61.4%
-REML = 615.71  Scale est. = 0.98787   n = 400
> 
> 
> 
> cleanEx()
> nameEx("smooth.construct.so.smooth.spec")
> ### * smooth.construct.so.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.so.smooth.spec
> ### Title: Soap film smoother constructer
> ### Aliases: smooth.construct.so.smooth.spec
> ###   smooth.construct.sf.smooth.spec smooth.construct.sw.smooth.spec soap
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> 
> require(mgcv)
> 
> ##########################
> ## simple test function...
> ##########################
> 
> fsb <- list(fs.boundary())
> nmax <- 100
> ## create some internal knots...
> knots <- data.frame(v=rep(seq(-.5,3,by=.5),4),
+                     w=rep(c(-.6,-.3,.3,.6),rep(8,4)))
> ## Simulate some fitting data, inside boundary...
> set.seed(0)
> n<-600
> v <- runif(n)*5-1;w<-runif(n)*2-1
> y <- fs.test(v,w,b=1)
> names(fsb[[1]]) <- c("v","w")
> ind <- inSide(fsb,x=v,y=w) ## remove outsiders
> y <- y + rnorm(n)*.3 ## add noise
> y <- y[ind];v <- v[ind]; w <- w[ind] 
> n <- length(y)
> 
> par(mfrow=c(3,2))
> ## plot boundary with knot and data locations
> plot(fsb[[1]]$v,fsb[[1]]$w,type="l");points(knots,pch=20,col=2)
> points(v,w,pch=".");
> 
> ## Now fit the soap film smoother. 'k' is dimension of boundary smooth.
> ## boundary supplied in 'xt', and knots in 'knots'...
>  
> nmax <- 100 ## reduced from default for speed.
> b <- gam(y~s(v,w,k=30,bs="so",xt=list(bnd=fsb,nmax=nmax)),knots=knots)
> 
> plot(b) ## default plot
> plot(b,scheme=1)
> plot(b,scheme=2)
> plot(b,scheme=3)
> 
> vis.gam(b,plot.type="contour")
> 
> ################################
> # Fit same model in two parts...
> ################################
> 
> par(mfrow=c(2,2))
> vis.gam(b,plot.type="contour")
> 
> b1 <- gam(y~s(v,w,k=30,bs="sf",xt=list(bnd=fsb,nmax=nmax))+
+             s(v,w,k=30,bs="sw",xt=list(bnd=fsb,nmax=nmax)) ,knots=knots)
> vis.gam(b,plot.type="contour")
> plot(b1)
>  
> ##################################################
> ## Now an example with known boundary condition...
> ##################################################
> 
> ## Evaluate known boundary condition at boundary nodes...
> fsb[[1]]$f <- fs.test(fsb[[1]]$v,fsb[[1]]$w,b=1,exclude=FALSE)
> 
> ## Now fit the smooth...
> bk <- gam(y~s(v,w,bs="so",xt=list(bnd=fsb,nmax=nmax)),knots=knots)
> plot(bk) ## default plot
> 
> ##########################################
> ## tensor product example (marked 
> ## 'Not run' to reduce CRAN checking load)
> ##########################################
> ## Not run: 
> ##D n <- 10000
> ##D v <- runif(n)*5-1;w<-runif(n)*2-1
> ##D t <- runif(n)
> ##D y <- fs.test(v,w,b=1)
> ##D y <- y + 4.2
> ##D y <- y^(.5+t)
> ##D fsb <- list(fs.boundary())
> ##D names(fsb[[1]]) <- c("v","w")
> ##D ind <- inSide(fsb,x=v,y=w) ## remove outsiders
> ##D y <- y[ind];v <- v[ind]; w <- w[ind]; t <- t[ind] 
> ##D n <- length(y)
> ##D y <- y + rnorm(n)*.05 ## add noise
> ##D knots <- data.frame(v=rep(seq(-.5,3,by=.5),4),
> ##D                     w=rep(c(-.6,-.3,.3,.6),rep(8,4)))
> ##D 
> ##D ## notice NULL element in 'xt' list - to indicate no xt object for "cr" basis...
> ##D bk <- gam(y~ 
> ##D  te(v,w,t,bs=c("sf","cr"),k=c(25,4),d=c(2,1),xt=list(list(bnd=fsb,nmax=nmax),NULL))+
> ##D  te(v,w,t,bs=c("sw","cr"),k=c(25,4),d=c(2,1),xt=list(list(bnd=fsb,nmax=nmax),NULL))
> ##D           ,knots=knots)
> ##D 
> ##D par(mfrow=c(3,2))
> ##D m<-100;n<-50 
> ##D xm <- seq(-1,3.5,length=m);yn<-seq(-1,1,length=n)
> ##D xx <- rep(xm,n);yy<-rep(yn,rep(m,n))
> ##D tru <- matrix(fs.test(xx,yy),m,n)+4.2 ## truth
> ##D 
> ##D image(xm,yn,tru^.5,col=heat.colors(100),xlab="v",ylab="w",
> ##D       main="truth")
> ##D lines(fsb[[1]]$v,fsb[[1]]$w,lwd=3)
> ##D contour(xm,yn,tru^.5,add=TRUE)
> ##D 
> ##D vis.gam(bk,view=c("v","w"),cond=list(t=0),plot.type="contour")
> ##D 
> ##D image(xm,yn,tru,col=heat.colors(100),xlab="v",ylab="w",
> ##D       main="truth")
> ##D lines(fsb[[1]]$v,fsb[[1]]$w,lwd=3)
> ##D contour(xm,yn,tru,add=TRUE)
> ##D 
> ##D vis.gam(bk,view=c("v","w"),cond=list(t=.5),plot.type="contour")
> ##D 
> ##D image(xm,yn,tru^1.5,col=heat.colors(100),xlab="v",ylab="w",
> ##D       main="truth")
> ##D lines(fsb[[1]]$v,fsb[[1]]$w,lwd=3)
> ##D contour(xm,yn,tru^1.5,add=TRUE)
> ##D 
> ##D vis.gam(bk,view=c("v","w"),cond=list(t=1),plot.type="contour")
> ## End(Not run)
> 
> #############################
> # nested boundary example...
> #############################
> 
> bnd <- list(list(x=0,y=0),list(x=0,y=0))
> seq(0,2*pi,length=100) -> theta
> bnd[[1]]$x <- sin(theta);bnd[[1]]$y <- cos(theta)
> bnd[[2]]$x <- .3 + .3*sin(theta);
> bnd[[2]]$y <- .3 + .3*cos(theta)
> plot(bnd[[1]]$x,bnd[[1]]$y,type="l")
> lines(bnd[[2]]$x,bnd[[2]]$y)
> 
> ## setup knots
> k <- 8
> xm <- seq(-1,1,length=k);ym <- seq(-1,1,length=k)
> x=rep(xm,k);y=rep(ym,rep(k,k))
> ind <- inSide(bnd,x,y)
> knots <- data.frame(x=x[ind],y=y[ind])
> points(knots$x,knots$y)
> 
> ## a test function
> 
> f1 <- function(x,y) {
+   exp(-(x-.3)^2-(y-.3)^2)
+ }
> 
> ## plot the test function within the domain 
> par(mfrow=c(2,3))
> m<-100;n<-100 
> xm <- seq(-1,1,length=m);yn<-seq(-1,1,length=n)
> x <- rep(xm,n);y<-rep(yn,rep(m,n))
> ff <- f1(x,y)
> ind <- inSide(bnd,x,y)
> ff[!ind] <- NA
> image(xm,yn,matrix(ff,m,n),xlab="x",ylab="y")
> contour(xm,yn,matrix(ff,m,n),add=TRUE)
> lines(bnd[[1]]$x,bnd[[1]]$y,lwd=2);lines(bnd[[2]]$x,bnd[[2]]$y,lwd=2)
> 
> ## Simulate data by noisy sampling from test function...
> 
> set.seed(1)
> x <- runif(300)*2-1;y <- runif(300)*2-1
> ind <- inSide(bnd,x,y)
> x <- x[ind];y <- y[ind]
> n <- length(x)
> z <- f1(x,y) + rnorm(n)*.1
> 
> ## Fit a soap film smooth to the noisy data
> nmax <- 60
> b <- gam(z~s(x,y,k=c(30,15),bs="so",xt=list(bnd=bnd,nmax=nmax)),knots=knots,method="REML")
> plot(b) ## default plot
> vis.gam(b,plot.type="contour") ## prettier version
> 
> ## trying out separated fits....
> ba <- gam(z~s(x,y,k=c(30,15),bs="sf",xt=list(bnd=bnd,nmax=nmax))+
+           s(x,y,k=c(30,15),bs="sw",xt=list(bnd=bnd,nmax=nmax)),knots=knots,method="REML")
> plot(ba)
> vis.gam(ba,plot.type="contour")
> 
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.sos.smooth.spec")
> ### * smooth.construct.sos.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.sos.smooth.spec
> ### Title: Splines on the sphere
> ### Aliases: smooth.construct.sos.smooth.spec Predict.matrix.sos.smooth
> ###   Spherical.Spline
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv)
> set.seed(0)
> n <- 400
> 
> f <- function(la,lo) { ## a test function...
+   sin(lo)*cos(la-.3)
+ }
> 
> ## generate with uniform density on sphere...  
> lo <- runif(n)*2*pi-pi ## longitude
> la <- runif(3*n)*pi-pi/2
> ind <- runif(3*n)<=cos(la)
> la <- la[ind];
> la <- la[1:n]
> 
> ff <- f(la,lo)
> y <- ff + rnorm(n)*.2 ## test data
> 
> ## generate data for plotting truth...
> lam <- seq(-pi/2,pi/2,length=30)
> lom <- seq(-pi,pi,length=60)
> gr <- expand.grid(la=lam,lo=lom)
> fz <- f(gr$la,gr$lo)
> zm <- matrix(fz,30,60)
> 
> require(mgcv)
> dat <- data.frame(la = la *180/pi,lo = lo *180/pi,y=y)
> 
> ## fit spline on sphere model...
> bp <- gam(y~s(la,lo,bs="sos",k=60),data=dat)
> 
> ## pure knot based alternative...
> ind <- sample(1:n,100)
> bk <- gam(y~s(la,lo,bs="sos",k=60),
+       knots=list(la=dat$la[ind],lo=dat$lo[ind]),data=dat)
> 
> b <- bp
> 
> cor(fitted(b),ff)
[1] 0.9950604
> 
> ## plot results and truth...
> 
> pd <- data.frame(la=gr$la*180/pi,lo=gr$lo*180/pi)
> fv <- matrix(predict(b,pd),30,60)
> 
> par(mfrow=c(2,2),mar=c(4,4,1,1))
> contour(lom,lam,t(zm))
> contour(lom,lam,t(fv))
> plot(bp,rug=FALSE)
> plot(bp,scheme=1,theta=-30,phi=20,pch=19,cex=.5)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.t2.smooth.spec")
> ### * smooth.construct.t2.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.t2.smooth.spec
> ### Title: Tensor product smoothing constructor
> ### Aliases: smooth.construct.t2.smooth.spec
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## see ?t2
> 
> 
> 
> 
> cleanEx()
> nameEx("smooth.construct.tensor.smooth.spec")
> ### * smooth.construct.tensor.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.tensor.smooth.spec
> ### Title: Tensor product smoothing constructor
> ### Aliases: smooth.construct.tensor.smooth.spec
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## see ?gam
> 
> 
> 
> 
> cleanEx()
> nameEx("smooth.construct.tp.smooth.spec")
> ### * smooth.construct.tp.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.tp.smooth.spec
> ### Title: Penalized thin plate regression splines in GAMs
> ### Aliases: smooth.construct.tp.smooth.spec
> ###   smooth.construct.ts.smooth.spec tprs
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv); n <- 100; set.seed(2)
> x <- runif(n); y <- x + x^2*.2 + rnorm(n) *.1
> ## is smooth significantly different from straight line?
> summary(gam(y~s(x,m=c(2,0))+x,method="REML")) ## not quite

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x, m = c(2, 0)) + x

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.02133    0.05315  -0.401    0.689    
x            1.18249    0.10564  11.193   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df     F p-value  
s(x) 0.9334      8 0.304  0.0781 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =   0.91   Deviance explained = 91.1%
-REML = -70.567  Scale est. = 0.012767  n = 100
> ## is smooth significatly different from zero?
> summary(gam(y~s(x),method="REML")) ## yes!

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   0.5600     0.0113   49.56   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
       edf Ref.df     F p-value    
s(x) 1.933  2.417 413.1  <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =   0.91   Deviance explained = 91.1%
-REML = -69.353  Scale est. = 0.012767  n = 100
> ## see ?gam
> 
> 
> 
> cleanEx()
> nameEx("smooth.terms")
> ### * smooth.terms
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.terms
> ### Title: Smooth terms in GAM
> ### Aliases: smooth.terms
> ### Keywords: regression
> 
> ### ** Examples
> 
> ## see examples for gam and gamm
> 
> 
> 
> cleanEx()
> nameEx("smoothCon")
> ### * smoothCon
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smoothCon
> ### Title: Prediction/Construction wrapper functions for GAM smooth terms
> ### Aliases: smoothCon PredictMat
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## example of using smoothCon and PredictMat to set up a basis
> ## to use for regression and make predictions using the result
> library(MASS) ## load for mcycle data.
> ## set up a smoother...
> sm <- smoothCon(s(times,k=10),data=mcycle,knots=NULL)[[1]]
> ## use it to fit a regression spline model...
> beta <- coef(lm(mcycle$accel~sm$X-1))
> with(mcycle,plot(times,accel)) ## plot data
> times <- seq(0,60,length=200)  ## creat prediction times
> ## Get matrix mapping beta to spline prediction at 'times'
> Xp <- PredictMat(sm,data.frame(times=times))
> lines(times,Xp%*%beta) ## add smooth to plot
> 
> ## Same again but using a penalized regression spline of
> ## rank 30....
> sm <- smoothCon(s(times,k=30),data=mcycle,knots=NULL)[[1]]
> E <- t(mroot(sm$S[[1]])) ## square root penalty
> X <- rbind(sm$X,0.1*E) ## augmented model matrix
> y <- c(mcycle$accel,rep(0,nrow(E))) ## augmented data
> beta <- coef(lm(y~X-1)) ## fit penalized regression spline
> Xp <- PredictMat(sm,data.frame(times=times)) ## prediction matrix
> with(mcycle,plot(times,accel)) ## plot data
> lines(times,Xp%*%beta) ## overlay smooth
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’

> nameEx("sp.vcov")
> ### * sp.vcov
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sp.vcov
> ### Title: Extract smoothing parameter estimator covariance matrix from
> ###   (RE)ML GAM fit
> ### Aliases: sp.vcov
> ### Keywords: models smooth regression
> 
> ### ** Examples
>  
> require(mgcv)
> n <- 100
> x <- runif(n);z <- runif(n)
> y <- sin(x*2*pi) + rnorm(n)*.2
> mod <- gam(y~s(x,bs="cc",k=10)+s(z),knots=list(x=seq(0,1,length=10)),
+            method="REML")
> sp.vcov(mod)
           [,1]         [,2]       [,3]
[1,] 0.33857051 4.225473e-02 0.02229567
[2,] 0.04225473 1.391354e+05 0.01094412
[3,] 0.02229567 1.094412e-02 0.02187639
> 
> 
> 
> cleanEx()
> nameEx("step.gam")
> ### * step.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: step.gam
> ### Title: Alternatives to step.gam
> ### Aliases: step.gam
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## an example of GCV based model selection as
> ## an alternative to stepwise selection, using
> ## shrinkage smoothers...
> library(mgcv)
> set.seed(0);n <- 400
> dat <- gamSim(1,n=n,scale=2)
Gu & Wahba 4 term additive model
> dat$x4 <- runif(n, 0, 1)
> dat$x5 <- runif(n, 0, 1)
> attach(dat)
> ## Note the increased gamma parameter below to favour
> ## slightly smoother models...
> b<-gam(y~s(x0,bs="ts")+s(x1,bs="ts")+s(x2,bs="ts")+
+    s(x3,bs="ts")+s(x4,bs="ts")+s(x5,bs="ts"),gamma=1.4)
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = "ts") + s(x1, bs = "ts") + s(x2, bs = "ts") + 
    s(x3, bs = "ts") + s(x4, bs = "ts") + s(x5, bs = "ts")

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.9150     0.1052   75.21   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
            edf Ref.df      F  p-value    
s(x0) 2.642e+00      9  2.517 9.78e-06 ***
s(x1) 2.151e+00      9 33.233  < 2e-16 ***
s(x2) 7.380e+00      9 82.835  < 2e-16 ***
s(x3) 2.133e-09      9  0.000   0.6022    
s(x4) 5.613e-01      9  0.208   0.0672 .  
s(x5) 1.678e-09      9  0.000   0.5392    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Rank: 54/55
R-sq.(adj) =  0.724   Deviance explained = 73.3%
GCV = 4.7211  Scale est. = 4.4302    n = 400
> plot(b,pages=1)
> 
> ## Same again using REML/ML
> b<-gam(y~s(x0,bs="ts")+s(x1,bs="ts")+s(x2,bs="ts")+
+    s(x3,bs="ts")+s(x4,bs="ts")+s(x5,bs="ts"),method="REML")
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = "ts") + s(x1, bs = "ts") + s(x2, bs = "ts") + 
    s(x3, bs = "ts") + s(x4, bs = "ts") + s(x5, bs = "ts")

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.9150     0.1051   75.32   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
           edf Ref.df      F  p-value    
s(x0) 2.885762      9  2.522 1.37e-05 ***
s(x1) 2.400354      9 33.539  < 2e-16 ***
s(x2) 8.020248      9 83.252  < 2e-16 ***
s(x3) 0.000138      9  0.000    0.654    
s(x4) 0.696364      9  0.245    0.074 .  
s(x5) 0.001376      9  0.000    0.461    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.725   Deviance explained = 73.4%
-REML = 896.49  Scale est. = 4.4173    n = 400
> plot(b,pages=1)
> 
> ## And once more, but using the null space penalization
> b<-gam(y~s(x0,bs="cr")+s(x1,bs="cr")+s(x2,bs="cr")+
+    s(x3,bs="cr")+s(x4,bs="cr")+s(x5,bs="cr"),
+    method="REML",select=TRUE)
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = "cr") + s(x1, bs = "cr") + s(x2, bs = "cr") + 
    s(x3, bs = "cr") + s(x4, bs = "cr") + s(x5, bs = "cr")

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.9150     0.1052   75.24   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
            edf Ref.df      F  p-value    
s(x0) 2.2254365      9  2.563 1.49e-06 ***
s(x1) 2.6764675      9 33.409  < 2e-16 ***
s(x2) 7.9724407      9 81.746  < 2e-16 ***
s(x3) 0.0005948      9  0.000   1.0000    
s(x4) 0.7706253      9  0.237   0.0831 .  
s(x5) 0.3861344      9  0.059   0.2318    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.724   Deviance explained = 73.4%
-REML = 889.08  Scale est. = 4.4268    n = 400
> plot(b,pages=1)
> 
> 
> detach(dat);rm(dat)
> 
> 
> 
> cleanEx()
> nameEx("summary.gam")
> ### * summary.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.gam
> ### Title: Summary for a GAM fit
> ### Aliases: summary.gam print.summary.gam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(0)
> 
> dat <- gamSim(1,n=200,scale=2) ## simulate data
Gu & Wahba 4 term additive model
> 
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
> plot(b,pages=1)
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.3182     0.1525      48   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df      F p-value    
s(x0) 2.318  2.892  4.116  0.0105 *  
s(x1) 2.306  2.859 43.773  <2e-16 ***
s(x2) 7.655  8.524 31.805  <2e-16 ***
s(x3) 1.233  1.425  0.122  0.7184    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.687   Deviance explained = 70.9%
GCV = 5.0135  Scale est. = 4.6497    n = 200
> 
> ## now check the p-values by using a pure regression spline.....
> b.d <- round(summary(b)$edf)+1 ## get edf per smooth
> b.d <- pmax(b.d,3) # can't have basis dimension less than 3!
> bc<-gam(y~s(x0,k=b.d[1],fx=TRUE)+s(x1,k=b.d[2],fx=TRUE)+
+         s(x2,k=b.d[3],fx=TRUE)+s(x3,k=b.d[4],fx=TRUE),data=dat)
> plot(bc,pages=1)
> summary(bc)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, k = b.d[1], fx = TRUE) + s(x1, k = b.d[2], fx = TRUE) + 
    s(x2, k = b.d[3], fx = TRUE) + s(x3, k = b.d[4], fx = TRUE)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.3182     0.1532   47.78   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
      edf Ref.df      F p-value    
s(x0)   2      2  6.723 0.00151 ** 
s(x1)   2      2 60.552 < 2e-16 ***
s(x2)   8      8 34.550 < 2e-16 ***
s(x3)   2      2  0.912 0.40371    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.685   Deviance explained = 70.7%
GCV = 5.0727  Scale est. = 4.6922    n = 200
> 
> ## Example where some p-values are less reliable...
> dat <- gamSim(6,n=200,scale=2)
4 term additive + random effectGu & Wahba 4 term additive model
> b <- gam(y~s(x0,m=1)+s(x1)+s(x2)+s(x3)+s(fac,bs="re"),data=dat)
> ## Here s(x0,m=1) can be penalized to zero, so p-value approximation
> ## cruder than usual...
> summary(b) 

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, m = 1) + s(x1) + s(x2) + s(x3) + s(fac, bs = "re")

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   15.277      1.541   9.915   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
         edf Ref.df       F  p-value    
s(x0)  5.003  8.000   4.437 5.18e-06 ***
s(x1)  2.301  2.861  46.024  < 2e-16 ***
s(x2)  6.856  7.947  46.522  < 2e-16 ***
s(x3)  1.000  1.000   0.341     0.56    
s(fac) 2.972  3.000 173.985  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =   0.85   Deviance explained = 86.3%
GCV = 4.6185  Scale est. = 4.1767    n = 200
> 
> ## p-value check - increase k to make this useful!
> k<-20;n <- 200;p <- rep(NA,k)
> for (i in 1:k)
+ { b<-gam(y~te(x,z),data=data.frame(y=rnorm(n),x=runif(n),z=runif(n)),
+          method="ML")
+   p[i]<-summary(b)$s.p[1]
+ }
> plot(((1:k)-0.5)/k,sort(p))
> abline(0,1,col=2)
> ks.test(p,"punif") ## how close to uniform are the p-values?

	One-sample Kolmogorov-Smirnov test

data:  p
D = 0.10829, p-value = 0.9532
alternative hypothesis: two-sided

> 
> ## A Gamma example, by modify `gamSim' output...
>  
> dat <- gamSim(1,n=400,dist="normal",scale=1)
Gu & Wahba 4 term additive model
> dat$f <- dat$f/4 ## true linear predictor 
> Ey <- exp(dat$f);scale <- .5 ## mean and GLM scale parameter
> ## Note that `shape' and `scale' in `rgamma' are almost
> ## opposite terminology to that used with GLM/GAM...
> dat$y <- rgamma(Ey*0,shape=1/scale,scale=Ey*scale)
> bg <- gam(y~ s(x0)+ s(x1)+s(x2)+s(x3),family=Gamma(link=log),
+           data=dat,method="REML")
> summary(bg)

Family: Gamma 
Link function: log 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.95252    0.03225   60.54   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df      F  p-value    
s(x0) 4.210  5.179  4.870 0.000282 ***
s(x1) 2.511  3.122 77.634  < 2e-16 ***
s(x2) 7.688  8.559 48.662  < 2e-16 ***
s(x3) 1.001  1.001  0.212 0.646048    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.487   Deviance explained = 61.9%
-REML = 1145.9  Scale est. = 0.41609   n = 400
> 
> 
> 
> 
> cleanEx()
> nameEx("t2")
> ### * t2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: t2
> ### Title: Define alternative tensor product smooths in GAM formulae
> ### Aliases: t2
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> 
> # following shows how tensor product deals nicely with 
> # badly scaled covariates (range of x 5% of range of z )
> require(mgcv)
> test1<-function(x,z,sx=0.3,sz=0.4)  
+ { x<-x*20
+   (pi**sx*sz)*(1.2*exp(-(x-0.2)^2/sx^2-(z-0.3)^2/sz^2)+
+   0.8*exp(-(x-0.7)^2/sx^2-(z-0.8)^2/sz^2))
+ }
> n<-500
> old.par<-par(mfrow=c(2,2))
> x<-runif(n)/20;z<-runif(n);
> xs<-seq(0,1,length=30)/20;zs<-seq(0,1,length=30)
> pr<-data.frame(x=rep(xs,30),z=rep(zs,rep(30,30)))
> truth<-matrix(test1(pr$x,pr$z),30,30)
> f <- test1(x,z)
> y <- f + rnorm(n)*0.2
> b1<-gam(y~s(x,z))
> persp(xs,zs,truth);title("truth")
> vis.gam(b1);title("t.p.r.s")
> b2<-gam(y~t2(x,z))
> vis.gam(b2);title("tensor product")
> b3<-gam(y~t2(x,z,bs=c("tp","tp")))
> vis.gam(b3);title("tensor product")
> par(old.par)
> 
> test2<-function(u,v,w,sv=0.3,sw=0.4)  
+ { ((pi**sv*sw)*(1.2*exp(-(v-0.2)^2/sv^2-(w-0.3)^2/sw^2)+
+   0.8*exp(-(v-0.7)^2/sv^2-(w-0.8)^2/sw^2)))*(u-0.5)^2*20
+ }
> n <- 500
> v <- runif(n);w<-runif(n);u<-runif(n)
> f <- test2(u,v,w)
> y <- f + rnorm(n)*0.2
> 
> ## tensor product of 2D Duchon spline and 1D cr spline
> m <- list(c(1,.5),0)
> b <- gam(y~t2(v,w,u,k=c(30,5),d=c(2,1),bs=c("ds","cr"),m=m))
> 
> ## look at the edf per penalty. "rr" denotes interaction term 
> ## (range space range space). "rn" is interaction of null space
> ## for u with range space for v,w...
> pen.edf(b) 
t2(v,w,u)rr t2(v,w,u)nr t2(v,w,u)rn 
  36.136432    0.483695   41.173645 
> 
> ## plot results...
> op <- par(mfrow=c(2,2))
> vis.gam(b,cond=list(u=0),color="heat",zlim=c(-0.2,3.5))
> vis.gam(b,cond=list(u=.33),color="heat",zlim=c(-0.2,3.5))
> vis.gam(b,cond=list(u=.67),color="heat",zlim=c(-0.2,3.5))
> vis.gam(b,cond=list(u=1),color="heat",zlim=c(-0.2,3.5))
> par(op)
> 
> b <- gam(y~t2(v,w,u,k=c(25,5),d=c(2,1),bs=c("tp","cr"),full=TRUE),
+          method="ML")
> ## more penalties now. numbers in labels like "r1" indicate which 
> ## basis function of a null space is involved in the term. 
> pen.edf(b) 
 t2(v,w,u)rr  t2(v,w,u)1r  t2(v,w,u)2r  t2(v,w,u)3r  t2(v,w,u)r1  t2(v,w,u)r2 
3.058307e+01 2.899981e+00 2.612023e-01 1.468683e-05 1.884064e+01 2.635854e-06 
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("te")
> ### * te
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: te
> ### Title: Define tensor product smooths or tensor product interactions in
> ###   GAM formulae
> ### Aliases: te ti
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> 
> # following shows how tensor pruduct deals nicely with 
> # badly scaled covariates (range of x 5% of range of z )
> require(mgcv)
> test1 <- function(x,z,sx=0.3,sz=0.4) { 
+   x <- x*20
+   (pi**sx*sz)*(1.2*exp(-(x-0.2)^2/sx^2-(z-0.3)^2/sz^2)+
+   0.8*exp(-(x-0.7)^2/sx^2-(z-0.8)^2/sz^2))
+ }
> n <- 500
> old.par <- par(mfrow=c(2,2))
> x <- runif(n)/20;z <- runif(n);
> xs <- seq(0,1,length=30)/20;zs <- seq(0,1,length=30)
> pr <- data.frame(x=rep(xs,30),z=rep(zs,rep(30,30)))
> truth <- matrix(test1(pr$x,pr$z),30,30)
> f <- test1(x,z)
> y <- f + rnorm(n)*0.2
> b1 <- gam(y~s(x,z))
> persp(xs,zs,truth);title("truth")
> vis.gam(b1);title("t.p.r.s")
> b2 <- gam(y~te(x,z))
> vis.gam(b2);title("tensor product")
> b3 <- gam(y~ ti(x) + ti(z) + ti(x,z))
> vis.gam(b3);title("tensor anova")
> 
> ## now illustrate partial ANOVA decomp...
> vis.gam(b3);title("full anova")
> b4 <- gam(y~ ti(x) + ti(x,z,mc=c(0,1))) ## note z constrained!
> vis.gam(b4);title("partial anova")
> plot(b4)
> 
> par(old.par)
> 
> ## now with a multivariate marginal....
> 
> test2<-function(u,v,w,sv=0.3,sw=0.4)  
+ { ((pi**sv*sw)*(1.2*exp(-(v-0.2)^2/sv^2-(w-0.3)^2/sw^2)+
+   0.8*exp(-(v-0.7)^2/sv^2-(w-0.8)^2/sw^2)))*(u-0.5)^2*20
+ }
> n <- 500
> v <- runif(n);w<-runif(n);u<-runif(n)
> f <- test2(u,v,w)
> y <- f + rnorm(n)*0.2
> # tensor product of 2D Duchon spline and 1D cr spline
> m <- list(c(1,.5),rep(0,0)) ## example of list form of m
> b <- gam(y~te(v,w,u,k=c(30,5),d=c(2,1),bs=c("ds","cr"),m=m))
> op <- par(mfrow=c(2,2))
> vis.gam(b,cond=list(u=0),color="heat",zlim=c(-0.2,3.5))
> vis.gam(b,cond=list(u=.33),color="heat",zlim=c(-0.2,3.5))
> vis.gam(b,cond=list(u=.67),color="heat",zlim=c(-0.2,3.5))
> vis.gam(b,cond=list(u=1),color="heat",zlim=c(-0.2,3.5))
> par(op)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("tensor.prod.model.matrix")
> ### * tensor.prod.model.matrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: tensor.prod.model.matrix
> ### Title: Utility functions for constructing tensor product smooths
> ### Aliases: tensor.prod.model.matrix tensor.prod.penalties
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> X <- list(matrix(1:4,2,2),matrix(5:10,2,3))
> tensor.prod.model.matrix(X)
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    5    7    9   15   21   27
[2,]   12   16   20   24   32   40
> 
> S<-list(matrix(c(2,1,1,2),2,2),matrix(c(2,1,0,1,2,1,0,1,2),3,3))
> tensor.prod.penalties(S)
[[1]]
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    2    0    0    1    0    0
[2,]    0    2    0    0    1    0
[3,]    0    0    2    0    0    1
[4,]    1    0    0    2    0    0
[5,]    0    1    0    0    2    0
[6,]    0    0    1    0    0    2

[[2]]
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    2    1    0    0    0    0
[2,]    1    2    1    0    0    0
[3,]    0    1    2    0    0    0
[4,]    0    0    0    2    1    0
[5,]    0    0    0    1    2    1
[6,]    0    0    0    0    1    2

> 
> 
> 
> 
> cleanEx()
> nameEx("trichol")
> ### * trichol
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: trichol
> ### Title: Choleski decomposition of a tri-diagonal matrix
> ### Aliases: trichol
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> ## simulate some diagonals...
> set.seed(19); k <- 7
> ld <- runif(k)+1
> sd <- runif(k-1) -.5
> 
> ## get diagonals of chol factor...
> trichol(ld,sd)
$ld
[1] 1.056944 1.216233 1.254832 1.017814 1.164840 1.105459 1.105716

$sd
[1]  0.06933549  0.27677293  0.18014527 -0.09189728 -0.04316683  0.26319837

> 
> ## compare to dense matrix result...
> A <- diag(ld);for (i in 1:(k-1)) A[i,i+1] <- A[i+1,i] <- sd[i]
> R <- chol(A)
> diag(R);diag(R[,-1])
[1] 1.056944 1.216233 1.254832 1.017814 1.164840 1.105459 1.105716
[1]  0.06933549  0.27677293  0.18014527 -0.09189728 -0.04316683  0.26319837
> 
> 
> 
> 
> cleanEx()
> nameEx("uniquecombs")
> ### * uniquecombs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: uniquecombs
> ### Title: find the unique rows in a matrix
> ### Aliases: uniquecombs
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv)
> 
> ## matrix example...
> X <- matrix(c(1,2,3,1,2,3,4,5,6,1,3,2,4,5,6,1,1,1),6,3,byrow=TRUE)
> print(X)
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    1    2    3
[3,]    4    5    6
[4,]    1    3    2
[5,]    4    5    6
[6,]    1    1    1
> Xu <- uniquecombs(X);Xu
     [,1] [,2] [,3]
[1,]    1    1    1
[2,]    1    2    3
[3,]    1    3    2
[4,]    4    5    6
attr(,"index")
[1] 2 2 4 3 4 1
> ind <- attr(Xu,"index")
> ## find the value for row 3 of the original from Xu
> Xu[ind[3],];X[3,]
[1] 4 5 6
[1] 4 5 6
> 
> ## data frame example...
> df <- data.frame(f=factor(c("er",3,"b","er",3,3,1,2,"b")),
+       x=c(.5,1,1.4,.5,1,.6,4,3,1.7))
> uniquecombs(df)
   f   x
1  1 4.0
2  2 3.0
3  3 0.6
4  3 1.0
5  b 1.4
6  b 1.7
7 er 0.5
> 
> 
> 
> cleanEx()
> nameEx("vcov.gam")
> ### * vcov.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vcov.gam
> ### Title: Extract parameter (estimator) covariance matrix from GAM fit
> ### Aliases: vcov.gam
> ### Keywords: models smooth regression
> 
> ### ** Examples
>  
> require(mgcv)
> n <- 100
> x <- runif(n)
> y <- sin(x*2*pi) + rnorm(n)*.2
> mod <- gam(y~s(x,bs="cc",k=10),knots=list(x=seq(0,1,length=10)))
> diag(vcov(mod))
 (Intercept)       s(x).1       s(x).2       s(x).3       s(x).4       s(x).5 
0.0003512824 0.0018908285 0.0020516235 0.0019509499 0.0019280202 0.0022531870 
      s(x).6       s(x).7       s(x).8 
0.0019856105 0.0019067656 0.0014655286 
> 
> 
> 
> cleanEx()
> nameEx("vis.gam")
> ### * vis.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vis.gam
> ### Title: Visualization of GAM objects
> ### Aliases: vis.gam persp.gam
> ### Keywords: hplot models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(0)
> n<-200;sig2<-4
> x0 <- runif(n, 0, 1);x1 <- runif(n, 0, 1)
> x2 <- runif(n, 0, 1)
> y<-x0^2+x1*x2 +runif(n,-0.3,0.3)
> g<-gam(y~s(x0,x1,x2))
> old.par<-par(mfrow=c(2,2))
> # display the prediction surface in x0, x1 ....
> vis.gam(g,ticktype="detailed",color="heat",theta=-35)  
> vis.gam(g,se=2,theta=-35) # with twice standard error surfaces
> vis.gam(g, view=c("x1","x2"),cond=list(x0=0.75)) # different view 
> vis.gam(g, view=c("x1","x2"),cond=list(x0=.75),theta=210,phi=40,
+         too.far=.07)
> # ..... areas where there is no data are not plotted
> 
> # contour examples....
> vis.gam(g, view=c("x1","x2"),plot.type="contour",color="heat")
> vis.gam(g, view=c("x1","x2"),plot.type="contour",color="terrain")
> vis.gam(g, view=c("x1","x2"),plot.type="contour",color="topo")
> vis.gam(g, view=c("x1","x2"),plot.type="contour",color="cm")
> 
> 
> par(old.par)
> 
> # Examples with factor and "by" variables
> 
> fac<-rep(1:4,20)
> x<-runif(80)
> y<-fac+2*x^2+rnorm(80)*0.1
> fac<-factor(fac)
> b<-gam(y~fac+s(x))
> 
> vis.gam(b,theta=-35,color="heat") # factor example
> 
> z<-rnorm(80)*0.4   
> y<-as.numeric(fac)+3*x^2*z+rnorm(80)*0.1
> b<-gam(y~fac+s(x,by=z))
> 
> vis.gam(b,theta=-35,color="heat",cond=list(z=1)) # by variable example
> 
> vis.gam(b,view=c("z","x"),theta= -135) # plot against by variable
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("ziP")
> ### * ziP
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ziP
> ### Title: GAM zero-inflated Poisson regression family
> ### Aliases: ziP
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> rzip <- function(gamma,theta= c(-2,.3)) {
+ ## generate zero inflated Poisson random variables, where 
+ ## lambda = exp(gamma), eta = theta[1] + exp(theta[2])*gamma
+ ## and 1-p = exp(-exp(eta)).
+    y <- gamma; n <- length(y)
+    lambda <- exp(gamma)
+    eta <- theta[1] + exp(theta[2])*gamma
+    p <- 1- exp(-exp(eta))
+    ind <- p > runif(n)
+    y[!ind] <- 0
+    np <- sum(ind)
+    ## generate from zero truncated Poisson, given presence...
+    y[ind] <- qpois(runif(np,dpois(0,lambda[ind]),1),lambda[ind])
+    y
+ } 
> 
> library(mgcv)
> ## Simulate some ziP data...
> set.seed(1);n<-400
> dat <- gamSim(1,n=n)
Gu & Wahba 4 term additive model
> dat$y <- rzip(dat$f/4-1)
> 
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=ziP(),data=dat)
> 
> b$outer.info ## check convergence!!
$conv
[1] "full convergence"

$iter
[1] 10

$grad
              [,1]
[1,]  3.216939e-04
[2,]  1.718311e-03
[3,]  4.371838e-05
[4,]  1.001580e-04
[5,] -7.565128e-05
[6,] -5.846647e-04

$hess
             [,1]        [,2]         [,3]        [,4]        [,5]         [,6]
[1,] 101.93262677 175.2537686  0.232671668  0.44079859  0.81421001 -0.068086736
[2,] 175.25376864 380.7369770 -0.100772446  1.03832132  0.14196719 -0.371676462
[3,]   0.23267167  -0.1007724  0.682549166 -0.03902081 -0.03111944 -0.008807434
[4,]   0.44079859   1.0383213 -0.039020807  0.33279908  0.00478774 -0.020386558
[5,]   0.81421001   0.1419672 -0.031119440  0.00478774  2.89502918  0.016320649
[6,]  -0.06808674  -0.3716765 -0.008807434 -0.02038656  0.01632065  0.030793117

$score.hist
 [1] 715.5231 655.8980 630.4674 615.6185 612.3983 611.4905 611.4670 611.4402
 [9] 611.4034 611.4030

> b

Family: Zero inflated Poisson(-1.77,1.187) 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.42 2.76 7.40 1.30  total = 14.88 

REML score: 611.403     
> plot(b,pages=1)
> plot(b,pages=1,unconditional=TRUE) ## add s.p. uncertainty 
> gam.check(b)

Method: REML   Optimizer: outer newton
full convergence after 10 iterations.
Gradient range [-0.0005846647,0.001718311]
(score 611.403 & scale 1).
Hessian positive definite, eigenvalue range [0.02840486,465.2732].
Model rank =  37 / 37 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

         k'   edf k-index p-value
s(x0) 9.000 2.419   0.838    0.08
s(x1) 9.000 2.765   0.941    0.79
s(x2) 9.000 7.398   0.876    0.27
s(x3) 9.000 1.302   0.870    0.19
> ## more checking...
> ## 1. If the zero inflation rate becomes decoupled from the linear predictor, 
> ## it is possible for the linear predictor to be almost unbounded in regions
> ## containing many zeroes. So examine if the range of predicted values 
> ## is sane for the zero cases? 
> range(predict(b,type="response")[b$y==0])
[1] 0.060769 9.696370
> 
> ## 2. Further plots...
> par(mfrow=c(2,2))
> plot(predict(b,type="response"),residuals(b))
> plot(predict(b,type="response"),b$y);abline(0,1,col=2)
> plot(b$linear.predictors,b$y)
> qq.gam(b,rep=20,level=1)
> 
> ## 3. Refit fixing the theta parameters at their estimated values, to check we 
> ## get essentially the same fit...
> thb <- b$family$getTheta()
> b0 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=ziP(theta=thb),data=dat)
> b;b0

Family: Zero inflated Poisson(-1.77,1.187) 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.42 2.76 7.40 1.30  total = 14.88 

REML score: 611.403     

Family: Zero inflated Poisson(-1.77,1.187) 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.42 2.77 7.40 1.30  total = 14.89 

REML score: 611.403     
> 
> ## Example fit forcing minimum linkage of prob present and
> ## linear predictor. Can fix some identifiability problems.
> b2 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=ziP(b=.3),data=dat)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("ziplss")
> ### * ziplss
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ziplss
> ### Title: Zero inflated Poisson location-scale model family
> ### Aliases: ziplss
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## simulate some data...
> f0 <- function(x) 2 * sin(pi * x); f1 <- function(x) exp(2 * x)
> f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
+             (10 * x)^3 * (1 - x)^10
> n <- 500;set.seed(5)
> x0 <- runif(n); x1 <- runif(n)
> x2 <- runif(n); x3 <- runif(n)
> 
> ## Simulate probability of potential presence...
> eta1 <- f0(x0) + f1(x1) - 3
> p <- binomial()$linkinv(eta1) 
> y <- as.numeric(runif(n)<p) ## 1 for presence, 0 for absence
> 
> ## Simulate y given potentially present (not exactly model fitted!)...
> ind <- y>0
> eta2 <- f2(x2[ind])/3
> y[ind] <- rpois(exp(eta2),exp(eta2))
> 
> ## Fit ZIP model... 
> b <- gam(list(y~s(x2)+s(x3),~s(x0)+s(x1)),family=ziplss())
> b$outer.info ## check convergence
$conv
[1] "full convergence"

$iter
[1] 7

$grad
              [,1]
[1,] -2.597607e-06
[2,] -1.918936e-05
[3,] -1.186818e-04
[4,] -3.540047e-04

$hess
            [,1]        [,2]         [,3]         [,4]
[1,] 3.444649816 0.004447179 0.0000000000 0.0000000000
[2,] 0.004447179 0.008913900 0.0000000000 0.0000000000
[3,] 0.000000000 0.000000000 1.3632659544 0.0001291374
[4,] 0.000000000 0.000000000 0.0001291374 0.0003571959

$score.hist
[1] 881.4945 879.6256 879.0983 879.0793 879.0736 879.0719 879.0713

> 
> summary(b) 

Family: ziplss 
Link function: identity identity 

Formula:
y ~ s(x2) + s(x3)
~s(x0) + s(x1)

Parametric coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)    1.14488    0.04888  23.420   <2e-16 ***
(Intercept).1 -0.05605    0.06467  -0.867    0.386    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
          edf Ref.df   Chi.sq  p-value    
s(x2)   8.025  8.714 1071.346  < 2e-16 ***
s(x3)   1.148  1.282    0.208    0.817    
s.1(x0) 3.869  4.787   31.414 7.35e-06 ***
s.1(x1) 1.004  1.008   70.563  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Deviance explained = 61.6%
-REML = 879.07  Scale est. = 1         n = 500
> plot(b,pages=1)
> 
> 
> 
> ### * <FOOTER>
> ###
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  52.848 0.2 54.218 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
